\documentclass[a4paper]{article}
\usepackage{import}
\input{../../setup.sty}


\onehalfspacing
\title{Intelligenza Artificiale}
\author{Università di Verona\\Imbriani Paolo -VR500437\\Professor Alessandro Farinelli}

\begin{document}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{../UniversityofVerona.png}
    \label{fig:centered-image}
\end{figure}

\maketitle

\pagebreak

\tableofcontents

\pagebreak

\section{Introduzione}

Alle origini dell'intelligenza artificiale vi è un bisogno diverso da quello che abbiamo oggi.
Alan Turing, negli anni 50 si era chiesto se le macchine potessero pensare, creando un test famoso ancora ora 
come "test di Turing" dove un interrogatore umano si deve interfacciare con un umano e una macchina 
e doveva capire chi dei due fosse chi. 
Nel 1956 ci fu uno studio fatto da il progetto di ricerca di Dartmouth, che aveva l'intento
di risolvere compiti che richiedeva l'intelligenza di una persona attraverso una macchina,
comprendendo che le \textit{anche le macchine possono imparare}.
La definizione più "accettata" di Intelligenza Artificiale è quella dove viene vista come una 
complessa e affascinante \textit{disciplina} che studia come simulare l'intelligeza in scenari complessi usando come
strumenti agenti autonomi per delle task ripetitive, sporche e pericolose che sfruttano l'analisi dei dati
(predizione e classificazione).

\dfn{}
{
    L'intelligenza artificiale è una disciplina che studia come \textbf{simulare} l'intelligenza
    umana in scenari complessi.
}
\noindent
Bisogna distinguere machine learning e programmazione:
\begin{itemize}
    \item \textbf{Programmazione}: macchine programmate per ogni task che devono eseguire (il concetto chiave
    è \textbf{il programma})
    \item \textbf{Machine Learning}: insegnare alla macchina (attraverso esempi) come risolvere task più complesse (il concetto chiave 
    è il \textbf{modello})
\end{itemize}

\subsection{Machine Learning}
L'idea di far apprendere una macchina si possono dividere in tre paradigmi contraddisti:
\begin{itemize}
    \item Unsupervised learning
    \item Supervised learning
    \item Reinforcement learning
\end{itemize}
Esistono poi i trasformatori, che sono modelli di machine learning probabilistici che si basano sul concetto di attenzione, che sono alla base di modelli come GPT.
Il concetto dell'attenzione è quello di dare più importanza ad alcune parole rispetto ad altre in un contesto, per esempio in una frase.
La potenza di questi trasformatori è che riescono a fare un'analisi del contesto molto più profonda rispetto ai modelli precedenti, 
permettendo di fare analisi di immagini come per esempio riconoscere oggetti in un'immagine o riconoscere dove è presente l'acqua
all'interno di una foto.

\subsection{Agenti intelligenti}

Un agente intelligente è un'entità che percepisce il suo ambiente attraverso dei sensori e agisce su di esso attraverso degli attuatori.
\begin{itemize}
    \item Percepisce l'ambiente attraverso dei \textbf{sensori}
    \item Agisce sull'ambiente attraverso degli \textbf{attuatori}
    \item Ha un \textbf{obiettivo} da raggiungere
\end{itemize}
Come dovrebbe comportarsi un agente intelligente?
\begin{itemize}
    \item \textbf{Razionale}: agisce per massimizzare il raggiungimento dell'obiettivo
    \item \textbf{Performance measure}: misura di quanto bene l'agente sta raggiungendo l'obiettivo
\end{itemize}
Quando vogliamo ragionare sul Reinforcement Learning, è utilire usare il \textit{Markov Decision Process}.
\dfn{}
{
    Un \textbf{Markov Decision Process (MDP)} è una tupla $(S, A, P, R)$ dove:
    \begin{itemize}
        \item $S$ è un insieme di stati
        \item $A$ è un insieme di azioni
        \item $P(s'|s,a)$ è la probabilità di transizione dallo stato $s$ allo stato $s'$ eseguendo l'azione $a$
        \item $R(s,a,s')$ è la ricompensa ottenuta eseguendo l'azione $a$ nello stato $s$ e transizionando nello stato $s'$
    \end{itemize}
}
\noindent
Poi si ha la \textit{policy} che è una funzione che mappa uno stato in un'azione. 

\section{Risolvere problemi con la ricerca}

\subsection{Agents and enviroments}

Gli agenti includono umani, robot, softbot, termostati, ecc. La funzione
agente mappa la storia delle percezioni in azioni.
\[f : \mathcal{P}^* \mapsto A\]
Il \textit{programma dell'agente} viene eseguito su un'architettura fisica che produce
$f$.

\ex{}
{
    Immaginiamo di avere un agente aspirapolvere che percepisce
    il luogo e i suoi contenuti.
    \begin{itemize}
        \item \textbf{Percezioni}: bump, Dirty e location (A o B)
        \item \textbf{Azioni}: left, right, suck, noOp
    \end{itemize}
    un esempio di sequenza percepita potrebbe essere:
    \[(A, Dirty), Suck, (A, Dirty), Suck, (A, Clean), Right\] \[ 
    (B, Dirty), Suck, (B, Clean), Left, (A, Clean), NoOp\]
    Cosa fa la funzione \textit{Right}? Può essere implementata
    in un piccolo programma agente?
    Se un agentes ha $|\mathcal{P}|$ possibili percezioni, quante
    "entries" avrà la tabella della funzione agente dopo $T$ time steps?
    \[\sum_{t=1}^{T} |P|^t\]
    L'obiettivo dell'IA è quello di progettare \textbf{piccoli}
    programmi agenti che permettono di rappresentare grandi funzioni agenti.
}
\noindent
\begin{minted}{python}
function Reflex-Vacuum-Agent([location,status]) returns an action
    if status = dirty then return suck
    else if location = A then return right
    else if location = B then return left
\end{minted}
\noindent

\subsubsection{Multi-robot Patrolling}

\ex{}
{
    Considerate il seguente ambiente:
    \begin{itemize}
        \item Tre stanze (A,B,C) e due robot ($r_1, r_2$)
        \item $r_1$ può pattugliare $A$ e $B$, $r_2$ può pattugliare $B$ e $C$
        \item $r_1$ inizia da $A$ e $r_2$ inizia da $C$
        \item Il tempo di viaggio tra le stanze è $0$
        \item Performance Measure: minimizzare il tempo medio di inattività tra le stanze
        \item Media di inattività: somma degli intervalli nella quale la stanza non è stata visitata 
        da nessun robot
        \item Quale potrebbe essere un comportamento razionale di questo ambiente?
    \end{itemize}
    Quello che succede in maniera ragionevole è la seguente, dove S è la tupla
    in cui i robot sono posizionati:
    TODO
    \\
    Nei diversi casi si ha che il miglior modo per fare girare i robot
    è quello di farli muovere alternando chi entra nella stanza B 
    minimizzando anche la varianza nelle varie stanze perché dobbiamo
    stare attenti a non penalizzare troppo una stanza.
}

\subsection{Tipi di ambiente}

Il tipo di ambiente determina la progettazione di un agente? 
Nel mondo reale è ovviamente parzialmente visibile, stocastico, sequenziale,
dinamico, continuo, multi-agente.

\begin{itemize}
    \item \textbf{Completamente osservabile vs parzialmente osservabile}: un agente ha accesso
    completo allo stato dell'ambiente in ogni istante di tempo?
    \item \textbf{Deterministico vs stocastico}: il prossimo stato dell'ambiente è completamente
    determinato dallo stato corrente e dall'azione eseguita dall'agente?
    \item \textbf{Episodico vs sequenziale}: l'esperienza dell'agente è divisa in episodi
    indipendenti?
    \item \textbf{Statico vs dinamico}: l'ambiente può cambiare mentre l'agente sta pensando?
    \item \textbf{Discreto vs continuo}: il numero di stati, percezioni e azioni è finito o infinito?
    \item \textbf{Singolo agente vs multi-agente}: l'agente agisce da solo o ci sono altri agenti
    che possono influenzare l'ambiente?
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        & Crosswords & Robo-selector & Poker & Taxi \\
        \hline
        Osservabile & Sì  & Parziale & Parziale & Parziale \\
        Deterministico & Sì & No & No &  No\\
        Episodico & No & Sì & No & No\\
        Statico & Sì & No & Sì & No \\
        Discreto & Sì & No & Sì & No \\
        Singolo agente & Sì & Sì & No & No\\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item Se il problema è deterministico e completamente osservabile, è un \textbf{single-state problem}
    \item Se il problema non è osservabile, è un \textbf{conformant problem}
    \item Se il problema è non deterministico o parzialmente osservabile, è un \textbf{contingency problem}
    \item Quando non conosco lo spazio degli stati è un \textbf{exploration problem}
\end{itemize}

\subsection{Problem Solving Agents}

Una forma ristretta di agente generale sono i: \textbf{Goal Based Agent}
\begin{itemize}
    \item Formula un goal e un problema partendo dallo stato corrente
    \item Cerco una soluzione a questo problema
    \item Eseguo la soluzione ignorando le percezioni
\end{itemize}
Notiamo che questo si chiama anche offline problem; la soluzione viene eseguita ad "occhi chiusi".
\begin{minted}{python}
function Simple-Problem-Solving-Agent(percept) returns an action
    static: solution, state, problem, action
    state <- Update-State(state, percept)
    if seq is empty then
        goal <- Formulate-Goal(state)
        problem <- Formulate-Problem(state, goal)
        seq <- Search(problem)
    action <- First(seq)
    seq <- Rest(seq)
    return action
\end{minted}

\ex{Vacanze in Romania}
{
    In viaggio in Romania, se attulamente ad Arad.
    Il viaggio parte domani da Bucharest.
    \begin{itemize}
        \item \textbf{Formulate Goal:} essere a Bucharest
        \item \textbf{Formulate Problem:} stati: varie città, azioni: guidare tra le città
        \item \textbf{Search:} trovare una sequenza di azioni che portano da Arad a Bucharest
        \item \textbf{Esempio di Soluzione:} Arad, Sibiu, Fagaras, Bucharest
    \end{itemize}
}

\subsubsection{Tree Search Algorithm}

Idea base: offline, esplorazione simulata di spazio di stati, generando
successori di stati già esplorati.
\begin{minted}{python}
function Tree-Search(problem) returns a solution, or failure
    initialize the frontier using the initial state of problem
    loop do
        if the frontier is empty then return failure
        node <- Pop an element from the frontier
        if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
        expand node, adding the resulting nodes to the frontier
    end
\end{minted}

\dfn{}
{
    Uno \textbf{stato} è una rappresentazione di una configurazione
    fisica.
}
\dfn{}
{
    Un \textbf{nodo} è una struttura dati che contiene:
    \begin{itemize}
        \item uno stato
        \item un puntatore al nodo genitore
        \item l'azione che ha generato lo stato
        \item il costo del cammino dal nodo radice a questo nodo
    \end{itemize}
    Gli stati non hanno parenti, azioni, figli, costi e profondità!
}

\begin{minted}{python}
function Expand(node, problem) returns a set of nodes
    successors <- an empty list
    for each action in problem.ACTIONS(node.STATE) do
        child <- CHILD-NODE(problem, node, action)
        add child to successors
    return successors    
\end{minted}
\noindent

\subsection{Strategie di ricerca}

Una strategia è definita dal scegliere l'ordine dei nodi di espansione.
Strategie vengono valutate insieme alle seguenti metriche:
\begin{itemize}
    \item Completezza: la strategia trova una soluzione se esiste
    \item Tempo: tempo di esecuzione della strategia
    \item Spazio: memoria usata dalla strategia
    \item Optimalità: la strategia trova la soluzione ottima?
\end{itemize}
Tempo e spazio sono misurati in termini di:
\begin{itemize}
    \item $b$ branching factor (numero massimo di figli per nodo)
    \item $d$ profondità della soluzione più superficiale
    \item $m$ profondità massima dell'albero di ricerca (potrebbe essere infinito)
\end{itemize}

\subsubsection{Stati ripetuti}
Fallire nel riconoscere stati ripetuti può trasformare un problema lineare in un problema
esponenziale. Bisogna quindi mantenere una lista di stati già visitati e non espandere
nodi che portano a stati già visitati:
\begin{lstlisting}[language=Python]
function Graph-Search( problem, frontier) returns a solution, or failure
  explored <- an empty set
  frontier <- Insert(Make-Node(problem.Initial-State))
  while not IsEmty(frontier) do
    node <- Pop(frontier)
    if problem.Goal-Test(node.State) then return node
    if node.State is not in explored then
      add node.State to explored
      frontier <- InsertAll(Expand(node, problem))
    end if
  end loop
  return failure
\end{lstlisting}

\subsection{Ricerca non informata}
Gli algoritmi di ricerca non informata utilizzano soltanto i dati disponibili nella
definizione del problema e i principali sono:
\begin{itemize}
  \item Breadth-first search
  \item Uniform-cost search (Dijkstra)
  \item Depth-first search
  \item Depth-limited search
  \item Iterative deepening search
\end{itemize}

\subsubsection{Breadth-first search}
Questo algoritmo espande il nodo non esplorato più superficiale, cioè il nodo più vicino
alla radice. Utilizza una coda FIFO per la frontiera e i nuovi successori vengono
aggiunti alla fine della coda.
\begin{lstlisting}[language=Python]
function BFS( problem) returns a solution, or failure
  node <- node with State=problem.Initial-State,Path-Cost=0
  if problem.Goal-Test(node.State) then return node
  explored <- empty set frontier <- FIFO queue with node as the only element
  loop do
    if frontier is empty then return failure
    node <- Pop(frontier)
    add node.State to explored
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem,node,action)
      if child.State is not in (explored or frontier) then
        if problem.Goal-Test(child.State) then return child
        frontier <- Insert(child)
      end if
    end for
  end loop
\end{lstlisting}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, soltanto se \( b \) è finito, cioè se il branching factor
    è limitato
  \item \textbf{Complessità di tempo}: \( b + b^2 + b^3 + \ldots + b^d = O(b^d) \)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), perchè bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, soltanto se il costo delle azioni è uniforme
\end{itemize}

\subsubsection{Uniform-cost search}
Questo algoritmo espande il nodo non esplorato con il \textbf{costo del percorso più basso}.
La frontiera è una coda di priorità ordinata in base al costo del percorso.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, se il costo minimo delle azioni \( \ge \varepsilon \) 
    (con piccola ma \( \varepsilon > 0 \))
  \item \textbf{Complessità di tempo}: Numero di nodi \( g \le  \) del costo del percorso
    ottimale \( C^* \). \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Complessità di spazio}: \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Ottimale}: Sì perchè i nodi vengono espansi in ordine di costo del percorso
\end{itemize}
Ci sono due modifiche principali rispetto alla BFS che garantiscono l'ottimalità:
\begin{enumerate}
  \item Il goal test viene fatto quando il nodo viene estratto dalla frontiera, non quando
    viene generato. (Questo elemento spiega il \( +1 \) nella complessità
  \item Controllare se un nodo generato è già presente nella frontiera con un costo più
    alto e in tal caso sostituirlo con il nuovo nodo a costo più basso
\end{enumerate}

\subsubsection{Depth-first search}
Questo algoritmo espande il nodo non esplorato più profondo, cioè il nodo più lontano
dalla radice. Utilizza una pila LIFO per la frontiera e i nuovi successori vengono
aggiunti all'inizio.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ramo infinito,
    a meno che l'albero di ricerca non abbia una profondità limitata. Si potrebbero
    evitare loop modificando l'algoritmo per evitare stati ripetuti sul percorso corrente
  \item \textbf{Complessità di tempo}: \( O(b^m) \), dove \( m \) è la profondità massima
    dell'albero di ricerca
  \item \textbf{Complessità di spazio}: \( O(bm) \), bisogna memorizzare soltanto il
    percorso corrente e i nodi fratelli
  \item \textbf{Ottimale}: No, perchè non garantisce di trovare la soluzione migliore
\end{itemize}

\subsubsection{Iterative deepening search}
Questo algoritmo combina i vantaggi della BFS e della DFS. Esegue una serie di ricerche
in profondità limitata, aumentando progressivamente il limite di profondità fino a
trovare una soluzione.
\begin{lstlisting}[language=Python]
# Depth-Limited Search
function DLS(problem, limit) returns soln/fail/cutoff
  R-DLS(Make-Node(problem.Initial-State), problem, limit)


function R-DLS(node, problem, limit) returns soln/fail/cutoff
  if problem.Goal-Test(node.State) then return node
  else if limit = 0 then return cutoff # raggiunta la profondita' massima
  else
    # flag: c'e' stato un cutoff in uno dei sottoalberi?
    cutoff-occurred? <- false
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem, node, action)
      result <- R-DLS(child, problem, limit-1)
      if result = cutoff then cutoff-occurred? <- true
      else if result 6 = failure then return result
    end for
    if cutoff-occurred? then return cutoff else return failure
  end else

# Iterative Deepening Search
function IDS(problem) returns a solution
  inputs: problem, a problem
  for depth <- 0 to infinity do
    result <- DLS(problem, depth)
    if result 6 = cutoff then return result
  end
\end{lstlisting}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì
  \item \textbf{Complessità di tempo}: \( db^1 + (d-1)b^2 + \ldots + b^d = O(b^d) \) 
  \item \textbf{Complessità di spazio}: \( O(bd) \) 
  \item \textbf{Ottimale}: Sì, se il costo delle azioni è uniforme
\end{itemize}

\ex{}
{
  Assumi:
  \begin{enumerate}
    \item Un albero di ricerca ben bilanciato, tutti i nodi hanno lo stesso numero di figli
    \item Il goal state è l'ultimo che viene espanso nel suo livello (il più a destra)
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la ricerca in ampiezza quanti nodi
      vengono generati?
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la iterative deepening quanti nodi
      vengono generati?
  \end{enumerate}
}
\ex{}
{
  Un uomo ha un lupo, una pecora e un cavolo. L'uomo è sulla riva di un fiume con una
  barca che può trasportare solo lui e un altro oggetto. Il lupo mangia la pecora e la
  pecora mangia il cavolo, quindi non può lasciarli insieme da soli.
  \begin{enumerate}
    \item Formalizza il problema come un problema di ricerca
    \item Usa BFS per risolvere il problema
  \end{enumerate}

  \vspace{1em}
  \noindent
  \textbf{Soluzione:}

  Formalizziamo gli stati come una tupla:
  \[
    <W, S, C, M, B>
  \] 
  dove:
  \begin{itemize}
    \item \( W \): posizione del lupo
    \item \( S \): posizione della pecora
    \item \( C \): posizione del cavolo
    \item \( M \): posizione dell'uomo
    \item \( B \): stato della barca
  \end{itemize}
  La posizione può essere \( 0 \) (left) o \( 1 \) (right).

  Lo stato iniziale è:
  \[
    <0, 0, 0, 0, 0>
  \] 
  Lo stato obiettivo è:
  \[
    <1, 1, 1, 1, 1>
  \]
  Le azioni possibili sono:
  \begin{itemize}
    \item Porta il lupo (CW)
    \item Porta la pecora (CS)
    \item Porta il cavolo (CC)
    \item Porta niente (CN)
  \end{itemize}
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      Operatore & Precondizione & Funzione \\
      \hline
      \footnotesize CW & \footnotesize \( M = B, M = W, S \neq C \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<\bar{W},S,C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CS & \footnotesize \( M = B, M = S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,\bar{S},C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CC & \footnotesize \( M = B, M = C, W \neq S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,\bar{C},\bar{M},\bar{B}\right> \)\\
      \footnotesize CN & \footnotesize \( M = B \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,C,\bar{M},\bar{B}\right> \)\\
      \hline
    \end{tabular}
  \end{table}
  Notiamo che in tutte le precondizioni c'è \( M = B \) perchè l'uomo deve essere
  sempre con la barca, quindi si possono unire i due stati in uno solo \( M \).
}

\subsection{Ricerca informata}
Gli algoritmi di ricerca informata utilizzano informazioni aggiuntive (euristiche)
per guidare la ricerca verso la soluzione in modo più efficiente.

\subsubsection{Best-first search}
Questo algoritmo usa una \textbf{funzione di valutazione} per ogni nodo che stima la
"desiderabilità". La frontiera è una coda ordinata in ordine decrescente di desiderabilità.
A seconda di come viene definita la desiderabilità si ottengono diversi algoritmi:
\begin{itemize}
  \item Greedy best-first search
  \item A*
\end{itemize}

\subsubsection{Greedy best-first search}
Questo algoritmo espande il nodo che sembra essere il più vicino alla soluzione
secondo una funzione di valutazione euristica \( h(n) \) che stima il costo
rimanente per raggiungere l'obiettivo da un nodo \( n \).
\ex{}
{
  In una mappa di una città, la funzione di valutazione potrebbe essere la distanza
  in linea d'aria dal nodo corrente alla destinazione. In questo modo, l'algoritmo
  esplora prima i nodi che sembrano più vicini alla destinazione, riducendo il numero
  di nodi esplorati rispetto a una ricerca non informata.
}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ciclo infinito. È
    completo se lo spazio di ricerca è finito e ci sono controlli per evitare stati
    ripetuti
  \item \textbf{Complessità di tempo}: \( O(b^m) \) nel peggiore dei casi, ma può essere
    molto più veloce con una buona euristica
  \item \textbf{Complessità di spazio}: \( O(b^m) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: No
\end{itemize}

\subsubsection{A* search}
Questo algoritmo evita di espandere cammini che sono già molto costosi e ha come
funzione di valutazione:
\[
  f(n) = g(n) + h(n)
\] 
dove:
\begin{itemize}
  \item \( g(n) \): costo del percorso dal nodo iniziale a \( n \)
  \item \( h(n) \): stima del costo rimanente per raggiungere l'obiettivo da \( n \)
  \item \( f(n) \): stima del costo totale del percorso passando per \( n \)
\end{itemize}
L'euristica, per poter garantire l'ottimalità, deve essere \textbf{ammissibile}, cioè
per ogni nodo la stima di quel nodo deve essere minore o uguale del vero costo per arrivare
all'obbiettivo, quindi non deve \textbf{sovrastimare} il costo rimanente:
\[
  h(n) \le h^*(n) \quad h(n) \ge 0 \to h(G) = 0
\] 
dove \( h^*(n) \) è il costo effettivo del percorso da \( n \).
\thm{}
{
  Per A* l'euristica ammissibile implica l'ottimalità
}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, tranne se ci sono nodi infiniti con \( f \le f(G) \) 
  \item \textbf{Complessità di tempo}: Esponenziale in errore relativo in \( h \times  \) 
    lunghezza del numeo di passi della soluzione ottimale. (Se l'euristica è buona, la
    complessità sarà molto più bassa)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, se l'euristica è ammissibile e consistente
\end{itemize}

\subsection{Ricerca locale}

In molte problemi di ottimizzazione il "path" è irrilevante,
il traguardo è importante. In questi casi, allora lo spazio degli stati è 
un insieme di configurazioni:
\begin{itemize}
  \item Trovare la configurazione ottimale (TSP (Travelling Salesperson Problem), etc...)
  \item Trovdare una configurazione che soddisfi dei vincoli (n-Queens, per esempio, dove 
  ci sono 8 regine su una scacchiera e per trovare la configurazione
  dove nessuna delle 8 è sotto attacco, parto da una configurazione "base"
  e sposto le regine finché non trovo la configurazione traguardo, etc...)
\end{itemize}
Si possono usare algoritmi di "iterative improvement": 
\begin{itemize}
  \item Mantenere un singolo stato corrente
  \item Cercare di migliorarlo
\end{itemize}
Spazio costante, fatto apposta per online e offline search. 
Varianti di questo approccio arrivano fino a $1\%$ di soluzione ottimali.

\ex{Problema delle $n$ regine}
{
  \begin{itemize}
    \item Inserire $n$ regine su una scacchiera $n \times n$ in modo che nessuna regina
      possa attaccarne un'altra (quindi due regine non devono essere sulla stessa riga, colonna o diagonale).
    \item Muovi una regina per volta, cercando di ridurre il numero di conflitti.
  \end{itemize}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{nQueens.png}
  \end{figure}
  \noindent
  Quasi sempre si solve una problema di questo tipo in pochi passi, anche per $n=1$ milione.
}
\noindent
Ecco ora l'algoritmo di "hill-climbing" (come scalare il monte everest in una fitta nebbia con amnesia):
\begin{minted}{python}
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs:  problem, a problem
    local variables: current, a node
                     neighbor, a node
    current <- MAKE-NODE(problem.INITIAL-STATE)
    loop do
        neighbor <- a highest-value neighbor of current
        if neighbor.VALUE <= current.VALUE then return 
          current.STATE
        current <- neighbor
\end{minted}
\noindent
Utile per considerare lo \textit{state scape landscape}:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{hill-climbing.png}
\end{figure}
\noindent
Ci sono varianti di questo algoritmo:
\begin{itemize}
  \item \textbf{Random-restart hill climbing} è una variante che supera il massimo locale, trivialmente completo.
  \item \textbf{Random sideways moves} è buono perché esce dalle \textit{shoulder} ma non completamente perché può rimanere bloccato in un ciclo infinito su "flat local maxima". 
\end{itemize}

\subsubsection{Simulated annealing}

Simulated annealing è un algoritmo di ottimizzazione ispirato al processo di 
raffreddamento dei metalli. L'idea è di permettere occasionalmente mosse che peggiorano la soluzione corrente
per evitare di rimanere bloccati in massimi locali.
\begin{itemize}
  \item Inizia con una temperatura alta che permette molte mosse peggiorative
  \item La temperatura diminuisce gradualmente, riducendo la probabilità di accettare mosse peggiorative
  \item Alla fine, la temperatura raggiunge zero e l'algoritmo si comporta come hill-climbing
  \item La scelta della schedule di raffreddamento è cruciale per le prestazioni dell'algoritmo
\end{itemize}
\begin{lstlisting}[language=Python]
function Simulated-Annealing(problem, schedule) returns a solution state
    inputs: problem, a problem
    schedule, a mapping from time to "temperature"
    local variables: current, a node
                     next, a node
                     T, a "temperature" controlling prob. of downward steps
    current <- Make-Node(problem.Initial-State)
    for t <- 1 to infinity do
      T <- schedule(t)
      if T = 0 then return current
      next <- a randomly selected successor of current
      deltaE <- next.Value - current.Value
      if deltaE > 0 then current <- next
      else current <- next only with probability e^{delta E/T} 
\end{lstlisting}
A temperatura fissata $T$, la probabilità di accettare una mossa che peggiora la soluzione di $\Delta E$ è $e^{\Delta E / T}$.
\[p(x) = \alpha e^{\frac{E(x)}{kT}}\]
Decrescendo $T$ abbastanza, si può garantire la convergenza alla soluzione ottimale.
Perché
\[e^{\frac{E(x^*)}{kT}} / e^{\frac{E(x)}{kt}} = e^{\frac{E(x^*) - E(x)}{kt}} \gg 1 \quad \text{per } T \to 0\]

\subsubsection{Local beam search}

Local Beam Search è un algoritmo di ricerca locale che mantiene $k$ stati invece di uno solo. Inizia con $k$ stati casuali
e ad ogni iterazione:
\begin{itemize}
   \item Genera tutti i successori di tutti i $k$ stati correnti
  \item Seleziona randomicamente i $k$ migliori successori tra tutti quelli generati
  \item Ripete fino a quando non viene trovata una soluzione o non ci sono più miglioramenti
  \item Se tutti i $k$ stati convergono allo stesso punto, si può introdurre
    diversità sostituendo alcuni stati con nuovi stati casuali
\end{itemize}

\subsection{Ricerca locale in spazio continuo}

La ricerca locale può essere estesa a spazi di stato continui.
Per risolvere questi problemi si possono utilizzare tecniche come:
\begin{itemize}
  \item \textbf{Discretizzazione:} suddividere lo spazio continuo in una griglia di punti discreti e applicare
    algoritmi di ricerca locale su questi punti
    \item \textbf{Randomiche Perturbazioni:} introdurre piccole perturbazioni casuali alle soluzioni correnti per esplorare
    lo spazio delle soluzioni con metodi come il simulated annealing (il prossimo stato è scelto randomicamente)
    \item \textbf{Gradiente}: utilizzare il gradiente della funzione obiettivo per guidare la ricerca verso
    direzioni di miglioramento (il prossimo stato è scelto in base alla direzione del gradiente).
    Il metodo del gradiente calcola:
    \[\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)\]
    Per trovare la direizione di massima crescita della funzione obiettivo si pone 
    il gradiente uguale a zero:
    \[\nabla f(x) = 0\]
\end{itemize}
\noindent
A volte però non riusciamo a risolvere $\nabla f (x) = 0$ analiticamente, 
quindi possiamo migliorarla localmente:
\begin{itemize}
  \item Si performa un update nella direzione della salita per ogni coordinata
  \item Più la funzione è ripida più si fanno passi grandi
\end{itemize}
Aggionrare una coordinata viene effettuato tramite una
funzione generale $g(x_1, x_2)$
\[
x_1 \leftarrow x_1 + \alpha \frac{\partial g(x_1,x_2)}{\partial x_1} \quad
x_2 \leftarrow x_2 + \alpha \frac{\partial g(x_1,x_2)}{\partial x_2}
\]
Oppure in forma vettoriale:
\[
X = \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\quad 
nabla g(X) = \begin{bmatrix}
\frac{\partial g(x_1,x_2)}{\partial x_1} \\
\frac{\partial g(x_1,x_2)}{\partial x_2}
\end{bmatrix}
\]
\[X \leftarrow X + \alpha \nabla g(X)\]
Dove $\alpha$ è lo step size, cioè la dimensione del passo da fare:
\begin{itemize}
  \item Se è troppo grande si rischia di saltare soluzioni
  \item Se è troppo piccolo la convergenza sarà molto lenta
\end{itemize}

\subsubsection{Algoritmo di Newton-Raphson}
È una tecnica generale per trovare le radici di una funzione
cioè risolvere un'equazione $f(x) = 0$.
Per farlo si trova un'approsimazione iniziale $\bar{x}_0$ della soluzione
e iterativamente si aggiorna l'approissimazione usando la formula:
\[
\bar{x}_{n+1} = \bar{x}_n - \frac{f(\bar{x}_n)}{f'(\bar{x}_n)}
\]
dove:
\[
g'(x) = \frac{d}{dx} g(x)
\]
\ex{}
{
  Consideriamo la funzione $f(x) = x^2 - a$.
  \begin{itemize}
    \item Mostrare che il metodo di Newton conduce a:
    \[x_{n+1} = \frac{1}{2}\left(x_n + \frac{a}{x_n}\right)\]
    \item Fissato $a = 4, x_0 = 1$ cacolare le prime tre iterazioni. ($x_i = \{1,2,3\}$)
  \end{itemize}
  Quindi abbiamo \[
    f(x) = x^2 - a
  \] 
  \[x_{n+1} = \bar{x}_n - \frac{f(\bar{x}_n)}{f'(\bar{{x}_n})}
  \]
  \[x_{n+1} = \bar{x}_n - \frac{\bar{x}_n - a}{2\bar{x}_n}  \]
}

\subsection{Constrained Satisfaction Problem}

Un \textbf{Constrained Satisfaction Problem (CSP)} è un problema
definito da:
\begin{itemize}
  \item Un insieme di variabili \( X = \{X_1, X_2, \ldots, X_n\} \)
  \item Un insieme di domini \( D = \{D_1, D_2, \ldots, D_n\} \) dove ogni \( D_i \)
    è l'insieme dei valori possibili per la variabile \( X_i \)
  \item Un insieme di vincoli \( C = \{C_1, C_2, \ldots, C_m\} \) che specificano
    le relazioni tra le variabili
\end{itemize}
Assunzioni: single agent, azioni deterministiche, stato completamente osservabile

\ex{Map-Coloring}
{
  Il Map-coloring è un problema specifico di Graph coloring.
  Dato un insieme di regioni geografiche e un insieme di colori,
  assegnare un colore a ogni regione in modo che regioni adiacenti
  non abbiano lo stesso colore.
  \begin{itemize}
    \item Variabili: regioni geografiche (es. WA, NT, Q, NSW, V, SA, T)
    \item Domini: colori (es. rosso, verde, blu)
    \item Vincoli: regioni adiacenti non possono avere lo stesso colore
  \end{itemize}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{map-coloring.png}
  \end{figure}
}
\ex{N-Queens Problem}
{
  Il N-Queens Problem è un problema di posizionamento di N regine su una
  scacchiera \( N \times N \) in modo che nessuna regina possa attaccarne
  un'altra.
  \begin{itemize}
    \item Variabili: posizioni delle regine sulle righe della scacchiera
    \item Domini: colonne della scacchiera (es. 1, 2, ..., N)
    \item Vincoli: questa volta formuliamo i vincoli in maniera più complessa.
    Un vincolo per ogni coppia di variabili specificando le posizioni "permesse" PER OGNI ogni due regine.
  \end{itemize}
  Questa formulazione rende alcuni vincoli impliciti, per esempio, 
  non è possibile assegnare due regine la stessa colonna quindi
  non ci sta bisogno di controllare.
}
\subsubsection{Grafo dei vincoli}

Un \textbf{grafo dei vincoli} chiamato anche grafo primale
consiste nel costruire
un nodo per ogni variabile
e un arco per ogni vincoli tra due variabili.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{graphcoloring.png}
\end{figure}

\subsubsection{Ipergrafi e Grafi duali}

Le relazioni tra ipergrafi e grafi binari:
\begin{itemize}
  \item Si può sempre convertire un ipergrafo in un grafo binario
  \item Ogni variabile ha un dominio esponenzialmente grande
\end{itemize}

\subsubsection{Problemi combinatori}

Dato un insieme di possibili soluzioni bisogna trovare quella migliore 
che soddisfa i vincoli. Alcuni esempi:
\begin{itemize}
  \item Decisionali: colorare un grafo con $k$ colori
  \item Ottimizzazione: trovare la colorazione con il minor di conflitti
  \item Ottimizzazione Multi-obiettivo: portfolio investment, minimizzare il rischio e
  massimizzare il guadagno
  \item Modelli grafici:
  \begin{itemize}
    \item Insieme di variabili, domini e funzioni locali (vincoli)
    \item Funzioni globali è un aggregazione di funzioni locali
    \item Soluzioni: l'assegnamento di variabili che ottimizza la funzione globale 
  \end{itemize}
\end{itemize}

\dfn{Rete a vincoli}
{
  Una tupla di tre elementi CN = $(X, D, C)$ dove:
  \begin{itemize}
    \item $X = \{X_1, X_2, \ldots, X_n\}$ è un insieme di variabili
    \item $D = \{D_1, D_2, \ldots, D_n\}$ è un insieme di domini associati alle variabili
    \item $C = \{C_1, C_2, \ldots, C_m\}$ è un insieme di vincoli che specificano le relazioni tra le variabili
    \item Ogni vincolo $C_i$ è una tupla $(S_i, R_i)$ dove:
    \begin{itemize}
      \item $S_i \subseteq X$ è lo scopo, l'insieme delle variabili coinvolte in $R_i$
      \item $R_i$ sottoinsieme del prodotto cartesiano delle variabili in $S_i$
      \item $R_i$ specifica le tuple permesse su $S_i$
    \end{itemize}
    \item \textbf{Soluzione:} assegnamento di tutte le variabili che soddisfano tutti i vincoli.
    \item Obiettivo: consistency check, trovare una o tutte le soluzioni,
    ottimizzare una funzione obiettivo.
  \end{itemize}
} 
Ci si può avvicinare alla soluzione
attraverso una soluzione parziale:
\begin{itemize}
  \item Soluzione parziale consistente: soluzione parziale che soddisfa
  tutti i vincoli di cui lo scope non contiene variabili non assegnate
  \item Una soluzione parziale consistente non è necessariamente
  estendibile a una soluzione completa
\end{itemize}

\subsubsection{Tree Decomposition}

\dfn{Cycle Cutset}
{
  Dato un grafo non orientato, un sottoinsieme di nodi nel grafo è 
  un cycle cutset se la rimozione di questi nodi rende il grafo
  aciclico.
}
Il concetto è:
\begin{itemize}
  \item Una volta che una variabile viene assegnata può essere rimossa dal grafo 
  \item Se rimoviamo un cycle cutset allora il grafo rimanente è un albero
  \item Si può usare arc-consistency per risolvere l'albero rimanente
  \item Dobbiamo controllare ogni possibile assegnazione delle variabili del cycle cutset 
  e fare propagazione negli archi 
  \item La complessità è comunque esponenziale ma nella dimensione del cycle cutset.
\end{itemize}

\section{Logical Agents}

Perché costruire un agente basato sulla logica? 
Solitamente hanno due componenti:
\begin{itemize}
  \item \textbf{Knowledge base (KB):} insieme di proposizioni che rappresentano
    ciò che l'agente sa sul mondo
  \item \textbf{Inference engine:} meccanismo per dedurre nuove proposizioni
\end{itemize}
La knowledge base è un insieme di proposizioni in un linguaggio formale.
Un approccio dichiarativo per costruire un agente:
\begin{itemize}
  \item Dire cosa sa l'agente 
  \item Poi può chiedersi da solo cosa fare e le risposte dovrebbero seguire la knowledge base.
\end{itemize}
Gli agenti possono essere visti al livello di conoscenza i.e cosa sanno, 
non come sono implementati.
O a livello di implementazione, cioè come sono costruiti.
\begin{minted}{python}
function KB-Agent(percept) returns an action
    inputs: percept, a percept
    static: KB, a knowledge base, initially empty
            action, an action, initially null 
            t, a counter initially 0
    KB <- Tell(KB, Percept-To-Sentence(percept))
    action <- Ask(KB, Action-Sentence())
    KB <- Tell(KB, Action-To-Sentence(action))
    t <- t + 1
    return action
\end{minted}
\ex{Wumpus World PEAS}
{
  Un esempio famoso di agente logico è l'agente Wumpus World.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{wumpus.png}
  \end{figure}


  \begin{itemize}
    \item \textbf{Performance measure:} +1000 per uscita, -1 per ogni azione,
      -1000 per essere mangiati dal Wumpus o cadere in un buco
    \item \textbf{Environment:} 
    \begin{itemize}
      \item Celle adiacenti al wumpus sono maleodoranti
      \item Celle adiacenti a un buco sono ventose
      \item C'è scintillio se l'oro è nella stessa cella
      \item Sparare uccide il wumpus se si è rivolti verso di lui
      \item Sparare consuma la sola freccia
      \item Prendere raccoglie l'oro se si è nella stessa cella
      \item Rilasciare lascia cadere l'oro nella stessa cella
    \end{itemize}
    \item \textbf{Actuators:} muovi, gira a sinistra, gira a destra, spara,
      prendi oro, esci
    \item \textbf{Sensors:} brivido, puzza, scintilla, bump, urlo, sensore di
      glitter, sensore di impatto
  \end{itemize}
  Questo problema è:
  \begin{itemize}
    \item Osservabile? No—solo percezione locale
    \item Deterministico? Sì—i risultati sono esattamente specificati
    \item Episodico? No—sequenziale a livello di azioni
    \item Statico? Sì—Wumpus e buchi non si muovono
    \item Discreto? Sì 
    \item Single-agent? Sì—Wumpus è essenzialmente una caratteristica naturale
  \end{itemize}
}
\subsection{Logica in generale}

La logiche sono linguaggi formali per rappresentare informazioni
come per trarre conclusioni.
Come sappiamo la logica è divisa in:
\begin{itemize}
  \item \textbf{Sintassi:} definisce le frasi nel linguaggio
  \item \textbf{Semantica:} definisce il "significato" delle frasi;
    cioè definisce la verità di una frase in un mondo
\end{itemize} 
Un esempio è il linguaggio dell'aritmetica:
\begin{itemize}
  \item \( x + 2 \ge y \) è una frase; \( x2 + y > \) non è una frase
  \item \( x + 2 \ge y \) è vera se il numero \( x + 2 \) non è minore del numero \( y \)
  \item \( x + 2 \ge y \) è vera in un mondo dove \( x = 7, y = 1 \)
  \item \( x + 2 \ge y \) è falsa in un mondo dove \( x = 0, y = 6 \)
\end{itemize}

\subsubsection{Entailment}

L'entailment è una relazione tra frasi in un linguaggio logico e ha 
a che fare con i modelli non con la prova formale.
\[KB \models \alpha\]
Knowledge base \( KB \) entaila la frase \( \alpha \) se e solo se
$alpha$ è vera in ogni mondo dove $KB$ è vera.
\ex{}
{
Per esempio se $KB$ contiene "La Juventus ha vinto" e "Roma ha vinto"
allora $KB$ entaila "O la Juventus o Roma ha vinto".
Oppure se $x + y = 4$ allora $4 = x + y$.
Entailmente è una relazione tra frasi (cioè sintassi)
basata sulla semantica.
I computer sono molto bravi a processare regole sintattiche.
}

\subsubsection{Modelli}

I logici solitamente ragionano in termini di modelli, che sono formalmente 
strutturati in mondi rispetto alla verità che deve essere valutata. 
Diciamo che $m$ è un modello per una frase $alpha$ se $alpha$ è vera in $m$.
$M(\alpha)$ è l'insieme di tutti i modelli per $alpha$.
Allora $KB \models \alpha$ se e solo se $M(KB) \subseteq M(\alpha)$.
\ex{}
{
  Prendendo l'esempio di prima se $KB$ contiene "La Juventus ha vinto e la Roma ha vinto"
  allora $\alpha$ può essere "La Juventus ha vinto".
}

\subsection{Inferenza}

$KB \vdash_i \alpha$ vuol dire che $\alpha$ può essere derivata da $KB$ con una procedura $i$. 
Le conseguenze di $KB$ sono un pagliaio; $\alpha$ è un ago.
Entailmente = ago nel pagliaio; inferenza = trovarlo.
\begin{itemize}
  \item \textbf{Soundness:} $i$ è corretto (sound) se 
  dove $KB \vdash_i \alpha$ , è anche vero che $KB \models \alpha$
  \item \textbf{Completeness:} $i$ è completo se
  dove $KB \models \alpha$, è anche vero che $KB \vdash_i \alpha$
\end{itemize}
Preview: la logica del primo ordine è abbastanza espressiva
da poter dire quasi tutto ciò che ci interessa,
ed esiste una procedura di inferenza corretta e completa.
Quindi, la procedura risponderà a qualsiasi domanda la cui risposta
segua da ciò che è noto dalla KB.

\subsection{Logica proposizionale}

\subsubsection{Sintassi}

La logica proposizionale è il linguaggio logico più semplice. 
I simboli $P_1, P_2$ sono proposizioni atomiche.
\begin{itemize}
  \item Se $S$ è una proposizione, allora $\neg S$ è una proposizione
  \item Se $S_1$ e $S_2$ sono proposizioni, allora $S_1 \land S_2$, $S_1 \lor S_2$, 
    $S_1 \Rightarrow S_2$, $S_1 \Leftrightarrow S_2$ sono proposizioni
\end{itemize}

\subsubsection{Semantica}

Ogni modello specifica se qualcosa è vero o falso per ogni simbolo proposizionale.
Le regole per valutare rispetto ad un modello $M$ sono:
\begin{itemize}
  \item $\neq S$ è vero in $M$ se e solo se $S$ è falso in $M$
  \item $S_1 \land S_2$ è vero in $M$ se e solo se sia $S_1$ che $S_2$ sono veri in $M$
  \item $S_1 \lor S_2$ è vero in $M$ se e solo se almeno uno tra $S_1$ e $S_2$ è vero in $M$
  \item $S_1 \Rightarrow S_2$ è vero in $M$ se e solo se $S_1$ è falso in $M$ o $S_2$ è vero in $M$
  \item $S_1 \Longleftrightarrow S_2$ è vero in $M$ se e solo se entrambi sono veri in $M$
\end{itemize}

\ex{Wumpus World Sentences}
{
  Consideriamo $P_{i,j}$ sia vero se ci sta un pozzo nella cella $(i,j)$
  e $B_{i,j}$ sia vero se c'è del vento nella cella $(i,j)$.
  \begin{itemize}
    \item $R_1: \neg P_{1,1}$ (non c'è un pozzo nella cella (1,1))
    \item $R_2: \neg B_{1,1}$
    \item $R_3: B_{1,2}$
  \end{itemize}
  "Il pozzo è in una cella adiacente alla cella con vento":
  \[
    R_4: B_{1,1} \Leftrightarrow (P_{1,2} \lor P_{2,1})
  \]
  \[
    R_5: B_{1,2} \Leftrightarrow (P_{1,1} \lor P_{1,3} \lor P_{2,2})
  \]z
  Una cella è ventosa \textbf{se e solo se} c'è un pozzo in una cella adiacente.
}
\begin{minted}{python}
function TT-Entails(KB,a) returns true or false
  inputs: KB, the knowledge base, a sentence in prop. logic
  a, the query, a sentence in prop. logic
  symbols <- a list of the proposition symbols in KB and a
  return TT-Check-All(KB,a,symbols,[])  
\end{minted}
\noindent
Quello che fa questa funzione è quello di fare una DFS di ogni possibile
assegnazione, controllando se in ogni modello dove \( KB \) è vero
anche \( \alpha \) è vero. Ritornerà vero o falso in base a 
questo controllo. Un altro algoritmo è:
\begin{minted}{python}
function TT-Check-All (KB, alpha, symbols, model) return true or false
  if symbols is empty then
    if MODEL-IS-TRUE(KB, model) then
      return MODEL-IS-TRUE(alpha, model)
    else
      return true
  else
    P <- FIRST(symbols)
    rest <- REST(symbols)
    return TT-Check-All(KB, alpha, rest,
      EXTEND-MODEL(model, P, true)) and
      TT-Check-All(KB, alpha, rest,
      EXTEND-MODEL(model, P, false))
\end{minted}
\noindent
Questo algoritmo ci permette di verificare se \( KB \models \alpha \)
attraverso una ricerca in profondità di tutte le possibili assegnazioni
delle variabili proposizionali. La complessità di questo algoritmo è
\[O(2^n)\] dve \( n \) è il numero di simboli proposizionali ed è 
co-NP completo.

\subsection{Metodi di dimostrazioni}

I metodi di dimostrazione si dividono in due categorie:
\begin{itemize}
  \item \textbf{Model checking:} 
  \begin{itemize}
    \item Enumerazione delle tabelle di verità (sempre esponenziale per $n$)
    \item Migliora il backtracking (e.g DPLL)
    \item Ricerca con euristiche nello spazio dei modelli (sound ma non completo) 
  \end{itemize}
  \item \textbf{Applicazione delle regole d'inferenza:}
  \begin{itemize}
    \item Legittime (sound) generazioni di nuove formule da quelle vecchie
    \item Dimostrazione: una sequenza di applicazione di regole di inferenza (Può essere regole di inferenza come operatori).
    \item Tipicamente richiedono traduzioni delle formule in forme normali
  \end{itemize}
\end{itemize}
Due formule sono logicamente equivalenti quando sono vere negli stessi modelli.
\[\alpha \equiv \beta \iff \alpha \vDash \beta \land \beta \vDash \alpha\]
Ci sono diverse formule equivalenti utili:
\begin{itemize}
  \item \(\neg(\alpha \land \beta) \equiv \neg \alpha \lor \neg \beta\) (Legge di De Morgan 1)
  \item \(\neg(\alpha \lor \beta) \equiv \neg \alpha \land \neg \beta\) (Legge di De Morgan 2)
  \item \(\alpha \Rightarrow \beta \equiv \neg \alpha \lor \beta\)
  \item \(\alpha \Leftrightarrow \beta \equiv (\alpha \Rightarrow \beta) \land (\beta \Rightarrow \alpha)\)
  \item \(\neg(\alpha \Rightarrow \beta) \equiv \alpha \land \neg \beta\)
  \item \(\neg(\alpha \Leftrightarrow \beta) \equiv (\alpha \land \neg \beta) \lor (\neg \alpha \land \beta)\)
  \item \(\alpha \lor (\beta \land \gamma) \equiv (\alpha \lor \beta) \land (\alpha \lor \gamma)\) (Distributività)
  \item \(\alpha \land (\beta \lor \gamma) \equiv (\alpha \land \beta) \lor (\alpha \land \gamma)\) (Distributività)
\end{itemize}
Una proposizione è valida se è vera in ogni modello.
\[\top \quad \alpha \lor \neg \alpha \quad A \implies A \quad (A \land (A \implies B)) \implies B\]
La validità è connessa all'inferenza attraverso il teorema della deduzione.
\[KB \vDash \alpha \iff KB \implies \alpha \text{ è valida}\]
Una proposizione è soddisfacibile se è vera in almeno un modello.
\[A \lor B \quad C\]
Una proposizione è insoddisfacibile se non è vera in nessun modello.
\[\bot \quad A \land \neg A\]
La soddisfacibilità è connessa all'inferenza attraverso il seguente fatto:
\[KB \vDash \alpha \iff KB \land \neg \alpha \text{ è insoddisfacibile}\]
i.e dimostrare $\alpha$ attraverso la Reductio ad Absurdum.

\subsection{Sistema di inferenza}

È un insieme di regole di inferenza:
Le regole di inferenza sono scritte come:
\[
\frac{\text{premesse}}{\text{conclusione}}
\]
\[\frac{A_1 \dots A_k}{A}\]
Una derivazione $A$ è derivato da un insieme di formule $\Gamma$ 
se con il mio sistema di inferenza $R'$ esiste 
una sequenza $A_1, \dots, A_n$ tale che:
\begin{itemize}
  \item A è $A_n$
  \item $\forall i \in \{1 \dots n\}$ uno dei seguenti è vero:
  \begin{itemize}
    \item $A_i \in \Gamma$
    \item $A_i$ è derivazione diretta di $A_j$ con $j < i$ usando una regola di inferenza in $R'$
  \end{itemize}
\end{itemize}
La sequenza $A_1, \dots, A_n$ è chiamata dimostrazione di $A$ da $\Gamma$.
$\Gamma$ sono le premesse di $A$. 

\subsubsection{Proprietà dei sistemi di inferenza}

La \textbf{correttezza} delle regole di inferenza significa che le conclusioni 
sono conseguenze logiche delle premesse $\psi_1, \dots, \psi_n \vDash \psi$.
La \textbf{completezza} ci dice che se $H \vDash \psi$ allora esiste una derivazione di $\psi$ da $H$.
Si può arrivare alla completezza dicendo: esiste una derivazione della contraddizione se $H \cup \{\neg \psi\}$ non è soddisfacibile.
L'algoritmo che andremo a vedere è basato su questo principio. \\
Dimostrare che $\Gamma \vDash A$ per: 
\begin{itemize}
  \item Dimostrazione per refutazione (reductio ad absurdum): provare
  che $\Gamma \land \neg A$ non è soddisfacibile.
  L'algoritmo si chiama \textbf{Resolution} che ha bisogno di CNF (Conjunctive Normal Form) ed è completo.
  \item Esiste anche il \textbf{Forward/Backward reasoning} sound, completo e anche polinomiale ma SOLO per 
  una parte ristretta di logica proposizionale chiamata Horn clauses.
\end{itemize}
La forma normale congiuntiva (CNF) è una congiunzione di clausole,
dove una clausola è una disgiunzione di letterali e.g $(A \lor \neg B \lor C) \land (\neg A \lor D)$.
La \textbf{resoluzione} è una regola di inferenza per CNF completa per la logica proposizionale.
\[
\frac{l_1 \lor \dots \lor l_k \quad m_1 \lor \dots \lor m_n}
{l_1 \lor \dots \lor l_{i-1} \lor l_{i+1} \lor \dots \lor l_k \lor m_1 \lor \dots \lor m_{j-1} \lor m_{j+1} \lor \dots \lor m_n}
\]
dove $l_i$ e $m_j$ sono letterali complementari. La resoluzione è corretta e completa per la logica proposizionale.
\subsubsection{Conversione in CNF}
Partiamo da una formula qualsiasi e la convertiamo in CNF seguendo questi passi:

\[B_{1,1} \iff P_{1,2} \lor P_2,1\]
\begin{itemize}
  \item Elimina $\iff$, rimpiazza $alpha \iff beta$ con $(alpha \Rightarrow beta) \land (beta \Rightarrow alpha)$:
  \[(B_{1,1} \Rightarrow P_{1,2} \lor P_{2,1}) \land (P_{1,2} \lor P_{2,1} \Rightarrow B_{1,1})\]
  \item Elimina $\Rightarrow$, rimpiazza $alpha \Rightarrow beta$ con $\neg alpha \lor beta$:
  \[(\neg B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land (\neg (P_{1,2} \lor P_{2,1}) \lor B_{1,1})\]
  \item Muovi $\neg$ verso le proposizioni atomiche usando le leggi di De Morgan e la doppia negazione:
  \[(\neg B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land ((\neg P_{1,2} \land \neg P_{2,1}) \lor B_{1,1})\]
  \item Applica la distributività e appiattisci:
  \[(\neg B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land (\neg P_{1,2} \lor B_{1,1}) \land (\neg P_{2,1} \lor B_{1,1})\]
\end{itemize}
Come possiamo notare alla fine abbiamo ottenuto una formula in CNF, ovvero una congiunzione di disgiunzioni.
\subsubsection{Algoritmo di Resolution}

Dimostrazione per assurdo ovvero mostrare che $KB \land \neg \alpha$ è insoddisfacibile.
\begin{minted}[fontsize=\small]{python}
function PL-Resolution (KB, alpha) returns true or false
  inputs: KB, the knowledge base, a sentence in prop. logic
          alpha, the query, a sentence in prop. logic
  clauses <- CNF(KB and not alpha)
  new <- {}
  repeat
    for each pair of clauses (Ci, Cj) in clauses do
      resolvents <- PL-Resolve(Ci, Cj)
      if resolvents contains the empty clause then
        return true
      new <- new union resolvents
    if new contains clauses then
      return false
    clauses <- clauses union new
\end{minted}
Per alcuni casi dove potremmo ottenere la clausola vuota, devo decidere quale risolvente generare:
\[
\frac{P \lor \neg Q \quad \quad \neg P \lor Q}{\neg Q \lor Q}
\quad \text{oppure} \quad 
\frac{P \lor Q \quad \quad \neg P \lor \neg Q}{P \lor \neg P}
\]
Questo algoritmo però se gli viene data una formula 
che non è soddisfacibile, nel momento in cui genera tutti i possibili risolventi 
deve poter generare una contraddizione per potersi fermare.
\ex{Esempio di risoluzione dove la risoluzione $KB \; \cancel{\vDash} \; \alpha$}
{
  \[\alpha = P_{1,2} \quad KB = B_{1,1} \iff (P_{1,2} \lor P_{2,1}) \land \neg B_{1,1}\]
  Traduciamo $KB$ in CNF:
  \[(\neg B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land (\neg P_{1,2} \lor B_{1,1}) \land (\neg P_{2,1} \lor B_{1,1}) \land \neg B_{1,1} \land P_{1,2}\]
  A queste assunzioni aggiungiamo $\neg \alpha$ ovvero $\neg P_{1,2}$. Generiamo le seguenti clausole:
  \begin{itemize}
    \item $\neg B_{1,1} \lor P_{1,2} \lor B_{1,1}$
    \item $P_{1,2} \lor \neg P_{1,2} \lor P_{2,1}$
    \item $\neg B_{1,1} \lor P_{2,1} \lor B_{1,1}$
    \item $\neg P_{2,1} \lor P_{2,1} \lor P_{1,2}$
    \item $\neg P_{2,1}$
  \end{itemize}
  Alcune clausole vengono generate mettendo insieme le clausole originali con quelle generate.
  \begin{itemize}
    \item $\neg B_{1,1} \lor P_{1,2}$ 
    \item $\neg P_{2,1} \lor P_{1,2}$
  \end{itemize}
  A questo punto non possiamo più generare nuove clausole e non abbiamo generato la contraddizione. 
  L'algoritmo quindi non riesce a terminare e nota che abbiamo generato le stesse formule quindi $KB \; \cancel{\models} \; \alpha$.
}

\subsection{Forward and Backward Chaining}

\subsubsection{Horn Clauses}

Una \textbf{Horn clause} è:
\begin{itemize} 
  \item Una proposizione atomica, oppure
  \item Una disgiunzione di letterali con al massimo un letterale positivo
\end{itemize} 
Per esempio: $C \land (B \implies A) \land (C \land D \implies B)$.
Possiamo ancora fare il modus ponens, completo per Horn clauses.
\[
\frac{\alpha_1, \dots, \alpha_n \quad \quad \alpha_1, \dots, \alpha_n \implies B}
{\beta}
\]
Può essere usato per il \textbf{forward chaining} e il \textbf{backward chaining}.
Questi algoritmi sono molto naturali e possono essere eseguiti in tempo lineare.
Idea: Usa qualsiasi regola di cui le premesse sono soddisfatte in $KB$ e aggiungi la sua conclusione a $KB$, fino a quando la query è trovata.
\[
\begin{aligned}
P &\implies Q\\
L \land M &\implies P\\
B \land L &\implies M\\
A \land P &\implies L\\
A \land B &\implies L\\
A\\
B
\end{aligned}
\]
\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\textwidth]{forwardchaining.png}
\end{figure}
\begin{minted}{python}
function PL-FC-Entails(KB, q) return true or false
  inputs: KB, the knowledge base, a set of Horn clauses
          q, the query, a proposition symbol
  count[p] = number of premises in rules with conclusion p
  inferred[p] = false for each proposition symbol p
  agenda = all proposition symbols known to be true in KB
  while agenda is not empty do
    p = POP(agenda)
    if p == q then return true
    if inferred[p] == false then
      inferred[p] = true
      for each horne_clause in KB where p is one of the pi do
        count[r] = count[r] - 1
        if count[r] == 0 then
          PUSH(r, agenda)
  return false
\end{minted}

\begin{enumerate}
  \item FC arriva ad un punto fisso dove non vengono derivate nuove formule atomiche
  \item Consideriamo lo stato finale come un modello m, assegnando vero/falso ai simboli
  \item Ogni clausola nella KB originale è vera in m, per assurdo supponiamo che una clausola
  \( a_1 \land \dots \land a_k \implies b \) sia falsa in m. Allora \( a_1 \land \dots \land a_k \) è vera in m e b è falsa in m.
  Quindi l'algoritmo non ha raggiunto un punto fisso!
  \item Quindi m è un modello di KB
  \item Per ogni a (formula atomica), se \( KB \vDash a \), a è vera in ogni modello di KB, incluso m
\end{enumerate}
Idea generale: costruire un qualsiasi modello di KB attraverso inferenza corretta, controllare $\alpha$. 
Per il back chaining: lavora al contrario dalla query $q$. 
Per provare $q$ per BC, controlla se $q$ è noto in $KB$ o 
prova per BC le premesse di qualche regola includendo $q$.
Evita loop: cerca se il goal è già nella pila dei goal attivi.
Evita lavoro ripetuto: controlla se il nuovo subgoal:
\begin{itemize}
  \item È stato già provato 
  \item È già fallito
\end{itemize}
FC è \textit{data driven} mentre BC è \textit{goal driven}.

\section{Rappresentare l'incertezza}
\ex{}
{
Consideriamo un'azione $A_t$ = lasciare l'aeroporto $t$ minuti prima del volo.
$A_t$ mi permetterà di arrivare in tempo?
Problemi: 
\begin{itemize}
  \item Osservabilità parziale (non so se c'è traffico, incidenti, ecc)
  \item Sensori rumorosi (report del traffico)
  \item Incertezza sulle conseguenze delle azioni (tempo di viaggio variabile)
  \item Complessità immensa per modellare e predirre il traffico 
\end{itemize}
Quindi un approccio puramente logico potrebbe:
\begin{itemize}
  \item Rischia della falsità: "$A_{25}$ mi porterà in tempo"
  \item Porta alla conclusione dche sono troppo debole per la scelta delle decisioni
\end{itemize}
}
Ci sono dei metodi per gestire l'incertezza:
\begin{itemize}
  \item Logica non monotonica:
  \begin{itemize}
    \item Assumo che la mia macchina non ha problemi alle gomme 
    \item Assumo $A_{25}$ funziona se non ho contraddizioni nelle prove 
    \item Problemi: Quali assunzioni sono ragionevoli? Come gestire la contraddizione?
  \end{itemize}
  \item Fuzzy logic: gestisce gradi di verità NON incertezza come:
  \begin{itemize}
    \item WETGRASS è vera al 70\%
  \end{itemize}
  \item Probabilità:
  \begin{itemize}
    \item Data una prova disponibile, $A_{25}$ arriveremo in tempo con una probabilità $0.04$.
  \end{itemize}
\end{itemize}
\subsection{Probabilità}
La probabilità misura il grado di credenza in una proposizione.
Questa riassume effetti come:
\begin{itemize}
  \item Laziness: fallire ad enumerare l'eccezioni
  \item Ignoranza: non so se c'è traffico, condizioni iniziali
\end{itemize}
La probabilità \textbf{Soggettiva} o \textbf{bayesiana}:
Le probabilità mettano in relazione delle propozioni con lo stato di conoscenza di un'altra propozione. 

\section{Markov Decision Process}

\subsection{POMPD (Partially Observable Markov Decision Process)}
Un POMDP è una generalizzazione di un MDP per ambienti parzialmente osservabili.
POMPD ha un modello di ossercazione $O(s,e)$ 
definisce la probabilità che l'agente ottenga l'evidenza $e$
in uno stato $s$.
Se l'agente non sa in quale stato si trova, allora non ha senso parlare di policy $\pi(s)$.
\thm{Teorema di Astrom}
{
  La policy ottimale in una POMDP è una funzione $\pi(b)$ dove $b$
  è la \textbf{belief state} dell'agente, cioè una distribuzione di probabilità.
}
\noindent
Posso convertire una POMPD in un MDP con stati che sono belief states,
dove $T(b,a,b')$ è la probabilità 
che il nuovo belief state sia $b'$ dato il vecchio belief state $b$ e l'azione $a$. 


\section{Machine Learning: introduzione e regressione lineare}

Automaticamente costruisce modelli secondo alcuni dati ed esistono diversi tipi
di apprendimento:
\begin{itemize}
  \item \textbf{Supervised learning:} ($y = f(x)$) dato (x,y) impara $f()$ (approssimazione di funzione)
  \ex{Supervised Learning: riconoscere cifre scritte a mano}
{
  L'idea è di avere un training set:
  \begin{itemize}
    \item Immagini conosciute che rappresentono le cifre. (x)
    \item labels che rappresento il valore della cifra. (t)
  \end{itemize}
  Il \textbf{training} costruisce il modello che mappa le immagini a cifre y(x).
  Il \textbf{test set} vengono date nuove mai viste prime immagini senza etichette. 
  E il \textbf{testing }applica il modello al testing set.
}
  \item \textbf{Unsupervised learning:} dato x impara $f()$ come rappresentazione compatta di $x$ (Clustering)
  
  \ex{Unsupervised Learning: gruppe di celle basati sull'espressione del gene}
  {

     \begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth]{unsupervised.png}
     \end{figure}
     \noindent
     Il clustering è un esempio di unsupervised learning dove abbiamo una matrice dove:
    \begin{itemize}
      \item Righe: geni 
      \item Colonne: celle 
      \item Entry (gene, cella) = livello di espressione del gene nella cella
      \item nessuna etichetta 
      \item obiettivo: trovare gruppi di geni e celle con modelli simili
      \begin{itemize}
        \item Per celle (colonne) 
        \item Per geni simili 
        \item Per entrambi (bi-clustering)
      \end{itemize}
    \end{itemize}
  }

  \item \textbf{Reinforcement learning:} dato $x,z$ impara $f()$ per generare $y$ ($x$ è lo stato, $z$ è la ricompensa)
  \ex{Pianificare traiettorie con un braccio robotico}
  {
    \begin{itemize}
      \item Controllo del problema (decision making sequenziale)
      \item Segnale di ricompensa che guida le azioni del robot (alta ricompensa quando l'obiettivo viene raggiunto)
      \item Ha bisogno di interazione con l'ambiente 
      \item Usa tecniche statiche orientati ai dati ma si concentra sul controllo non sull'analisi dei dati.
    \end{itemize}
  }
\end{itemize}

\subsection{Concetti di base e terminologie}

Variabili di input:
\begin{itemize}
  \item Varaibili indipendenti, predictors, features
  \item $X$ quando ho più variabili $X_1, X_2, \dots, X_n$
\end{itemize}
Variabili di output:
\begin{itemize}
  \item Risposta, variabile dipendente 
  \item $Y$
\end{itemize}
Variabili quantitative: (valori numerici)
\begin{itemize}
  \item Temperatura, altezza, guadagno, etc. 
\end{itemize}
Variabili qualitativi: (categorie)
\begin{itemize}
  \item Genere, brand del prodotto etc.
\end{itemize}
\begin{center}
  Regressione: output quantitativi\\
  Classificazione: output qualitativi
\end{center}

\subsubsection{Perché voglio stimare f(): Predizione}

Dato un set di input facili da ottenere $X = X_1, X_2, \dots, X_n$,
predirre l'output $Y$ che è costoso da misurare $Y = f(X) + \varepsilon$
dove $\varepsilon$ è il rumore (errore casuale indipendente da $X$) e 
$f(\dot)$ rappresenta informazione sistematica tra $X$ e $Y$.
\dfn{Predizione}
{
  La predizione costruisce un modello $\hat{f}(\dot)$ per calcolare 
  $\hat{Y} = \hat{f}(X)$ dato $X$. 
}
\noindent
L'obiettivo è minimizzare l'errore di predizione.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{prediction.png}
\end{figure}


L'accuratezza di una $\hat{Y}$ dipende da due quantità:
\begin{itemize}
  \item \textbf{Errore riducibile:} errore nella costruzione del modello 
  \item \textbf{Errore irriducibile:} variabilità di $\varepsilon$ (errore randomico indipendente da $X$).
\end{itemize}
Assumere $\hat{f}(\bullet)$ e $X$ fissati:
\[
E[(Y - \hat{Y})^2] = E[\overbrace{f(X) + \varepsilon}^{Y} - \overbrace{\hat{f}(X)}^{\hat{Y}}]^2 = \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Errore riducibile}} + \underbrace{Var(\varepsilon)}_{\text{Errore irriducibile}}
\]
\begin{itemize}
  \item $E(X)$ con $X$ come variabile aleatoria, è il valore atteso o media di $X$.
  \item $Var(X) = E[(X - E(X))^2]$ è la varianza di $X$.
\end{itemize}

\subsubsection{Perché voglio stimare f(): Inference}

Obiettivo: capire come $Y$ varia quando varia $X$ (non predirre).
Dobbiamo conoscere la forma di $f(\bullet)$ non una \textbf{black box}.
Possibili domande per l'inferenza:
\begin{itemize}
  \item Quali variabili indipendenti sono associati con la risposta? (il punto chiave è l'interpretabilità)
  \item Quale relazione tra la risposta e ogni variabile indipendente? (effetti positivi o negativi)
  \item Come possiamo codificare la relazione tra $X$ e $Y$ con modello lineare? (modelli lineari sono semplici e facili da usare ma hanno limitazioni sul potere rappresentativo)
\end{itemize}

\ex{Come stimare $f$?}
{
  Abbiamo:
  \begin{itemize}
    \item I dati di training: $n = 30$ data points 
    \item numero di predittori: $p = 2$
  \end{itemize}
  su anni di educazione e anzianità.
  \begin{itemize}
    \item $x_{ij}$ è il valore della osservazione $i$ del predittore $j$ dove $i = 1, \dots, n$ e $j = 1, \dots, p$.
    \item $y_{ij}$ è il valore della risposta per osservazione $i$.
  \end{itemize}
  Il training set: $(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)$
  dove $x_i = (x_{i1}, x_{i2}, \dots, x_{ip})^T$.\\
  \textbf{Obiettivo}: trovare la funzione $\hat{f}(\bullet)$ tale che 
  $Y \approx \hat{f}(X)$ per ogni osservazione nel training set.
}

\subsubsection{Termini parametrici}

Esiste una procedura a due passi:
\begin{itemize}
  \item Fare un assunzione sulla forma di $f$ (modello parametrico)
  \begin{itemize}
    \item esempio: assumi $f(\bullet)$ sia lineare
    \item $f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p$
    \item Abbiamo $p + 1$ parametri $\beta_0, \beta_1, \dots, \beta_p$
    \item Punto chiave: un modello è completamente identificato da parametri.
  \end{itemize}
  \item Trovare una procedura per capire quali sono i valori di $\beta$. 
  \begin{itemize}
    \item Trovare un modello lineare per stimare i parametri $\beta$.
    \item Trovare i valori per cui i parametri:
    \[Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p\]
    \item \textbf{Least Square} è l'approccio più comune per allenare modelli lineari.
  \end{itemize}
\end{itemize}
\ex{}
{
  Modello \textbf{guadagno} $\approx \beta_0 + \beta_1\text{educazione} + \beta_2\text{anzianità}$.
  I minimi quadrati stimano i parametri $\beta_0, \beta_1, \beta_2$.
  \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{linearegression.png}
  \end{figure}
}

\subsubsection{Metodi non parametrici}

L'idea è di non fare assunzioni su $f(\bullet)$.
Si cerca di fittare $f(\bullet)$ il più possibile sui dati di allenamento.
\begin{itemize}
  \item \textbf{Pro:} posso approssimare i dati di allenamento molto bene (nessuna assunzione sul modello)
  \item \textbf{Contro:} rischio di overfitting (non generalizza bene su dati mai visti prima)
\end{itemize}

\ex{}
{
  Piano "sottile" per estimare $f$ usando \textit{smooth spline} e \textit{rough spline}.
  \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{nonparam.png}
  \end{figure}
  Quella migliore è la smooth spline perché riesce a catturare il trend generale
  senza overfittare i dati di training.
}

\section{Machine Learning}

\subsection{Transformer: neural network + attention}

L'\textit{attention} è un meccanismo che permette 
al modello di focalizzarsi su parti specifiche dell'input
quando genera l'output.


\subsection{Reinforcement Learning}

Reinforcement learning: impara come mappare situazioni ad azioni 
così da poter massimizzare una ricompensa numerica cumulativa.
I concetti chiave dei RL sono: 
\begin{itemize}
  \item Trial and error mentre interagisce con l'ambiente
  \item Reward "in ritardo" (le azioni hanno effetto nel futuro)
\end{itemize}
Essenzialmente, dobbiamo stimare il valore a lungo termine 
di $V(s)$ e trovare $\pi(s)$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{RL.png}
\end{figure}

\subsubsection{Relazioni con gli MDP}

Guidare una MDP senza sapere le dinamiche:
\begin{itemize}
  \item Non conosce quali sono gli stati buoni/cattivi (no $R(s,a,s')$)
  \item Non sa a cosa portano le azioni (no $T(s,a,s')$)
  \item Quindi dobbiamo provare azioni/stati e ottenere le ricompense
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{recyclingbot.png}
  \caption{Esempio di un robot che impara a riciclare}
\end{figure}

\subsubsection{Come usare un modello?}

Ci sono due approcci:
\begin{itemize}
  \item \textbf{Metodi Model-based} metodi per provare ad imparare il modello:
  \begin{itemize}
    \item evitare di ripetere stati e azioni "cattivi"
    \item meno passi di esucuzione 
    \item uso efficiente dei dati
  \end{itemize}
  \item \textbf{Metodi Model-free}  per impare la $Q$-funzione e la policy direttamente:
  \begin{itemize}
    \item Semplicità, nessun bisogno di costruire ed usare un modello 
    \item nessun bias nella progettazione del modello
  \end{itemize}
\end{itemize}

\ex{Età attesa, Model Based vs Model Free }
{
  Obiettivo: calcolare l'eta attesa per questa classe.
Data la distribuzione di probabilità dell'età:
\[
  E[A] = \sum_a a \cdot P(a)
\]
\begin{itemize}
  \item \textbf{Model based}: stima $\hat{P}(a)$.
  \begin{itemize}
    \item $\hat{P}(a) = \frac{num(a)}{N}$
    \item $E[A] = \sum_a a \cdot \hat{P}(a)$
    \item dove $num(a)$ è il numero di studenti con età $a$ e $N$ è il numero totale di studenti
    \item Funziona perché abbiamo imparato il modello corretto 
  \end{itemize}
  \item \textbf{Model free:} nessuna stima
  \begin{itemize}
    \item $E[A] \approx \frac{1}{N} \sum_{i=1}^{N} a_i$
    \item dove $a_i$ è l'età dello studente $i$
    \item funziona perché ogni campoione appare con la giusta frequenza
  \end{itemize}
\end{itemize}
\noindent
L'idea generale: 
\begin{itemize}
  \item Stima $P(x)$ dai campioni.
  \begin{itemize}
    \item Ottieni campioni $x_i \tilde P(x)$
    \item Stima $\hat{P}(x) = count(x) / k$
  \end{itemize}
  \item Stima $\hat{T}(s,a,s')$ dai campioni:
  \begin{itemize}
    \item Ottieni i campioni $s_0, a_0, s_1, a_1, \dots, s_k$
    \item Stima $\hat{T}(s,a,s') = count(s,a,s') / count(s,a)$
    \item Funziona perché i campioni appaiono con la giusta frequenza 
  \end{itemize}
\end{itemize}
}

\ex{Imparare il modello per il robot che ricicla}
{
  Guardate l'immagine del robot che ricicla e dati i seguenti epsiodi di apprendimento:
  \[ 
    \begin{aligned}
      E_1 &: (L,R,H,0), (H,S,H,10), (H,S,L,10) \\
      E_2 &:  (L,R,H,0), (H,S,L,10), (L,R,H,10)\\
      E_3 &: (H,S,L,0), (L,R,H,10), (H,S,L,10) \\
    \end{aligned}
  \]
  Stima $T(s,a,s')$ e $R(s,a,s')$.
  Se voglio stimare $T(L,R,H)$:
  \[
    T(L,R,H) = \frac{count(L,R,H)}{count(L,R)} = \frac{3}{4} = 0.75
  \]
  Mentre se voglio stimare i reward $R(L,R,H)$:
  \[
    R(L,R,H) = \frac{r_1 + r_2 + r_3}{count(L,R,H)} = \frac{0 + 10 + 10}{3} = \frac{20}{3} \approx 6.67
  \]
}
\noindent

\subsubsection{Metodi Model-based}

Il seguente, è un algoritmo model-based per RL:

\begin{figure}[H] 
  \centering
  \begin{minipage}{0.8\textwidth} % Optional: controls the width of the algorithm
      \begin{algorithm}[H] % [H] also works inside the algorithm environment
          \caption{Model Based approach to RL}
          \begin{algorithmic}[1]
              \REQUIRE $A, S, S_0$
              \ENSURE $\hat{T}, \hat{R}, \hat{\pi}$
              \STATE Initialize $\hat{T}, \hat{R}, \hat{\pi}$
              \REPEAT
                  \STATE Execute $\hat{\pi}$ for a learning episode
                  \STATE Acquire a sequence of tuples $\langle s, a, s', r \rangle$
                  \STATE Update $\hat{T}$ and $\hat{R}$ according to tuples $\langle s, a, s', r \rangle$
                  \STATE Given current dynamics, compute a policy $\hat{\pi}$ (e.g., VI or PI)
              \UNTIL{termination condition is met}
          \end{algorithmic}
      \end{algorithm}
  \end{minipage}
\end{figure}

\subsubsection{Model-free Reinforcement Learning}

\begin{itemize}
  \item Vogliamo calcolare il peso atteso da $P(x)$
  \[  
    E[f(x)] = \sum_x f(x) \cdot P(x)
  \]
  \item Stima Model-based $P(x)$ dai campioni e poi calcolare:
  \[ 
    x_i \tilde P(x), \hat{P}(x) = num(x)/N, E[f(x)] \approx \sum_x f(x) \cdot \hat{P}(x)
  \]
  \item Stima model-free lo fa direttamente sui campioni:
  \[
    x_i \tilde P(x), E[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i)
  \]
\end{itemize}
\noindent
Obiettivo: calcolare il valore della funzione data una policy $\pi$
\begin{itemize}
  \item Media di tutti i campioni osservati
  \item Esegui $\pi$ per alcuni episodi di apprendimento
  \item Calcola la somma delle ricompense (scontate) ogni volta che uno stato viene visitato
  \item Calcola la media sui campioni raccolti
\end{itemize}



\end{document}