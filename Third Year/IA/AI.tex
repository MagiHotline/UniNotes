\documentclass[a4paper]{article}
\usepackage{import}
\input{../../setup.sty}

\onehalfspacing
\title{Intelligenza Artificiale}
\author{Università di Verona\\Imbriani Paolo -VR500437\\Professor Alessandro Farinelli}

\begin{document}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{../UniversityofVerona.png}
    \label{fig:centered-image}
\end{figure}

\maketitle

\pagebreak

\tableofcontents

\pagebreak

\section{Introduzione}

Alle origini dell'intelligenza artificiale vi è un bisogno diverso da quello che abbiamo oggi.
Alan Turing, negli anni 50 si era chiesto se le macchine potessero pensare, creando un test famoso ancora ora 
come "test di Turing" dove un interrogatore umano si deve interfacciare con un umano e una macchina 
e doveva capire chi dei due fosse chi. 
Nel 1956 ci fu uno studio fatto da il progetto di ricerca di Dartmouth, che aveva l'intento
di risolvere compiti che richiedeva l'intelligenza di una persona attraverso una macchina,
comprendendo che le \textit{anche le macchine possono imparare}.
La definizione più "accettata" di Intelligenza Artificiale è quella dove viene vista come una 
complessa e affascinante \textit{disciplina} che studia come simulare l'intelligeza in scenari complessi usando come
strumenti agenti autonomi per delle task ripetitive, sporche e pericolose che sfruttano l'analisi dei dati
(predizione e classificazione).

\dfn{}
{
    L'intelligenza artificiale è una disciplina che studia come \textbf{simulare} l'intelligenza
    umana in scenari complessi.
}
\noindent
Bisogna distinguere machine learning e programmazione:
\begin{itemize}
    \item \textbf{Programmazione}: macchine programmate per ogni task che devono eseguire (il concetto chiave
    è \textbf{il programma})
    \item \textbf{Machine Learning}: insegnare alla macchina (attraverso esempi) come risolvere task più complesse (il concetto chiave 
    è il \textbf{modello})
\end{itemize}

\subsection{Machine Learning}
L'idea di far apprendere una macchina si possono dividere in tre paradigmi contraddisti:
\begin{itemize}
    \item Unsupervised learning
    \item Supervised learning
    \item Reinforcement learning
\end{itemize}
Esistono poi i trasformatori, che sono modelli di machine learning probabilistici che si basano sul concetto di attenzione, che sono alla base di modelli come GPT.
Il concetto dell'attenzione è quello di dare più importanza ad alcune parole rispetto ad altre in un contesto, per esempio in una frase.
La potenza di questi trasformatori è che riescono a fare un'analisi del contesto molto più profonda rispetto ai modelli precedenti, 
permettendo di fare analisi di immagini come per esempio riconoscere oggetti in un'immagine o riconoscere dove è presente l'acqua
all'interno di una foto.

\subsection{Agenti intelligenti}

Un agente intelligente è un'entità che percepisce il suo ambiente attraverso dei sensori e agisce su di esso attraverso degli attuatori.
\begin{itemize}
    \item Percepisce l'ambiente attraverso dei \textbf{sensori}
    \item Agisce sull'ambiente attraverso degli \textbf{attuatori}
    \item Ha un \textbf{obiettivo} da raggiungere
\end{itemize}
Come dovrebbe comportarsi un agente intelligente?
\begin{itemize}
    \item \textbf{Razionale}: agisce per massimizzare il raggiungimento dell'obiettivo
    \item \textbf{Performance measure}: misura di quanto bene l'agente sta raggiungendo l'obiettivo
\end{itemize}
Quando vogliamo ragionare sul Reinforcement Learning, è utilire usare il \textit{Markov Decision Process}.
\dfn{}
{
    Un \textbf{Markov Decision Process (MDP)} è una tupla $(S, A, P, R)$ dove:
    \begin{itemize}
        \item $S$ è un insieme di stati
        \item $A$ è un insieme di azioni
        \item $P(s'|s,a)$ è la probabilità di transizione dallo stato $s$ allo stato $s'$ eseguendo l'azione $a$
        \item $R(s,a,s')$ è la ricompensa ottenuta eseguendo l'azione $a$ nello stato $s$ e transizionando nello stato $s'$
    \end{itemize}
}
\noindent
Poi si ha la \textit{policy} che è una funzione che mappa uno stato in un'azione. 

\section{Risolvere problemi con la ricerca}

\subsection{Agents and enviroments}

Gli agenti includono umani, robot, softbot, termostati, ecc. La funzione
agente mappa la storia delle percezioni in azioni.
\[f : \mathcal{P}^* \mapsto A\]
Il \textit{programma dell'agente} viene eseguito su un'architettura fisica che produce
$f$.

\ex{}
{
    Immaginiamo di avere un agente aspirapolvere che percepisce
    il luogo e i suoi contenuti.
    \begin{itemize}
        \item \textbf{Percezioni}: bump, Dirty e location (A o B)
        \item \textbf{Azioni}: left, right, suck, noOp
    \end{itemize}
    un esempio di sequenza percepita potrebbe essere:
    \[(A, Dirty), Suck, (A, Dirty), Suck, (A, Clean), Right\] \[ 
    (B, Dirty), Suck, (B, Clean), Left, (A, Clean), NoOp\]
    Cosa fa la funzione \textit{Right}? Può essere implementata
    in un piccolo programma agente?
    Se un agentes ha $|\mathcal{P}|$ possibili percezioni, quante
    "entries" avrà la tabella della funzione agente dopo $T$ time steps?
    \[\sum_{t=1}^{T} |P|^t\]
    L'obiettivo dell'IA è quello di progettare \textbf{piccoli}
    programmi agenti che permettono di rappresentare grandi funzioni agenti.
}
\noindent
\begin{minted}{python}
function Reflex-Vacuum-Agent([location,status]) returns an action
    if status = dirty then return suck
    else if location = A then return right
    else if location = B then return left
\end{minted}
\noindent

\subsubsection{Multi-robot Patrolling}

\ex{}
{
    Considerate il seguente ambiente:
    \begin{itemize}
        \item Tre stanze (A,B,C) e due robot ($r_1, r_2$)
        \item $r_1$ può pattugliare $A$ e $B$, $r_2$ può pattugliare $B$ e $C$
        \item $r_1$ inizia da $A$ e $r_2$ inizia da $C$
        \item Il tempo di viaggio tra le stanze è $0$
        \item Performance Measure: minimizzare il tempo medio di inattività tra le stanze
        \item Media di inattività: somma degli intervalli nella quale la stanza non è stata visitata 
        da nessun robot
        \item Quale potrebbe essere un comportamento razionale di questo ambiente?
    \end{itemize}
    Quello che succede in maniera ragionevole è la seguente, dove S è la tupla
    in cui i robot sono posizionati:
    TODO
    \\
    Nei diversi casi si ha che il miglior modo per fare girare i robot
    è quello di farli muovere alternando chi entra nella stanza B 
    minimizzando anche la varianza nelle varie stanze perché dobbiamo
    stare attenti a non penalizzare troppo una stanza.
}

\subsection{Tipi di ambiente}

Il tipo di ambiente determina la progettazione di un agente? 
Nel mondo reale è ovviamente parzialmente visibile, stocastico, sequenziale,
dinamico, continuo, multi-agente.

\begin{itemize}
    \item \textbf{Completamente osservabile vs parzialmente osservabile}: un agente ha accesso
    completo allo stato dell'ambiente in ogni istante di tempo?
    \item \textbf{Deterministico vs stocastico}: il prossimo stato dell'ambiente è completamente
    determinato dallo stato corrente e dall'azione eseguita dall'agente?
    \item \textbf{Episodico vs sequenziale}: l'esperienza dell'agente è divisa in episodi
    indipendenti?
    \item \textbf{Statico vs dinamico}: l'ambiente può cambiare mentre l'agente sta pensando?
    \item \textbf{Discreto vs continuo}: il numero di stati, percezioni e azioni è finito o infinito?
    \item \textbf{Singolo agente vs multi-agente}: l'agente agisce da solo o ci sono altri agenti
    che possono influenzare l'ambiente?
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        & Crosswords & Robo-selector & Poker & Taxi \\
        \hline
        Osservabile & Sì  & Parziale & Parziale & Parziale \\
        Deterministico & Sì & No & No &  No\\
        Episodico & No & Sì & No & No\\
        Statico & Sì & No & Sì & No \\
        Discreto & Sì & No & Sì & No \\
        Singolo agente & Sì & Sì & No & No\\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item Se il problema è deterministico e completamente osservabile, è un \textbf{single-state problem}
    \item Se il problema non è osservabile, è un \textbf{conformant problem}
    \item Se il problema è non deterministico o parzialmente osservabile, è un \textbf{contingency problem}
    \item Quando non conosco lo spazio degli stati è un \textbf{exploration problem}
\end{itemize}

\subsection{Problem Solving Agents}

Una forma ristretta di agente generale sono i: \textbf{Goal Based Agent}
\begin{itemize}
    \item Formula un goal e un problema partendo dallo stato corrente
    \item Cerco una soluzione a questo problema
    \item Eseguo la soluzione ignorando le percezioni
\end{itemize}
Notiamo che questo si chiama anche offline problem; la soluzione viene eseguita ad "occhi chiusi".
\begin{minted}{python}
function Simple-Problem-Solving-Agent(percept) returns an action
    static: solution, state, problem, action
    state <- Update-State(state, percept)
    if seq is empty then
        goal <- Formulate-Goal(state)
        problem <- Formulate-Problem(state, goal)
        seq <- Search(problem)
    action <- First(seq)
    seq <- Rest(seq)
    return action
\end{minted}

\ex{Vacanze in Romania}
{
    In viaggio in Romania, se attulamente ad Arad.
    Il viaggio parte domani da Bucharest.
    \begin{itemize}
        \item \textbf{Formulate Goal:} essere a Bucharest
        \item \textbf{Formulate Problem:} stati: varie città, azioni: guidare tra le città
        \item \textbf{Search:} trovare una sequenza di azioni che portano da Arad a Bucharest
        \item \textbf{Esempio di Soluzione:} Arad, Sibiu, Fagaras, Bucharest
    \end{itemize}
}

\subsubsection{Tree Search Algorithm}

Idea base: offline, esplorazione simulata di spazio di stati, generando
successori di stati già esplorati.
\begin{minted}{python}
function Tree-Search(problem) returns a solution, or failure
    initialize the frontier using the initial state of problem
    loop do
        if the frontier is empty then return failure
        node <- Pop an element from the frontier
        if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
        expand node, adding the resulting nodes to the frontier
    end
\end{minted}

\dfn{}
{
    Uno \textbf{stato} è una rappresentazione di una configurazione
    fisica.
}
\dfn{}
{
    Un \textbf{nodo} è una struttura dati che contiene:
    \begin{itemize}
        \item uno stato
        \item un puntatore al nodo genitore
        \item l'azione che ha generato lo stato
        \item il costo del cammino dal nodo radice a questo nodo
    \end{itemize}
    Gli stati non hanno parenti, azioni, figli, costi e profondità!
}

\begin{minted}{python}
function Expand(node, problem) returns a set of nodes
    successors <- an empty list
    for each action in problem.ACTIONS(node.STATE) do
        child <- CHILD-NODE(problem, node, action)
        add child to successors
    return successors    
\end{minted}
\noindent

\subsection{Strategie di ricerca}

Una strategia è definita dal scegliere l'ordine dei nodi di espansione.
Strategie vengono valutate insieme alle seguenti metriche:
\begin{itemize}
    \item Completezza: la strategia trova una soluzione se esiste
    \item Tempo: tempo di esecuzione della strategia
    \item Spazio: memoria usata dalla strategia
    \item Optimalità: la strategia trova la soluzione ottima?
\end{itemize}
Tempo e spazio sono misurati in termini di:
\begin{itemize}
    \item $b$ branching factor (numero massimo di figli per nodo)
    \item $d$ profondità della soluzione più superficiale
    \item $m$ profondità massima dell'albero di ricerca (potrebbe essere infinito)
\end{itemize}

\subsubsection{Stati ripetuti}
Fallire nel riconoscere stati ripetuti può trasformare un problema lineare in un problema
esponenziale. Bisogna quindi mantenere una lista di stati già visitati e non espandere
nodi che portano a stati già visitati:
\begin{lstlisting}[language=Python]
function Graph-Search( problem, frontier) returns a solution, or failure
  explored <- an empty set
  frontier <- Insert(Make-Node(problem.Initial-State))
  while not IsEmty(frontier) do
    node <- Pop(frontier)
    if problem.Goal-Test(node.State) then return node
    if node.State is not in explored then
      add node.State to explored
      frontier <- InsertAll(Expand(node, problem))
    end if
  end loop
  return failure
\end{lstlisting}

\subsection{Ricerca non informata}
Gli algoritmi di ricerca non informata utilizzano soltanto i dati disponibili nella
definizione del problema e i principali sono:
\begin{itemize}
  \item Breadth-first search
  \item Uniform-cost search (Dijkstra)
  \item Depth-first search
  \item Depth-limited search
  \item Iterative deepening search
\end{itemize}

\subsubsection{Breadth-first search}
Questo algoritmo espande il nodo non esplorato più superficiale, cioè il nodo più vicino
alla radice. Utilizza una coda FIFO per la frontiera e i nuovi successori vengono
aggiunti alla fine della coda.
\begin{lstlisting}[language=Python]
function BFS( problem) returns a solution, or failure
  node <- node with State=problem.Initial-State,Path-Cost=0
  if problem.Goal-Test(node.State) then return node
  explored <- empty set frontier <- FIFO queue with node as the only element
  loop do
    if frontier is empty then return failure
    node <- Pop(frontier)
    add node.State to explored
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem,node,action)
      if child.State is not in (explored or frontier) then
        if problem.Goal-Test(child.State) then return child
        frontier <- Insert(child)
      end if
    end for
  end loop
\end{lstlisting}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, soltanto se \( b \) è finito, cioè se il branching factor
    è limitato
  \item \textbf{Complessità di tempo}: \( b + b^2 + b^3 + \ldots + b^d = O(b^d) \)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), perchè bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, soltanto se il costo delle azioni è uniforme
\end{itemize}

\subsubsection{Uniform-cost search}
Questo algoritmo espande il nodo non esplorato con il \textbf{costo del percorso più basso}.
La frontiera è una coda di priorità ordinata in base al costo del percorso.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, se il costo minimo delle azioni \( \ge \varepsilon \) 
    (con piccola ma \( \varepsilon > 0 \))
  \item \textbf{Complessità di tempo}: Numero di nodi \( g \le  \) del costo del percorso
    ottimale \( C^* \). \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Complessità di spazio}: \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Ottimale}: Sì perchè i nodi vengono espansi in ordine di costo del percorso
\end{itemize}
Ci sono due modifiche principali rispetto alla BFS che garantiscono l'ottimalità:
\begin{enumerate}
  \item Il goal test viene fatto quando il nodo viene estratto dalla frontiera, non quando
    viene generato. (Questo elemento spiega il \( +1 \) nella complessità
  \item Controllare se un nodo generato è già presente nella frontiera con un costo più
    alto e in tal caso sostituirlo con il nuovo nodo a costo più basso
\end{enumerate}

\subsubsection{Depth-first search}
Questo algoritmo espande il nodo non esplorato più profondo, cioè il nodo più lontano
dalla radice. Utilizza una pila LIFO per la frontiera e i nuovi successori vengono
aggiunti all'inizio.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ramo infinito,
    a meno che l'albero di ricerca non abbia una profondità limitata. Si potrebbero
    evitare loop modificando l'algoritmo per evitare stati ripetuti sul percorso corrente
  \item \textbf{Complessità di tempo}: \( O(b^m) \), dove \( m \) è la profondità massima
    dell'albero di ricerca
  \item \textbf{Complessità di spazio}: \( O(bm) \), bisogna memorizzare soltanto il
    percorso corrente e i nodi fratelli
  \item \textbf{Ottimale}: No, perchè non garantisce di trovare la soluzione migliore
\end{itemize}

\subsubsection{Iterative deepening search}
Questo algoritmo combina i vantaggi della BFS e della DFS. Esegue una serie di ricerche
in profondità limitata, aumentando progressivamente il limite di profondità fino a
trovare una soluzione.
\begin{lstlisting}[language=Python]
# Depth-Limited Search
function DLS(problem, limit) returns soln/fail/cutoff
  R-DLS(Make-Node(problem.Initial-State), problem, limit)


function R-DLS(node, problem, limit) returns soln/fail/cutoff
  if problem.Goal-Test(node.State) then return node
  else if limit = 0 then return cutoff # raggiunta la profondita' massima
  else
    # flag: c'e' stato un cutoff in uno dei sottoalberi?
    cutoff-occurred? <- false
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem, node, action)
      result <- R-DLS(child, problem, limit-1)
      if result = cutoff then cutoff-occurred? <- true
      else if result 6 = failure then return result
    end for
    if cutoff-occurred? then return cutoff else return failure
  end else

# Iterative Deepening Search
function IDS(problem) returns a solution
  inputs: problem, a problem
  for depth <- 0 to infinity do
    result <- DLS(problem, depth)
    if result 6 = cutoff then return result
  end
\end{lstlisting}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì
  \item \textbf{Complessità di tempo}: \( db^1 + (d-1)b^2 + \ldots + b^d = O(b^d) \) 
  \item \textbf{Complessità di spazio}: \( O(bd) \) 
  \item \textbf{Ottimale}: Sì, se il costo delle azioni è uniforme
\end{itemize}

\ex{}
{
  Assumi:
  \begin{enumerate}
    \item Un albero di ricerca ben bilanciato, tutti i nodi hanno lo stesso numero di figli
    \item Il goal state è l'ultimo che viene espanso nel suo livello (il più a destra)
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la ricerca in ampiezza quanti nodi
      vengono generati?
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la iterative deepening quanti nodi
      vengono generati?
  \end{enumerate}
}
\ex{}
{
  Un uomo ha un lupo, una pecora e un cavolo. L'uomo è sulla riva di un fiume con una
  barca che può trasportare solo lui e un altro oggetto. Il lupo mangia la pecora e la
  pecora mangia il cavolo, quindi non può lasciarli insieme da soli.
  \begin{enumerate}
    \item Formalizza il problema come un problema di ricerca
    \item Usa BFS per risolvere il problema
  \end{enumerate}

  \vspace{1em}
  \noindent
  \textbf{Soluzione:}

  Formalizziamo gli stati come una tupla:
  \[
    <W, S, C, M, B>
  \] 
  dove:
  \begin{itemize}
    \item \( W \): posizione del lupo
    \item \( S \): posizione della pecora
    \item \( C \): posizione del cavolo
    \item \( M \): posizione dell'uomo
    \item \( B \): stato della barca
  \end{itemize}
  La posizione può essere \( 0 \) (left) o \( 1 \) (right).

  Lo stato iniziale è:
  \[
    <0, 0, 0, 0, 0>
  \] 
  Lo stato obiettivo è:
  \[
    <1, 1, 1, 1, 1>
  \]
  Le azioni possibili sono:
  \begin{itemize}
    \item Porta il lupo (CW)
    \item Porta la pecora (CS)
    \item Porta il cavolo (CC)
    \item Porta niente (CN)
  \end{itemize}
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      Operatore & Precondizione & Funzione \\
      \hline
      \footnotesize CW & \footnotesize \( M = B, M = W, S \neq C \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<\bar{W},S,C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CS & \footnotesize \( M = B, M = S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,\bar{S},C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CC & \footnotesize \( M = B, M = C, W \neq S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,\bar{C},\bar{M},\bar{B}\right> \)\\
      \footnotesize CN & \footnotesize \( M = B \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,C,\bar{M},\bar{B}\right> \)\\
      \hline
    \end{tabular}
  \end{table}
  Notiamo che in tutte le precondizioni c'è \( M = B \) perchè l'uomo deve essere
  sempre con la barca, quindi si possono unire i due stati in uno solo \( M \).
}

\subsection{Ricerca informata}
Gli algoritmi di ricerca informata utilizzano informazioni aggiuntive (euristiche)
per guidare la ricerca verso la soluzione in modo più efficiente.

\subsubsection{Best-first search}
Questo algoritmo usa una \textbf{funzione di valutazione} per ogni nodo che stima la
"desiderabilità". La frontiera è una coda ordinata in ordine decrescente di desiderabilità.
A seconda di come viene definita la desiderabilità si ottengono diversi algoritmi:
\begin{itemize}
  \item Greedy best-first search
  \item A*
\end{itemize}

\subsubsection{Greedy best-first search}
Questo algoritmo espande il nodo che sembra essere il più vicino alla soluzione
secondo una funzione di valutazione euristica \( h(n) \) che stima il costo
rimanente per raggiungere l'obiettivo da un nodo \( n \).
\ex{}
{
  In una mappa di una città, la funzione di valutazione potrebbe essere la distanza
  in linea d'aria dal nodo corrente alla destinazione. In questo modo, l'algoritmo
  esplora prima i nodi che sembrano più vicini alla destinazione, riducendo il numero
  di nodi esplorati rispetto a una ricerca non informata.
}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ciclo infinito. È
    completo se lo spazio di ricerca è finito e ci sono controlli per evitare stati
    ripetuti
  \item \textbf{Complessità di tempo}: \( O(b^m) \) nel peggiore dei casi, ma può essere
    molto più veloce con una buona euristica
  \item \textbf{Complessità di spazio}: \( O(b^m) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: No
\end{itemize}

\subsubsection{A* search}
Questo algoritmo evita di espandere cammini che sono già molto costosi e ha come
funzione di valutazione:
\[
  f(n) = g(n) + h(n)
\] 
dove:
\begin{itemize}
  \item \( g(n) \): costo del percorso dal nodo iniziale a \( n \)
  \item \( h(n) \): stima del costo rimanente per raggiungere l'obiettivo da \( n \)
  \item \( f(n) \): stima del costo totale del percorso passando per \( n \)
\end{itemize}
L'euristica, per poter garantire l'ottimalità, deve essere \textbf{ammissibile}, cioè
per ogni nodo la stima di quel nodo deve essere minore o uguale del vero costo per arrivare
all'obbiettivo, quindi non deve \textbf{sovrastimare} il costo rimanente:
\[
  h(n) \le h^*(n) \quad h(n) \ge 0 \to h(G) = 0
\] 
dove \( h^*(n) \) è il costo effettivo del percorso da \( n \).
\thm{}
{
  Per A* l'euristica ammissibile implica l'ottimalità
}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, tranne se ci sono nodi infiniti con \( f \le f(G) \) 
  \item \textbf{Complessità di tempo}: Esponenziale in errore relativo in \( h \times  \) 
    lunghezza del numeo di passi della soluzione ottimale. (Se l'euristica è buona, la
    complessità sarà molto più bassa)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, se l'euristica è ammissibile e consistente
\end{itemize}

\subsection{Ricerca locale}

In molte problemi di ottimizzazione il "path" è irrilevante,
il traguardo è importante. In questi casi, allora lo spazio degli stati è 
un insieme di configurazioni:
\begin{itemize}
  \item Trovare la configurazione ottimale (TSP (Travelling Salesperson Problem), etc...)
  \item Trovdare una configurazione che soddisfi dei vincoli (n-Queens, per esempio, dove 
  ci sono 8 regine su una scacchiera e per trovare la configurazione
  dove nessuna delle 8 è sotto attacco, parto da una configurazione "base"
  e sposto le regine finché non trovo la configurazione traguardo, etc...)
\end{itemize}
Si possono usare algoritmi di "iterative improvement": 
\begin{itemize}
  \item Mantenere un singolo stato corrente
  \item Cercare di migliorarlo
\end{itemize}
Spazio costante, fatto apposta per online e offline search. 
Varianti di questo approccio arrivano fino a $1\%$ di soluzione ottimali.

\ex{Problema delle $n$ regine}
{
  \begin{itemize}
    \item Inserire $n$ regine su una scacchiera $n \times n$ in modo che nessuna regina
      possa attaccarne un'altra (quindi due regine non devono essere sulla stessa riga, colonna o diagonale).
    \item Muovi una regina per volta, cercando di ridurre il numero di conflitti.
  \end{itemize}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{nQueens.png}
  \end{figure}
  \noindent
  Quasi sempre si solve una problema di questo tipo in pochi passi, anche per $n=1$ milione.
}
\noindent
Ecco ora l'algoritmo di "hill-climbing" (come scalare il monte everest in una fitta nebbia con amnesia):
\begin{minted}{python}
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs:  problem, a problem
    local variables: current, a node
                     neighbor, a node
    current <- MAKE-NODE(problem.INITIAL-STATE)
    loop do
        neighbor <- a highest-value neighbor of current
        if neighbor.VALUE <= current.VALUE then return 
          current.STATE
        current <- neighbor
\end{minted}
\noindent
Utile per considerare lo \textit{state scape landscape}:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{hill-climbing.png}
\end{figure}
\noindent
Ci sono varianti di questo algoritmo:
\begin{itemize}
  \item \textbf{Random-restart hill climbing} è una variante che supera il massimo locale, trivialmente completo.
  \item \textbf{Random sideways moves} è buono perché esce dalle \textit{shoulder} ma non completamente perché può rimanere bloccato in un ciclo infinito su "flat local maxima". 
\end{itemize}

\subsubsection{Simulated annealing}

Simulated annealing è un algoritmo di ottimizzazione ispirato al processo di 
raffreddamento dei metalli. L'idea è di permettere occasionalmente mosse che peggiorano la soluzione corrente
per evitare di rimanere bloccati in massimi locali.
\begin{itemize}
  \item Inizia con una temperatura alta che permette molte mosse peggiorative
  \item La temperatura diminuisce gradualmente, riducendo la probabilità di accettare mosse peggiorative
  \item Alla fine, la temperatura raggiunge zero e l'algoritmo si comporta come hill-climbing
  \item La scelta della schedule di raffreddamento è cruciale per le prestazioni dell'algoritmo
\end{itemize}
\begin{lstlisting}[language=Python]
function Simulated-Annealing(problem, schedule) returns a solution state
    inputs: problem, a problem
    schedule, a mapping from time to "temperature"
    local variables: current, a node
                     next, a node
                     T, a "temperature" controlling prob. of downward steps
    current <- Make-Node(problem.Initial-State)
    for t <- 1 to infinity do
      T <- schedule(t)
      if T = 0 then return current
      next <- a randomly selected successor of current
      deltaE <- next.Value - current.Value
      if deltaE > 0 then current <- next
      else current <- next only with probability e^{delta E/T} 
\end{lstlisting}
A temperatura fissata $T$, la probabilità di accettare una mossa che peggiora la soluzione di $\Delta E$ è $e^{\Delta E / T}$.
\[p(x) = \alpha e^{\frac{E(x)}{kT}}\]
Decrescendo $T$ abbastanza, si può garantire la convergenza alla soluzione ottimale.
Perché
\[e^{\frac{E(x^*)}{kT}} / e^{\frac{E(x)}{kt}} = e^{\frac{E(x^*) - E(x)}{kt}} \gg 1 \quad \text{per } T \to 0\]

\subsubsection{Local beam search}

Local Beam Search è un algoritmo di ricerca locale che mantiene $k$ stati invece di uno solo. Inizia con $k$ stati casuali
e ad ogni iterazione:
\begin{itemize}
   \item Genera tutti i successori di tutti i $k$ stati correnti
  \item Seleziona randomicamente i $k$ migliori successori tra tutti quelli generati
  \item Ripete fino a quando non viene trovata una soluzione o non ci sono più miglioramenti
  \item Se tutti i $k$ stati convergono allo stesso punto, si può introdurre
    diversità sostituendo alcuni stati con nuovi stati casuali
\end{itemize}

\subsection{Ricerca locale in spazio continuo}

La ricerca locale può essere estesa a spazi di stato continui.
Per risolvere questi problemi si possono utilizzare tecniche come:
\begin{itemize}
  \item \textbf{Discretizzazione:} suddividere lo spazio continuo in una griglia di punti discreti e applicare
    algoritmi di ricerca locale su questi punti
    \item \textbf{Randomiche Perturbazioni:} introdurre piccole perturbazioni casuali alle soluzioni correnti per esplorare
    lo spazio delle soluzioni con metodi come il simulated annealing (il prossimo stato è scelto randomicamente)
    \item \textbf{Gradiente}: utilizzare il gradiente della funzione obiettivo per guidare la ricerca verso
    direzioni di miglioramento (il prossimo stato è scelto in base alla direzione del gradiente).
    Il metodo del gradiente calcola:
    \[\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)\]
    Per trovare la direizione di massima crescita della funzione obiettivo si pone 
    il gradiente uguale a zero:
    \[\nabla f(x) = 0\]
\end{itemize}
\noindent
A volte però non riusciamo a risolvere $\nabla f (x) = 0$ analiticamente, 
quindi possiamo migliorarla localmente:
\begin{itemize}
  \item Si performa un update nella direzione della salita per ogni coordinata
  \item Più la funzione è ripida più si fanno passi grandi
\end{itemize}
Aggionrare una coordinata viene effettuato tramite una
funzione generale $g(x_1, x_2)$
\[
x_1 \leftarrow x_1 + \alpha \frac{\partial g(x_1,x_2)}{\partial x_1} \quad
x_2 \leftarrow x_2 + \alpha \frac{\partial g(x_1,x_2)}{\partial x_2}
\]
Oppure in forma vettoriale:
\[
X = \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\quad 
nabla g(X) = \begin{bmatrix}
\frac{\partial g(x_1,x_2)}{\partial x_1} \\
\frac{\partial g(x_1,x_2)}{\partial x_2}
\end{bmatrix}
\]
\[X \leftarrow X + \alpha \nabla g(X)\]
Dove $\alpha$ è lo step size, cioè la dimensione del passo da fare:
\begin{itemize}
  \item Se è troppo grande si rischia di saltare soluzioni
  \item Se è troppo piccolo la convergenza sarà molto lenta
\end{itemize}

\subsubsection{Algoritmo di Newton-Raphson}
È una tecnica generale per trovare le radici di una funzione
cioè risolvere un'equazione $f(x) = 0$.
Per farlo si trova un'approsimazione iniziale $\bar{x}_0$ della soluzione
e iterativamente si aggiorna l'approissimazione usando la formula:
\[
\bar{x}_{n+1} = \bar{x}_n - \frac{f(\bar{x}_n)}{f'(\bar{x}_n)}
\]
dove:
\[
g'(x) = \frac{d}{dx} g(x)
\]
\ex{}
{
  Consideriamo la funzione $f(x) = x^2 - a$.
  \begin{itemize}
    \item Mostrare che il metodo di Newton conduce a:
    \[x_{n+1} = \frac{1}{2}\left(x_n + \frac{a}{x_n}\right)\]
    \item Fissato $a = 4, x_0 = 1$ cacolare le prime tre iterazioni. ($x_i = \{1,2,3\}$)
  \end{itemize}
  Quindi abbiamo \[
    f(x) = x^2 - a
  \] 
  \[x_{n+1} = \bar{x}_n - \frac{f(\bar{x}_n)}{f'(\bar{{x}_n})}
  \]
  \[x_{n+1} = \bar{x}_n - \frac{\bar{x}_n - a}{2\bar{x}_n}  \]
}

\subsection{Constrained Satisfaction Problem}

Un \textbf{Constrained Satisfaction Problem (CSP)} è un problema
definito da:
\begin{itemize}
  \item Un insieme di variabili \( X = \{X_1, X_2, \ldots, X_n\} \)
  \item Un insieme di domini \( D = \{D_1, D_2, \ldots, D_n\} \) dove ogni \( D_i \)
    è l'insieme dei valori possibili per la variabile \( X_i \)
  \item Un insieme di vincoli \( C = \{C_1, C_2, \ldots, C_m\} \) che specificano
    le relazioni tra le variabili
\end{itemize}
Assunzioni: single agent, azioni deterministiche, stato completamente osservabile

\ex{Map-Coloring}
{
  Il Map-coloring è un problema specifico di Graph coloring.
  Dato un insieme di regioni geografiche e un insieme di colori,
  assegnare un colore a ogni regione in modo che regioni adiacenti
  non abbiano lo stesso colore.
  \begin{itemize}
    \item Variabili: regioni geografiche (es. WA, NT, Q, NSW, V, SA, T)
    \item Domini: colori (es. rosso, verde, blu)
    \item Vincoli: regioni adiacenti non possono avere lo stesso colore
  \end{itemize}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{map-coloring.png}
  \end{figure}
}
\ex{N-Queens Problem}
{
  Il N-Queens Problem è un problema di posizionamento di N regine su una
  scacchiera \( N \times N \) in modo che nessuna regina possa attaccarne
  un'altra.
  \begin{itemize}
    \item Variabili: posizioni delle regine sulle righe della scacchiera
    \item Domini: colonne della scacchiera (es. 1, 2, ..., N)
    \item Vincoli: questa volta formuliamo i vincoli in maniera più complessa.
    Un vincolo per ogni coppia di variabili specificando le posizioni "permesse" PER OGNI ogni due regine.
  \end{itemize}
  Questa formulazione rende alcuni vincoli impliciti, per esempio, 
  non è possibile assegnare due regine la stessa colonna quindi
  non ci sta bisogno di controllare.
}
\subsubsection{Grafo dei vincoli}

Un \textbf{grafo dei vincoli} chiamato anche grafo primale
consiste nel costruire
un nodo per ogni variabile
e un arco per ogni vincoli tra due variabili.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{graphcoloring.png}
\end{figure}

\subsubsection{Ipergrafi e Grafi duali}

Le relazioni tra ipergrafi e grafi binari:
\begin{itemize}
  \item Si può sempre convertire un ipergrafo in un grafo binario
  \item Ogni variabile ha un dominio esponenzialmente grande
\end{itemize}

\subsubsection{Problemi combinatori}

Dato un insieme di possibili soluzioni bisogna trovare quella migliore 
che soddisfa i vincoli. Alcuni esempi:
\begin{itemize}
  \item Decisionali: colorare un grafo con $k$ colori
  \item Ottimizzazione: trovare la colorazione con il minor di conflitti
  \item Ottimizzazione Multi-obiettivo: portfolio investment, minimizzare il rischio e
  massimizzare il guadagno
  \item Modelli grafici:
  \begin{itemize}
    \item Insieme di variabili, domini e funzioni locali (vincoli)
    \item Funzioni globali è un aggregazione di funzioni locali
    \item Soluzioni: l'assegnamento di variabili che ottimizza la funzione globale 
  \end{itemize}
\end{itemize}

\dfn{Rete a vincoli}
{
  Una tupla di tre elementi CN = $(X, D, C)$ dove:
  \begin{itemize}
    \item $X = \{X_1, X_2, \ldots, X_n\}$ è un insieme di variabili
    \item $D = \{D_1, D_2, \ldots, D_n\}$ è un insieme di domini associati alle variabili
    \item $C = \{C_1, C_2, \ldots, C_m\}$ è un insieme di vincoli che specificano le relazioni tra le variabili
    \item Ogni vincolo $C_i$ è una tupla $(S_i, R_i)$ dove:
    \begin{itemize}
      \item $S_i \subseteq X$ è lo scopo, l'insieme delle variabili coinvolte in $R_i$
      \item $R_i$ sottoinsieme del prodotto cartesiano delle variabili in $S_i$
      \item $R_i$ specifica le tuple permesse su $S_i$
    \end{itemize}
    \item \textbf{Soluzione:} assegnamento di tutte le variabili che soddisfano tutti i vincoli.
    \item Obiettivo: consistency check, trovare una o tutte le soluzioni,
    ottimizzare una funzione obiettivo.
  \end{itemize}
} 
Ci si può avvicinare alla soluzione
attraverso una soluzione parziale:
\begin{itemize}
  \item Soluzione parziale consistente: soluzione parziale che soddisfa
  tutti i vincoli di cui lo scope non contiene variabili non assegnate
  \item Una soluzione parziale consistente non è necessariamente
  estendibile a una soluzione completa
\end{itemize}

\subsubsection{Tree Decomposition}

\dfn{Cycle Cutset}
{
  Dato un grafo non orientato, un sottoinsieme di nodi nel grafo è 
  un cycle cutset se la rimozione di questi nodi rende il grafo
  aciclico.
}
Il concetto è:
\begin{itemize}
  \item Una volta che una variabile viene assegnata può essere rimossa dal grafo 
  \item Se rimoviamo un cycle cutset allora il grafo rimanente è un albero
  \item Si può usare arc-consistency per risolvere l'albero rimanente
  \item Dobbiamo controllare ogni possibile assegnazione delle variabili del cycle cutset 
  e fare propagazione negli archi 
  \item La complessità è comunque esponenziale ma nella dimensione del cycle cutset.
\end{itemize}

\section{Logical Agents}

Perché costruire un agente basato sulla logica? 
Solitamente hanno due componenti:
\begin{itemize}
  \item \textbf{Knowledge base (KB):} insieme di proposizioni che rappresentano
    ciò che l'agente sa sul mondo
  \item \textbf{Inference engine:} meccanismo per dedurre nuove proposizioni
\end{itemize}
La knowledge base è un insieme di proposizioni in un linguaggio formale.
Un approccio dichiarativo per costruire un agente:
\begin{itemize}
  \item Dire cosa sa l'agente 
  \item Poi può chiedersi da solo cosa fare e le risposte dovrebbero seguire la knowledge base.
\end{itemize}
Gli agenti possono essere visti al livello di conoscenza i.e cosa sanno, 
non come sono implementati.
O a livello di implementazione, cioè come sono costruiti.
\begin{minted}{python}
function KB-Agent(percept) returns an action
    inputs: percept, a percept
    static: KB, a knowledge base, initially empty
            action, an action, initially null 
            t, a counter initially 0
    KB <- Tell(KB, Percept-To-Sentence(percept))
    action <- Ask(KB, Action-Sentence())
    KB <- Tell(KB, Action-To-Sentence(action))
    t <- t + 1
    return action
\end{minted}
\ex{Wumpus World PEAS}
{
  Un esempio famoso di agente logico è l'agente Wumpus World.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{wumpus.png}
  \end{figure}


  \begin{itemize}
    \item \textbf{Performance measure:} +1000 per uscita, -1 per ogni azione,
      -1000 per essere mangiati dal Wumpus o cadere in un buco
    \item \textbf{Environment:} 
    \begin{itemize}
      \item Celle adiacenti al wumpus sono maleodoranti
      \item Celle adiacenti a un buco sono ventose
      \item C'è scintillio se l'oro è nella stessa cella
      \item Sparare uccide il wumpus se si è rivolti verso di lui
      \item Sparare consuma la sola freccia
      \item Prendere raccoglie l'oro se si è nella stessa cella
      \item Rilasciare lascia cadere l'oro nella stessa cella
    \end{itemize}
    \item \textbf{Actuators:} muovi, gira a sinistra, gira a destra, spara,
      prendi oro, esci
    \item \textbf{Sensors:} brivido, puzza, scintilla, bump, urlo, sensore di
      glitter, sensore di impatto
  \end{itemize}
  Questo problema è:
  \begin{itemize}
    \item Osservabile? No—solo percezione locale
    \item Deterministico? Sì—i risultati sono esattamente specificati
    \item Episodico? No—sequenziale a livello di azioni
    \item Statico? Sì—Wumpus e buchi non si muovono
    \item Discreto? Sì 
    \item Single-agent? Sì—Wumpus è essenzialmente una caratteristica naturale
  \end{itemize}
}
\subsection{Logica in generale}

La logiche sono linguaggi formali per rappresentare informazioni
come per trarre conclusioni.
Come sappiamo la logica è divisa in:
\begin{itemize}
  \item \textbf{Sintassi:} definisce le frasi nel linguaggio
  \item \textbf{Semantica:} definisce il "significato" delle frasi;
    cioè definisce la verità di una frase in un mondo
\end{itemize} 
Un esempio è il linguaggio dell'aritmetica:
\begin{itemize}
  \item \( x + 2 \ge y \) è una frase; \( x2 + y > \) non è una frase
  \item \( x + 2 \ge y \) è vera se il numero \( x + 2 \) non è minore del numero \( y \)
  \item \( x + 2 \ge y \) è vera in un mondo dove \( x = 7, y = 1 \)
  \item \( x + 2 \ge y \) è falsa in un mondo dove \( x = 0, y = 6 \)
\end{itemize}

\subsubsection{Entailment}

L'entailment è una relazione tra frasi in un linguaggio logico e ha 
a che fare con i modelli non con la prova formale.
\[KB \models \alpha\]
Knowledge base \( KB \) entaila la frase \( \alpha \) se e solo se
$alpha$ è vera in ogni mondo dove $KB$ è vera.
\ex{}
{
Per esempio se $KB$ contiene "La Juventus ha vinto" e "Roma ha vinto"
allora $KB$ entaila "O la Juventus o Roma ha vinto".
Oppure se $x + y = 4$ allora $4 = x + y$.
Entailmente è una relazione tra frasi (cioè sintassi)
basata sulla semantica.
I computer sono molto bravi a processare regole sintattiche.
}

\subsubsection{Modelli}

I logici solitamente ragionano in termini di modelli, che sono formalmente 
strutturati in mondi rispetto alla verità che deve essere valutata. 
Diciamo che $m$ è un modello per una frase $alpha$ se $alpha$ è vera in $m$.
$M(\alpha)$ è l'insieme di tutti i modelli per $alpha$.
Allora $KB \models \alpha$ se e solo se $M(KB) \subseteq M(\alpha)$.
\ex{}
{
  Prendendo l'esempio di prima se $KB$ contiene "La Juventus ha vinto e la Roma ha vinto"
  allora $\alpha$ può essere "La Juventus ha vinto".
}

\subsection{Inferenza}

$KB \vdash_i \alpha$ vuol dire che $\alpha$ può essere derivata da $KB$ con una procedura $i$. 
Le conseguenze di $KB$ sono un pagliaio; $\alpha$ è un ago.
Entailmente = ago nel pagliaio; inferenza = trovarlo.
\begin{itemize}
  \item \textbf{Soundness:} $i$ è corretto (sound) se 
  dove $KB \vdash_i \alpha$ , è anche vero che $KB \models \alpha$
  \item \textbf{Completeness:} $i$ è completo se
  dove $KB \models \alpha$, è anche vero che $KB \vdash_i \alpha$
\end{itemize}
Preview: la logica del primo ordine è abbastanza espressiva
da poter dire quasi tutto ciò che ci interessa,
ed esiste una procedura di inferenza corretta e completa.
Quindi, la procedura risponderà a qualsiasi domanda la cui risposta
segua da ciò che è noto dalla KB.

\subsection{Logica proposizionale}

\subsubsection{Sintassi}

La logica proposizionale è il linguaggio logico più semplice. 
I simboli $P_1, P_2$ sono proposizioni atomiche.
\begin{itemize}
  \item Se $S$ è una proposizione, allora $\neg S$ è una proposizione
  \item Se $S_1$ e $S_2$ sono proposizioni, allora $S_1 \land S_2$, $S_1 \lor S_2$, 
    $S_1 \Rightarrow S_2$, $S_1 \Leftrightarrow S_2$ sono proposizioni
\end{itemize}

\subsubsection{Semantica}

Ogni modello specifica se qualcosa è vero o falso per ogni simbolo proposizionale.
Le regole per valutare rispetto ad un modello $M$ sono:
\begin{itemize}
  \item $\neq S$ è vero in $M$ se e solo se $S$ è falso in $M$
  \item $S_1 \land S_2$ è vero in $M$ se e solo se sia $S_1$ che $S_2$ sono veri in $M$
  \item $S_1 \lor S_2$ è vero in $M$ se e solo se almeno uno tra $S_1$ e $S_2$ è vero in $M$
  \item $S_1 \Rightarrow S_2$ è vero in $M$ se e solo se $S_1$ è falso in $M$ o $S_2$ è vero in $M$
  \item $S_1 \Longleftrightarrow S_2$ è vero in $M$ se e solo se entrambi sono veri in $M$
\end{itemize}

\ex{Wumpus World Sentences}
{
  Consideriamo $P_{i,j}$ sia vero se ci sta un pozzo nella cella $(i,j)$
  e $B_{i,j}$ sia vero se c'è del vento nella cella $(i,j)$.
  \begin{itemize}
    \item $R_1: \neg P_{1,1}$ (non c'è un pozzo nella cella (1,1))
    \item $R_2: \neg B_{1,1}$
    \item $R_3: B_{1,2}$
  \end{itemize}
  "Il pozzo è in una cella adiacente alla cella con vento":
  \[
    R_4: B_{1,1} \Leftrightarrow (P_{1,2} \lor P_{2,1})
  \]
  \[
    R_5: B_{1,2} \Leftrightarrow (P_{1,1} \lor P_{1,3} \lor P_{2,2})
  \]
  Una cella è ventosa \textbf{se e solo se} c'è un pozzo in una cella adiacente.
}
\begin{minted}{python}
function TT-Entails(KB,a) returns true or false
  inputs: KB, the knowledge base, a sentence in prop. logic
  a, the query, a sentence in prop. logic
  symbols <- a list of the proposition symbols in KB and a
  return TT-Check-All(KB,a,symbols,[])  
\end{minted}

\end{document}