\documentclass[a4paper]{article}
\usepackage{import}
\input{../../setup.sty}
% Set custom margins
\onehalfspacing

\geometry{a4paper, margin=1in, includehead, nomarginpar}
\geometry{letterpaper, left=1.5in, right=1in, top=1in, bottom=1in}

\title{Analisi II}
\author{Università di Verona\\Imbriani Paolo - VR500437\\Professor Zivcovich Franco}

% Make section titles bigger
\titleformat{\section}[hang]{\huge\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\Large\normalfont\itshape}{\thesubsection}{1em}{}

\usepackage[italian]{babel}

\addto\captionsitalian{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Indice}%
}

\numberwithin{equation}{subsection}

\usepackage{fancyhdr}


\begin{document}

\pagestyle{fancy}

%... then configure it.
\fancyhead{} % clear all header fields
\fancyhead[RO,LE]{\textbf{Appunti di Analisi 2}}
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[LO,CE]{by Imbriani Paolo}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{../UniversityofVerona.png}
\end{figure}


\maketitle 

\pagebreak

\tableofcontents

\pagebreak

\fancyhead[RO,LE]{\textbf{Equazioni differenziali}}

\section{Equazioni differenziali}

\subsection{Modelli differenziali}

I fisici, per descrivere dei fenomeni, usano la matematica come strumento per formalizzare dei modelli naturali e in particolare vorrei concentrarmi sull'uso delle equazioni differenziali. 
Per esempio, si denota $x(t)$ lo spostamento nel tempo.
Con la derivata prima $x'(t)$ si denota la velocità della particella in quell'istante e con la derivata seconda $x''(t)$ l'accelerazione.
Quindi quando andiamo a tradurre
matematicamente le leggi che governano modelli naturali può essere naturale dover lavorare
con equazioni che coinvolgono una funzione incognita e qualcuna delle sue derivate.

\ex{}{La seconda legge del moto di Newton $F = ma$, che stabilisce la posizione
$x(t)$ al tempo $t$ di un corpo di massa m costante, soggetto a una forza $F(t)$, deve soddisfare
l'equazione differenziale:
\[m\frac{d^2x}{dt^2} = F(t) \; \; \text{equazione del moto}\]
}
\noindent
Quindi le equazioni differenziali nascono per descrivere fenomeni fisici e naturali. 
Possono essere classificate in modi diversi. Abbiamo infatti:
\begin{enumerate}
    \item \textit{Equazioni differenziali ordinarie} (ODE) se vengono coinvolte solo le derivate rispetto ad una sola variabile oppure \textit{equazioni differenziali parziali} (PDE) se vengono coinvolte derivate parziali
     dell'incognita rispetto a più variabili.
     \ex{}{L'equazione: \[\frac{\partial^2 u}{\partial t^2}=c^2 \frac{\partial^2u}{\partial x^2}\]
     rappresenta l'equazione delle onde che modellizza lo spostamento trasversale $u(x, t)$ nel
punto $x$ al tempo $t$ di una corda tesa che può vibrare.
     }
    \item Classificazione in base all'ordine: l'ordine di una ED è l'ordine massimo di derivazione che
    compare nell'equazione.
\ex{}{L'equazione: \[\frac{dy^2}{dt^2} + ty^3 - \cos{y} = \sin{t} \; \; \; \; \; \; \text{è di ordine 2}\]
 \[\frac{d^3y}{dt^3} - 2t\left(\frac{dy}{dt}\right)^2  = y \frac{dy^2}{dt^2} - e^t\; \; \; \; \; \; \text{è di ordine 3}\]
}
Possiamo dunque formalizzare i concetti finora introdotti attraverso la seguente definizione:
\end{enumerate}
\dfn{Equazione differenziale}{
Si dice \textbf{equazione differenziale} di ordine $n$ un'equazione del tipo
\begin{equation}
    F(t,y',y'', \dots , y^{(n)}) = 0   
\end{equation}
dove $y(t)$ è la funzione incognita e $F$ è una funzione assegnata delle $n + 2$ variabili $t, y, y'
, \dots , y(n)$ a valori reali.
\\
Si dice \textbf{ordine} di un'equazione differenziale il massimo ordine di derivazione che compare nell'equazione.\\
Si dice \textbf{soluzione (o curva integrale)} di (1.1.1) nell'intervallo $I \subset \mathbb{R}$ una funzione $\varphi$, definita almeno in $I$ e a valori reali per cui risulti:
\begin{equation*}
F(t, \varphi'(t), \varphi''(t), \dots , \varphi^{(n)}(t)) = 0 \; \; \; \; \; \; \forall t \in I
\end{equation*}
Infine si dice integrale generale dell'equazione (1.1.1) una formula che rappresenti la famiglia
di tutte le soluzioni dell'equazione (1.1.1), eventualmente al variare di uno o più parametri in essa
contenuti.
}
\ex{}{
Consideriamo una popolazione di individui, animali o vegetali che siano,
e sia $N(t)$ il numero degli individui. Osserviamo che N è funzione di del tempo $t$, assume solo
valori interi ed è a priori una funzione discontinua di t; tuttavia può essere approssimata da
una funzione continua e derivabile purché il numero degli individui sia abbastanza grande.
Supponiamo che la popolazione sia isolata e che la proporzione degli individui in età riproduttiva e la fecondità siano costanti.
Se escludiamo i casi di morte, immigrazione, emigrazione, allora il tasso di accrescimento coincide con quello di natalità 
e se indichiamo con $\lambda$ il tasso specifico di natalità (i.e. il numero
di nati per unità di tempo) l'equazione che descrive il modello diventa:
\[\frac{dN}{dt} = \lambda N(t)\]
Questo processo risulta realistico solo in popolazioni che crescono in situazioni ideali e sono
assenti tutti i fattori che ne impediscono la crescita.
}
La stessa equazione compare anche in altri modelli relativi a sistemi fisiologici ed ecologici.
\ex{}{Studiamo ora il modello di crescita (dovuto a Malthus, 1978) relativo
all'evoluzione di una popolazione isolata in presenza di risorse limitate ed in assenza di predatori
o antagonisti all'utilizzo delle risorse. In questo caso l'equazione che si ottiene è la seguente:
\begin{equation*}
    \frac{dN}{dt} = \lambda N(t) - \mu N(t)
\end{equation*}
dove come prima $\lambda$ è il tasso di natalità mentre $\mu$ è il tasso di mortalità (cioè rispettivamente
il numero di nati e morti nell'unità di tempo). Il numero $\varepsilon = \lambda - \mu$ è detto \textbf{potenziale
biologico.}
}
\noindent
Ci chiediamo ora come possiamo trovare una soluzione del problema studiato nell'Esempio
1.5. Supponiamo per il momento che sia $N \neq 0$. Allora:
\begin{equation*}
N = \varepsilon N = \frac{N}{N} = \varepsilon \Longrightarrow \frac{d}{dt} (\log{|N|}) = \varepsilon,
\end{equation*}
da cui otteniamo:
\begin{equation*}
\log{|N(t)|} = \varepsilon t + c_1 \Longrightarrow |N(t)| = e^{c_1}e^{\varepsilon t} =: k^2 e^{\varepsilon t}
\end{equation*}
dove abbiamo posto $e^{c_1} =: k^2 > 0$ costante positiva e arbitraria. A questo punto allora:
\[N(t) = \pm k^2e^{\varepsilon t}\]
Quindi possiamo dire sicuramente che:
\[N(t) = Ce^{\varepsilon t} \; \; \; C \in \mathbb{R} \backslash \{0\} \]
Tutto questo vale se $N \neq 0$; ma è banale verificare che anche $N = 0$ soddisfa l'equazione di
partenza, quindi possiamo dire che l'integrale generale è:
\[N(t) = Ce^{\varepsilon t} = Ce^{(\lambda - \mu)t} \; \; \; C \in \mathbb{R}\]
In particolare dall'ultima riga leggiamo che:
\begin{enumerate}
    \item Se $\lambda > \mu$ allora $N(t)$ è una funzione che cresce in maniera esponenziale.
    \item Se $\lambda < \mu$ allora $N(t)$ è una funzione che decresce fino ad estinguersi. 
    \item Se $\lambda = \mu$ allora $N(t)$ è una funzione stabile nel tempo.
\end{enumerate}
Osserviamo in particolare che non abbiamo trovato solo una soluzione, ma infinite soluzioni,
dipendenti da una costante arbitraria.

\subsection{Equazioni differenziali di primo ordine}

\subsubsection{Generalità}

Le equazioni differenziali di primo ordine sono le più semplici da trattare e sono di fondamentale importanza in quanto sono alla base di molte applicazioni pratiche.
Esse sono della forma:
\begin{equation}
    F(t, y, y') = 0
\end{equation}
con $F$ funzione assegnata delle tre variabili $t, y, y'$ a valori reali.
\ex{}{
    La ricerca delle primitive di una funzione $f$ continua su un intervallo $I$
    equivale a risolvere l'equazione differenziale $y'(t) = f(t)$ che ammette infinite soluzioni del tipo
\begin{equation*}
    y(t) = \int f(t) \; dt + C \; \; \; \; \; \; C \in \mathbb{R}
\end{equation*}
}
\noindent
\textbf{Si dimostra} che l'insieme delle soluzioni di una EDO del primo ordine è costituito da una famiglia di funzioni dipendenti da un parametro 
$C: t \mapsto \varphi(t; c)$. Tale famiglia prende il nome di \textbf{integrale generale} dell'equazione differenziale.
La condizione supplementare $y(t_0) = y_0$ permette di selezionare una soluzione specifica.
\dfn{Problema di Cauchy}{
Il problema di risolvere il seguente sistema di equazioni:
\begin{equation}
\begin{cases}
    F(t, y, y') = 0 \\
    y(t_0) = y_0
\end{cases}
\end{equation}
prende il nome di \textbf{problema di Cauchy}.
}
\dfn{Forma Normale}{
Un'equazione differenziale ordinaria del primo ordine si dice in \textbf{forma normale} se è scritta nella forma:
\begin{equation}
    y'(t) = f(t, y)
\end{equation}
}
\noindent
Per equazioni di questo tipo si può assicurare, sotto larghe ipotesi, che il problema di Cauchy (1.2.2) ammette un'unica soluzione almeno 
localmente (cioè per valori di $t$ in un intorno di $t_0$).
\\
Le soluzioni dell'ED espresse dall'integrale generale potrebbero talvolta
essere definite su insiemi diversi a seconda del valore della costante o anche su insiemi più complicati
di un intervallo (es. $t \neq 0$). Tuttavia quando parleremo di soluzione del problema di Cauchy andremo
sempre a intendere una funzione che:
\begin{itemize}
    \item[a)] è definita su un intervallo $I$ contenente $t_0$ in cui è assegnata la condizione iniziale.
    \item[b)] è derivabile in ogni punto di $I$ e soddisfa l'equazione in ogni punto di $I$. 
\end{itemize}
\ex{}{
Il problema di Cauchy 
\begin{equation*}
    \begin{cases}
        N'(t) = 3N(t) \\
        N(0) = 7
    \end{cases}
\end{equation*}
ammette un'unica soluzione data da $N(t) = ce^{3t}$. Imponendo il dato iniziale otteniamo $N(t) = 7e^{3t}, \forall t \in \mathbb{R}$ (o $\mathbb{R}^+$ se si sta parlando di problema di Cauchy che modellizza un fenomeno fisico).
}

\subsubsection{Equazioni a variabili separabili}

Le equazioni a variabili separabili sono una particolare clase di ED ordinarie del primo ordine
del tipo (1.2.3) che sono caratterizzate dalla presenza di una funzione $f$ prodotto di due funzioni,
una della sola variabile $t$ e l'altra solo dell'incognita $y$. Più nel dettaglio, sono equazioni del
tipo:
\begin{equation}
    y'(t) = a(t)b(y)
\end{equation}
con $a$ funzione continua su un intervallo $I \subset \mathbb{R}$ e $b$ funzione continua su un intervallo $J \subset \mathbb{R}$.
Cerchiamo di capire come determinare l'integrale generale di questo tipo di equazioni.
Distinguiamo due casi:
\begin{itemize}
    \item Se $\overline{y}$ è soluzione dell'equazione $b'(\overline{y}) = 0$ allora $y(t) = \overline{y}$ è soluzione dell'ED (1.2.4). Infatti in tal caso si annulla il secondo membro della (1.2.4) e di conseguenza anche il primo
    membro (perchè la derivata della funzione costante è zero). 
    \item Supponiamo ora che $b(y) \neq 0$. Allora la (1.2.4) può essere riscritta come:
    \begin{equation*}
        \frac{y'}{b(y)} = a(t)
    \end{equation*}
    Quindi un'ipotetica soluzione soddisfa l'identità:
    \begin{equation*}
        \int \frac{y'(t)}{b(y(t))} \; dt = \int a(t) \; dt + C
    \end{equation*}
    Con $C$ costante arbitraria.
    Ora si può effettuare il cambio di variabile dove $y'(t)dt = dy$:
    \begin{equation*}
        \int \frac{dy}{b(y)} = \int a(t) \; dt + C
    \end{equation*}
    Quindi questo è l'integrale generale dell'equazione (1.2.4). Se $B(y)$ è una primitiva di $\frac{1}{b(y)}$ e $A(t)$ è una primitiva di $a(t)$, allora l'integrale generale della ED è assegnato dall'equazione (in forma implicita):
    \begin{equation*}
        B(y) = A(t) + C \; \; \; \text{ con $C$ costante arbitraria}
    \end{equation*}
\end{itemize}
Osserviamo che non è detto che si riesca a ricavare $y$ esplicitamente o a
ridurre la precedente equazione in forma normale.
In generale, per le equazioni a variabili separabili, vale il seguente:
\thm{}{
Si consideri il seguente problema di Cauchy:
\begin{equation*}
    \begin{cases}
        y' = a(t)b(y)\\
        y(t_0) = y_0
    \end{cases}
\end{equation*}
con $a$ continua in un intorno $I$ di $t_0$ e $b$ continua in un intorno $J$ di $y_0$.
Allora esiste un intorno di $t_0$ che denoteremo con $I' \subset I$ e una funzione continua $y$ definita su $I'$ con derivata 
anch'essa continua su $I'$ tale che $y$ sia soluzione del problema di Cauchy.
Inoltre se anche $b'$ è continua su $J$ (o $b$ ha un rapporto incrementale limitato in $J$ anche se non è derivabile) allora tale soluzione è anche unica.
}
\ex{}{
Consideriamo il problema di Cauchy:
\begin{equation*}
    \begin{cases}
        y' = ty^3\\
        y(0) = 1
    \end{cases}
\end{equation*}
Prima di tutto si osserva che $y = 0$ è integrale singolare per l'equazione data. Quindi se $y \neq 0$, separando le variabili e integrando si ottiene: 
\begin{align*}
\int \frac{dy}{y^3} &= \int t dt + C\\
- \frac{1}{2y^2} &= \frac{t^2}{2} + C\\
y &= \pm \frac{1}{\sqrt{C - t^2}}
\end{align*}
Imponendo il dato di Cauchy si osserva che l'unica soluzione è quella che si ottiene per $k = 1$
e considerando il segno positivo davanti alla radice, cioè
\begin{equation*}
    y = \frac{1}{\sqrt{1 - t^2}}
\end{equation*}
}
\ex{}{
Risolvere il problema di Cauchy:
\begin{equation*}
    \begin{cases}
        yy' = 2\\
        y(0) = 1
    \end{cases}
\end{equation*}
Integrando ambo i membri della ED proposta si ottiene:
\begin{equation*}
    \int y \; dy = \int 2 \; dt \Longrightarrow \frac{y^2}{2} = 2t + C \Longrightarrow y = \pm \sqrt{4t + 2C}
\end{equation*}
quindi per ogni $C \in \mathbb{R}$ esistono due soluzioni (corrispondenti ai due segni davanti alla radice)
definite solo per $t \ge -\frac{C}{2}$. Imponendo il dato di Cauchy si ottiene $y(0) = \pm
\sqrt{2C = 1}$,
quindi per compatibilità occorre scegliere il segno positivo davanti alla radice. La soluzione del
problema proposto è dunque $y = 4t + 1$, definita solo per $t \ge -\frac{1}{4}.$
Andiamo a controllare se sono soddisfatte le condizioni del teorema: $a(t) = 2$ che è dunque una
funzione continua e derivabile ovunque; $b(t) = 1/y$ che è continua e derivabile se $y \neq 0$. Quindi
il problema di Cauchy per questa equazione ha una e una sola soluzione purché la condizione
iniziale non sia del tipo $y(t_0) = 0$. Infatti l'equazione non è soddisfatta in questo punto perché
si otterrebbe $0 = 2$. Quindi il problema di Cauchy:
\begin{equation*}
    \begin{cases}
        yy' = 2\\
        y(0) = 1
    \end{cases}
\end{equation*}
\textit{non ha soluzione}. Quindi abbiamo trovato un esempio di problema di Cauchy in cui viene a mancare l'esistenza
di soluzioni. In altre situazioni potrebbe venire a mancare l'unicità delle soluzioni, come
mostra l'esempio successivo.
}

\subsection{Equazioni lineari del primo ordine}

In questa sezione andremo a trattare un caso particolare di ED ordinarie del primo ordine,
il caso in cui $F$ sia una funzione lineare rispetto a $y$ e $y'$.
In questo caso tali equazioni si possono scrivere nella forma:
\begin{equation*}
    a_1(t)y'(t) + a_0(t)y(t) = g(t)
\end{equation*}
con $a_1, a_0, g$ funzioni continue su un intervallo $I \subset \mathbb{R}$.
Se il coefficiente $a_1(t)$ non si annulla, allora l'ED lineare si può scrivere nella forma:
\begin{equation}
    y'(t) + a(t)y(t) = f(t)
\end{equation}
Anche in questo caso supporremo $a$ e $f$ sia funzioni continue su un intervallo $I \subset \mathbb{R}$.
Se $f$ non è identicamente nulla, la (1.3.1) è detta \textbf{equazione completa}. Se $f \equiv 0$ invece, 
l'equazione si dice \textbf{omogenea} e di solito, vista l'importanza che riveste tale equazione nella
struttura dell'integrale generale, si è soliti indicare con una lettera usualmente la $z$, la soluzione di tale equazione, che diventa perciò:
\begin{equation}
    z'(t) + a(t)z(t) = 0
\end{equation}
Vale il seguente teorema:
\thm{}{
    L'integrale generale dell'equazione completa si ottiene aggiungendo all'integrale generale dell'equazione omogenea una particolare soluzione dell'equazione completa.
}
\noindent
Dal teorema $1.3.1$ sappiamo dunque che dobbiamo occuparci prima dello studio dell'equazione omogenea e poi alla ricerca di una soluzione particolare
dell'equazione completa.
\\

\vspace{1em}
\noindent
\textbf{Ricerca dell'integrale generale dell'equazione omeogenea:} 
Sia $A(t)$ una primitiva di $a(t)$ (tale per cui si abbia $A'(t) = a(t)$). Moltiplichiamo entrambi i membri di (1.3.2) per $e^{A(t)}$; si ottiene:
\begin{equation*}
    e^{A(t)}z'(t) + a(t)e^{A(t)}z(t) = 0
\end{equation*}
da cui:
\[\frac{d}{dt}[z(t)e^{A(t)}] = 0\]
e cioè se $z(t)e^{A(t)} =  C$ che si riscrive come:
\begin{equation}
    z(t) = Ce^{-\int a(t) \; dt}
\end{equation} 
\textbf{Ricerca di una soluzione particolare dell'equazione completa:} 
Si utilizza il metodo delle variazioni delle costanti.
L'idea è di ricercare una soluzione simile alla (1.3.3); stavolta però 
$C$ non è più una costante, ma una funzione $C(t)$ da determinare. Cerchiamo
dunque una soluzione della forma:
\[\overline{y}(t) = C(t)e^{-A(t)}\]
La $C(t)$ deve essere determinata in modo tale che $\overline{y}$ così strutturata sia la soluzione
dell'equazione completa (1.3.1). Quindi osserviamo che:
\[\overline{y}'(t) = C'(t)e^{-A(t)} - C(t)a(t)e^{-A(t)}\]
si deduce inserendo le corrispondenti informazioni all'interno dell'equazione completa (1.3.1)
\[e^{-A(t)}[C'(t) - C(t)a(t)] + a(t)C(t)e^{-A(t)} = f(t)\]
cioè
\[e^{-A(t)}C'(t) = f(t)\]
da cui $C'(t) = e^{-A(t)}f(t)$ e dunque:
\[C(t) = \int e^{A(s)}f(s) \; ds\]
Quindi la soluzione particolare dell'equazione completa (1.3.1) è:
\[\overline{y}(t) = e^{-A(t)} \int f(s)e^{A(s)} \; ds\]
Quindi riassumendo l'integrale generale dell'equazione completa (1.3.1) ha la forma:
\begin{equation}
    y(t) = ce^{-A(t)} + e^{-A(t)} \int f(s)e^{A(s)} \; ds 
\end{equation}
\textbf{Risoluzione del problema di Cauchy:} la costante arbitraria nella (1.3.4) 
è determinata dalla condizione iniziale $y(t_0) = y_0$. Scegliendo una primitiva
tale per cui $A(t_0) = 0$ (cioè $A(t) = \int_{t_0}^{t_1} a(s) \; ds$) l'integrale
generale che soddisfa il dato di Cauchy è:
\begin{equation}
    y(t) = e^{-A(t)} \left[ y_0 + \int_{t_0}^t f(s)e^{A(s)} \; ds \right]
\end{equation}
Vale il seguente teorema:
\thm{\textbf{Problema di Cauchy per un'equazione differenziale lineare del primo ordine}}{
    Siano $a, f$ funzioni continue su un intervallo $I \ni t_0$ e sia $y_0 \in \mathbb{R}$. Il problema di Cauchy:
    \begin{equation*}
        \begin{cases}
            y'(t) + a(t)y(t) = f(t)\\
            y(t_0) = y_0
        \end{cases}
    \end{equation*}
    ha una e una sola soluzione $y \in C^1(I)$ (dove $C^k(I)$ è l'insieme delle funzioni continue derivabili fino all'ordine $k$ e con tutte le derivate fino all'ordine $k$ continue); tale soluzione
    è assegnata alla (1.3.5).
}

\subsection{Equazioni differenziali del secondo ordine}

\subsubsection{Generalità}

\dfn{}{
Un'equazione differenziale ordinaria del secondo ordine è un'equazione \textit{lineare} se è del tipo:
\begin{equation}
    y''(t) + a(t)y'(t) + b(t)y(t) = g(t)
\end{equation}
}
\noindent
dove i coefficienti $a_i$ e il termine noto $g$ sono funzioni definite in un certo intervallo $I$ 
e continue nello stesso intervallo. L'equazione si dice omogenea se $g$ è identicamente nullo; in caso 
contrario si dice completa.
L'equazione si dice \textbf{a coefficienti costanti} se i coefficienti $a_i$ sono costanti (per inciso, il termine noto può invece dipendere da t); in caso
contrario si dice \textbf{a coefficienti variabili}. Infine se $a_1$ non si annulla mai, l'equazione si può riscrivere in forma nornmale come:
\begin{equation}
    y''(t) + a(t)y(t) + b(t)y = f(t)
\end{equation}
\textbf{Osservazione:} consideriamo il seguente operatore
\[L: C^2(I) \rightarrow C^0(I)\]
\[L: y \mapsto Ly\]
dove gli spazi $C^k(I)$, sono gli spazi delle funzioni continue derivabili fino all'ordine $k$ e con tutte
le derivate fino all'ordine $k$ continue e dove abbiamo indicato con $Ly$ il primo membro dell'equazione (1.4.1).
Prima di tutto l'operatore è ben definito, perché se $y \in C^2$ si ha che $Ly \in C^0(I)$ e questo grazie alla continuità 
dei coefficienti $a_i$. Inoltre è facile dimostrare che $L$ è un operatore lineare, nel senso che per ogni $\lambda_1, \lambda_2 \in \mathbb{R}$, per ogni $y_1, y_2 \in C^2(I)$ si ha:
\[L(\lambda_1y_1 + \lambda_2y_2) = \lambda_1Ly_1 + \lambda_2Ly_2\]
Questa proprietà dell'operatore $L$ giustifica il motivo per cui l'equazione (1.4.1) è detta lineare.
\ex{}{
    Il più semplice esempio di equazione differenziale del secondo ordine:
    \[y''(t) = 0\]
    Essa naturalmente equivale a dire $y'(t) = C_1$ e da cui $Y(t) = C_1t + C_2$, con $C_1, C_2$ costanti arbitrarie.
    Quindi le soluzioni di questa equazione sono tutte e soli polinomi di primo grado.
}
\ex{\textbf{Oscillatore armonico}}{
    Consideriamo un punto materiale di massa $n$ che rimane libero di muoversi in linea orizzontale,
    attaccato a una molla che esercita una forza di richiamo di tipo elastico. Denotiamo con $y(t)$ la posizione
    del punto sulla retta (rispetto alla configurazione di riposo). Allora si può dimostrare che $y$ soddisfa l'equazione:
    \[my'' = -ky\]
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[scale=1]
            % Disegno della parete
            \fill[gray!50] (0,0) rectangle (0.5,3);
            \node[below] at (0.25,0) {Parete};
        
            % Disegno della molla
            \draw[line width=1pt] (0.5,1.5) -- (1,1.5);
            \draw[line width=1pt] (1,1.5) 
                .. controls (1.2,1.8) and (1.4,1.2) .. (1.6,1.5)
                .. controls (1.8,1.8) and (2.0,1.2) .. (2.2,1.5);
            \draw[line width=1pt] (2.2,1.5) -- (2.7,1.5);
            \node[above] at (1.4,1.8) {Molla};
        
            % Disegno della massa
            \filldraw[fill=blue!30, draw=blue!70, line width=1pt] (2.7,0.5) rectangle (4.2,2.5);
            \node at (3.45,0.3) {Massa};
        \end{tikzpicture}
    \end{figure}
    dove $k > 0$ denota la costante elastica del sistema. Siccome ovviamente $m \neq 0$, allora si può riscrivere
    l'equazione in forma normale come:
    \begin{equation}
        y'' + \omega^2y = 0
    \end{equation}
    dove $\omega^2 = \frac{k}{m} > 0$. L'equazione prende il nome dell'oscillatore armonico.
    L'equazione (1.4.3) è un'equazione differenziale lineare del secondo ordine omogenea e a coefficienti costanti. Se sul
    punto agisce una forza esterna (dipendente solo dal tempo $t$) l'equazione si riscrive come:
    \[y'' + \omega^2y = f(t)\]
    nel caso venga presa in considerazione lo smorzamento dovuto all'attrito, l'equazione si trasforma in
    \[y'' + hy' + \omega^2y = 0\]
    con $h > 0$. 
    Sarà chiaro in seguito che l'integrale generale di una qualunque equazione lineare del secondo ordine dipende da due parametri
    arbitrarie. Quindi per determinare una soluzione specifica è necessario conoscere due condizioni iniziali.
}
\dfn{}{
    Si dice \textbf{problema di Cauchy} per un'equazione differenziale lineare del secondo ordine (per semplicità la consideriamo espressa in forma normale), il problema
    \begin{equation}
        \begin{cases}
            y''(t) + a(t)y'(t) + b(t)y(t) = g(t)\\
            y(t_0) = y_0\\
            y'(t_0) = y'_0
        \end{cases}
    \end{equation}
}
\thm{Esistenza e unicità per il problema di Cauchy (1.4.4)}{
    Siano $a, b, f$ funzioni continue in un intervallo $I \ni t_0$. Allora per ogni
    $y_0, y_1 \in \mathbb{R}$ il problema di Cauchy (1.4.4) ha una e una sola soluzione $y \in C^2(I)$. 
}
\noindent
Questo risultato è analogo al corrispondente enunciato per le equazioni differnziali lineari del primo ordine.
Anche in questo caso, la soluzione sarà ottenuta imponendo le condizioni iniziali nell'espressione che individua l'integrale generale dell'equazione (1.4.1).
Quindi il problema si riduce a comprendere come si possa determinare l'integrale generale dell'equazione differenziale lineare del secondo ordine (1.4.1).

\subsubsection{La struttura dell'integrale generale}

L'equazione $Lz = 0$ si dice \textit{equazione omogenea} associata all'
equazione completa $Ly = f$. Il seguente teorema permette di determinare facilmente
la struttura dell'integrale generale dell'equazione (1.4.1). Questo risultato 
non usa il fatto che $Ly = f$ sia un'equazione differenziale e non dipende dall'ordine 
dell'equazione, sfrutta solamente il fatto che $L$ sia un operatore lineare.

\thm{\textbf{Struttura dell'integrale generale dell'equazione linaere completa}}{
    Si può dimostrare che:
    \begin{enumerate}
        \item L'insieme delle soluzioni all'equazione omogenea $Lz = 0$ in un dato intervallo $I$ è uno spazio vettoriale di dimensione 2 (sottospazio di $C^2(I)$).
        \item L'integrale generale dell'equazione completa si ottiene sommando l'integrale generale dell'equazione omeogene
        e una soluzione particolare dell'equazione completa.
    \end{enumerate}
}
\ex{}{
    Si consideri l'equazione $t^2z'' - 3tz' + 3z = 0$. Sia $z_1 = t$. Allora
    $z_1'(t) = 1, z_2''(t) = 0$ da cui $t^20- 3t \, 1 + 3t = -3t + 3t = 0$; quindi $z_1$ è una soluzione dellì'equazione data.
    Sia ora $z_2 = t^3$. Allora $z_2'(t) = 3t^2, z_2''(t) = 6t$ da cui inserendo le informazioni nell'equazione si ottiene 
    $t^26t-3t3t^2 + 3t^3 = 6t^3 - 9t^3 + 3t^3 = 0$; quindi $z_2$ è una soluzione dell'equazione data.
    Le due soluzioni sono linearmente indipendenti, quindi l'integrale generale dell'equazione data è:
    \[z(t) = C_1t + C_2t^3\]
    al variare di $C_1, C_2 \in \mathbb{R}$. 
}
\noindent
Nel caso precendente è particolarmente facile vedere che le due soluzioni proposte sono linearmente indipendenti; 
In generale può non essere così immediato. Il seguente critico generale permette di decidere qualora due soluzioni
proposte siano o meno linearmente indipendenti.
\thm{\textbf{Determinante wroskiano e indipendenza}}{
    Siano $z_1, z_2$ due funzioni $C^2(I)$ soluzioni dell'equazione lineare omogenea:
    \[Lz \equiv z'' + a(t)z' + b(t) = 0\]
    nell'intervallo $I$. Allora esse sono linearmente indipendenti in $C^2(I)$ se e soltanto se la seguente matrice:
    \[W(t) = \begin{pmatrix}
        z_1(t) & z_2(t)\\
        z_1'(t) & z_2'(t)
    \end{pmatrix}\]
    detta \textbf{matrice Wronskiana} ha \textit{determinante} diverso da zero per ogni $t \in I$ (dalla regolarità delle soluzioni, 
    è sufficiente che il determinante di tale matrice sia diverso da zero in un punto $t_0 \in I$). 
}
\noindent
\textbf{Quindi} per determinare l'integrale generale dell'equazione lineare completa occorre:
\begin{enumerate}
    \item Determinare l'integrale generale dell'equazione omogenea, quindi serve determinare due soluzioni $z_1(t), z_2(t)$ dell'equazione omogenea linearmente indipendenti;
    \item Determinare una soluzione particolare dell'equazione completa. A questo punto l'integrale generale dell'equazione completa è dato da:
    \[\overline{y}(t) + C_1z_1(t) + C_2z_2(t)\]
    al variare di $C_1, C_2 \in \mathbb{R}$. 
\end{enumerate}

\subsubsection{Equazioni differenziali del secondo ordine omogenee a coefficienti costanti}

Consideriamo l'equazione differenziale lineare omogenea del secondo ordine a coefficienti costanti:
\[z''(t) + az'(t) + bz(t) = 0 \; \; \text{ con $a, b$ costanti}\]
In analogia con il caso delle equazioni differenziali del primo ordine, che ammette gli esponenziali come soluzioni, si cerca
anche qui una soluzione del tipo $t \mapsto e^{rt}$ con $r \in \mathbb{C}$. Sostituendo tale soluzione nell'equazione omogenea si ottiene:
\[z'(t) = re^{rt} \; \; \; \; \; \; \; \; z''(t) = r^2e^{rt}\]
\[e^{rt}(r^2 + ar + b) = 0\]
Siccome l'esponenziale è sempre positiva, affinché l'equazione precedente abbia soluzione
è necessario che la costante $r$ sia una radice dell'equazione di secondo grado:
\[r^2 + ar + b = 0\]
che viene detta equazioen caratteristica dell'equazione differenziale. Si possono avere tre casi in base al segno del discriminante:

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|}
    \hline
    Discriminante & Soluzione 1 & Soluzione 2\\
    \hline
    $\Delta > 0$ & $e^{r_1t}$ & $e^{r_2t}$\\
    $\Delta = 0$ & $e^{rt}$ & $te^{rt}$\\
    $\Delta < 0$ & $e^{\alpha t}\cos{\beta t}$ & $e^{\alpha t}\sin{\beta t}$ \\
    \hline
\end{tabular}
\end{figure}

\ex{}{
    \textit{Risolvere il seguente problema di Cauchy:}
    \begin{equation*}
        \begin{cases}
            z'' - 2z' - 3z = 0\\
            z(0) = 1\\
            z'(0) = 2
        \end{cases}
    \end{equation*}
    L'equazione proposta è differenziale ordinaria del secondo ordine lineare, a coefficienti
    costanti e omogenea. La sua equazione caratteristica è:
    \[r^2 - 2r - 3 = 0\]
    che ha soluzioni $r_1 = 3, r_2 = -1$. Quindi l'integrale generale dell'equazione omogenea è:
    \[z(t) = C_1e^{3t} + C_2e^{-t}\]
    al variare di $C_1, C_2 \in \mathbb{R}$. Imponendo le condizioni iniziali si ottiene:
    \[z(0) = C_1 + C_2 = 1\]
    \[z'(0) = 3C_1 - C_2 = 2\]
    da cui 
    \[
    \begin{cases}
        C_1 + C_2 = 1\\
        -C_1 + 3C_2 = 2
    \end{cases} \Longrightarrow 
    \begin{cases}
        C_1 = \frac{1}{4}\\
        C_2 = \frac{3}{4}
    \end{cases}
    \]
    Quindi la soluzione del problema di Cauchy proposto è:
    \[z(t) = \frac{1}{4}e^{3t} + \frac{3}{4}e^{-t}\]
}

\subsubsection{Equazioni lineari del secondo ordine non omogenee: Metodo di somiglianza}

Consideriamo l'equazione differenziale lineare del secondo ordine a coefficienti costanti:
\[y''(t) + ay'(t) + by(t) = f(t) \; \; \text{ con $a, b$ costanti}\]
nel caso in cui il termine noto $f$ abbia una forma particolare. 
Un metodo più generale si basa sul metodo di variazione delle costanti, che permette di determinare una soluzione particolare dell'equazione completa.
Quando $f$ è semplice si cerca una soluzione che \textit{assomiglia} nel senso che andremo a specificare:
\begin{itemize}
    \item Se $f(t) = p_r(t)$ è un polinomio di grado $r$, si cerca una soluzione che sia anch'essa un polinomio, con le seguenti caratteristiche:
    \begin{figure}[H]
        \centering
        \begin{tabular}{|c|c|}
            \hline
            $\overline{y}(t) = q_r(t)$ & se $b \neq 0$\\
            $\overline{y}(t) = tq_r(t)$ & se $b = 0$ e $a \neq 0$\\
            $\overline{y}(t) = t^2q_r(t)$ & se $b = 0$ e $a = 0$\\
            \hline
        \end{tabular}
    
    \end{figure}
    \item Se $f(t) = Ae^{\lambda t}$ con $\lambda \in \mathbb{C}$. In tal caso, si cerca una soluzione del tipo:
    $y(t) = e^{\lambda t}\gamma(t)$ con $\gamma(t)$ polinomio di grado opportuno.
    \item Se infine $\lambda^2 + a\lambda + b = 0$ e $2\lambda + a = 0$ allora si ha $\gamma(t) = A$ da cui:
    \[\gamma(t) = \frac{A}{2}t^2 \; \; \; \; \; \;  \overline{y}(t) = \frac{A}{2}t^2e^{\lambda t}\] 
    Quindi si ha una soluzione particolare:
    \[\overline{y}(t) = Ct^2e^{\lambda t}\]
    con $C \in \mathbb{R}$. 
    Osserviamo che in questa classe particolare di termini noti del tipo $Ae^{\lambda t}$ con $\lambda \in \mathbb{C}$ rientrano anche in casi:
    \[\cos{\omega t},\sin{\omega t}, e^{\mu t}\cos{\omega t}, e^{\mu t}\sin{\omega t} \; \; \; \text{con $\omega \in \mathbb{R}$ }\]

\end{itemize}

\ex{}{
Si trovi una soluzione particolare dell'equazione differenziale:
\[y'' + 3y = t + 2\cos{t}\]
Per trovare una soluzione particolare dell'equazione non omogenea dobbiamo cercare una funzione $f(t)$ tale che:
\[f(t) = t + 2\cos{t}\]
Questo termine è la somma di due funzioni:
\begin{itemize}
    \item $t$, un polinomio di primo grado
    \item $2\cos{t}$, una funzione trigonometrica
\end{itemize}
Per questo motivo cerchiamo una soluzione particolare come la somma di due soluzioni, ciascuna
ciascuna associata a uno dei due termini:
\[y_p = y_{p1} + y_{p2}\]
Quindi andiamo per step e cerchiamo di trovare una soluzione per il termine $t$ e un'altra per il termine $2\cos{t}$.
\begin{itemize}
    \item Per il termine $t$ che è un polinomio di primo grado, si cerca una funzione del tipo:
    \[y_{A_1}(t) = C_1t + C_0\]
    dove $A, B$ sono coefficienti da determinare.
    Calcoliamo le derivate:
    \[y_{A1}'(t) = C_1\]
    \[y_{A1}''(t) = 0\]
    Ora sostituiamo le derivate nell'equazione differenziale:
    \begin{align*}
        y'' + 3y &= t\\
        0 + 3(C_1t + C_0) &= t\\
        3C_1 + 3C_0 &= t
    \end{align*}
    Quindi $C_1 = 1/3$ e $C_0 = 0$. Quindi la soluzione per il termine $t$ è:
    \[y_{p1}(t) = \frac{1}{3}t\]
    \item Procediamo ora per $2\cos{t}$ che è una funzione trigonometrica. Si cerca una soluzione del tipo:
    \[y_{A2}(t) = A\gamma(t)e^{\lambda t}\]
    dove $\gamma(t)$ è ancora sconosciuta, $\lambda = i$ e $A = 2$.
    Quindi:
    \[y_{A2}(t) = 2\gamma(t)e^{it}\]
    Troviamo $\gamma(t)$, dato $a = 0$ e $b = 3$, si ha:
    \begin{align*}
        \gamma'' + (2\lambda + a)\gamma' + (\lambda^2 + \lambda a + b)\gamma &= A\\
        \cancel{\gamma''} + \cancel{2i\gamma'} + 2\gamma &= 2\\
        2\gamma &= 2 \Longrightarrow \gamma = 1
    \end{align*}
    Quindi la soluzione per il termine $2\cos{t}$ è:
    \[y_{p2}(t) = 2e^{it}\]
    Di conseguenza sommando le due soluzioni particolari si ottiene la soluzione particolare dell'equazione non omogenea:
    \[y_p(t) = \frac{1}{3}t + 2e^{it}\]
\end{itemize}
}

\pagebreak

\fancyhead[RO,LE]{\textbf{Calcolo infinitesimale per le curve}}

\section{Calcolo infinitesimale per le curve}

\subsection{Richiami di calcolo vettoriale}

Per lo studio del calcolo infinitesimale in più variabili occorre un certo uso del calcolo vettoriale.
In questo paragrafo andiamo a richiamare le nozioni di base che verranno incontrate nel seguito.
Ambientiamo tutto in $\mathbb{R}^n$; gli elementi di $\mathbb{R}^n$ li chiamano vettori e vengono
indicati con $x = (x_1, x_2, \dots, x_n)$. Gli $x_i$, $i = 1, \dots n$ si dicono componenti del vettore. 
Il modulo di un vettore è definito come:
\[|x| = \sqrt{\sum_{i = 1}^{n}x_i^2}\]
mentre si dice un versore un vettore di modulo unitario e si indica con:
\[vers(x) = \frac{x}{|x|}\]
$\mathbb{R}^n$ è dotato della base canonica:
\[e_0 = \begin{pmatrix}
    1\\
    0\\
    \vdots\\
    0
\end{pmatrix}, e_1 = \begin{pmatrix}
    0\\
    1\\
    \vdots\\
    0
\end{pmatrix}, \dots, e_n = \begin{pmatrix}
    0\\
    0\\
    \vdots\\
    1
\end{pmatrix}\] 
e ogni vettore $x \in \mathbb{R}^n$ si può scrivere come combinazione lineare della base canonica:
\[x = x_1e_1 + x_2e_2 + \dots + x_ne_n = \sum_{i=1}^{n} x_je_j\]
I versori della base canonica sono soliti indicati con $i, j, k$ (in $\mathbb{R})$. 
Per due vettori $x, y \in \mathbb{R}^n$ si definisce il prodotto scalare:
\[x \cdot y = \sum_{i=1}^{n}x_iy_i\]
Il risultato di un prodotto scalare è uno scalare. 
Se $x \cdot y = 0$ allora i due vettori si dicono ortogonali. 
Si usa la notazione $x \cdot y = |x||y|\cos{\theta}$, dove $\theta$ è l'angolo tra i due vettori.
\[\cos{\theta} = \frac{u \cdot v}{|u||v|}\]
Infinite introduciamo anche un'altra operazione che agisce tra due vettori di $\mathbb{R}^3$ e che da come risultato
un vettore. Questa operazione è il prodotto vettoriale. Si indica con $u \times v$.

\subsection{Funzioni a valore vettoriale, alcune definizioni}

\dfn{}
{   
    Sia $f : A \subset \mathbb{R} \mapsto \mathbb{R}$ con $A$ \textit{dominio} e $f$ la \textit{legge} che ad ogni elemento di $A$
    associa un solo elemento di $f(A)$ (chiamata immagine di $A$ tramite $f$) anch'esso reale.\\
    Si dice \textbf{\textit{funzione ad una variabile}} una funzione definita su $A \in \mathbb{R}$. \\
    Si dice \textbf{\textit{funzioni di più variabili}} una funzione definita su $A \subset \mathbb{R}^n$ con $n > 1$.\\
    Si dice \textbf{\textit{funzioni a valori reali}} una funzione che ha immagine $f(A) \subseteq \mathbb{R}$.\\
    Si dice \textbf{\textit{funzioni a valori vettoriali}} una funzione che ha immagine $f(A) \subseteq \mathbb{R}^n$.
}

\subsection{Calcolo differenziale e integrale per funzioni a valori vettoriali}

Concentriamoci ora su elementi del tipo $f : A \subset \mathbb{R} \mapsto \mathbb{R}^m, m > 1$. 
Questo è il caso più semplice di funzioni a valori vettoriali. Nel caso $m = 2,3$ 
vedremo che le funzioni hanno il significato geometrico di curve nel piano o nello spazio. 
Sia dunque $r:  I \mapsto \mathbb{R}^m$ con $I \subset \mathbb{R}$ un intervallo. Sia $t_0 \in I$ e sia $I \in \mathbb{R}^m$.
\dfn{}
{
    Si dice che:
    \[\lim_{t\rightarrow t_0} r(t) = l\]
    se
    \begin{equation}
        \lim_{t\rightarrow t_0} |r(t) - l| = 0
    \end{equation}
}
\noindent
Geometricamente la (2.3.1) significa che la distanza tra $r(t)$ e $l$ tende a zero per $t$ che tende a $t_0$ e questo
è possibile solo se le distanze tra le componenti di $r(t)$ e $l$ tendono a zero rispettivamente.
Quindi se $r(t) = (r_1(t), r_2(t), \dots, r_m(t))$ con $r_i(t)$ funzioni a valori reali, si ha:
\begin{equation}
    \lim_{t\rightarrow t_0} r(t) = l \Longleftrightarrow \lim_{t\rightarrow t_0} r_i(t) = l_i \; \; \; \; \; i = 1, \dots, m  
\end{equation}
\ex{}
{
    Sia $r: I \mapsto \mathbb{R}^3$ definita da $r(t) = (\cos{t} + \pi, e^t - 1, \sin{t^2})$. 
    Calcoliamo $\lim_{t \rightarrow 0} r(t)$. 
    Dalla (2.3.2) si ha che:
    \[\lim_{t \rightarrow 0} r(t) = \lim_{t \rightarrow 0} (r_1(t), r_2(t), r_3(t)) = \left(\lim_{t \rightarrow 0}(\cos{t} + \pi), \lim_{t \rightarrow 0} (e^{t} - 1), \lim_{t \rightarrow 0} (\sin{t^2})\right) = (1+\pi, 0, 0)\]
}

\dfn{}
{
    Sia $r: I \mapsto \mathbb{R}^m$ e sia $t_0 \in I$. Si dice che $r$ è derivabile in $t_0$ se esiste finito:
    \begin{align*}
        r'(t) &= \lim_{t \rightarrow t_0} \frac{r(t_0 + h) - r(t_0)}{h}\\
        &= \left(\lim_{h \rightarrow 0} \frac{r_1(t_0 + h) - r_1(t)}{h}, \dots, \lim_{h \rightarrow 0} \frac{r_m(t_0 + h) - r_m(t)}{h}\right)\\
        &\stackrel{(2.3.2)}{=} (r'_1(t_0), \dots, r'_m(t_0))
    \end{align*}
}
\noindent
I limiti di funzioni a valori vettoriali si fanno componente per componente. Quindi
anche la derivata di una funzione a valori vettoriali si calcola si fa componente per componente: il vettore derivata è il vettore delle derivate delle componenti.
\dfn{}
{
    Se $r$ è derivabile in tutto $I$ e inoltre la funzione $r'$ è continua in $I$, si dice
    che $r$ è di classe $C^1(I)$ e si scrive $r \in C^1(I)$. Analogamente si definiscono le derivate di ordine successivo e le funzioni di classe $C^k(I)$ per $k>1$. 
}

\thm{\textbf{Teorema fondamentale del calcolo integrale}}
{
    Se $r : [a,b] \mapsto \mathbb{R}^m$ è di classe $C^1([a,b])$ allora: 
    \[\int^{a}_b r'(t)dt = r(b) - r(a)\]
}
\noindent
Un altro risultato importante è il seguente:
\[\left|\int_a^b r(t)dt\right| \le \int_a^b |r(t)|dt\]
Osserviamo che il primo membro è l'integrale di una funzione a valori vettoriali, mentre il secondo è l'integrale di una funzione a valori scalari, cioè $|r(t)|$. 
In entrambi i casi, i moduli che compaiono nella formula sono moduli di vettori e non valori assoluti.

\subsection{Curve regolari}

Consideriamo un punto materiale che si muove nello spazio 
tridimensionale lungo una certa traiettoria; le sue coordinate sono funzioni
del tempo, quindi la sua posizione all'istante $t$ può essere descritta come 
da una funzione di questo tipo: 
\[r: [t_1, t_2] \mapsto \mathbb{R}^3\]
dove $r = (x,y,z)$ e 
\[
\begin{cases}
    x=x(t)\\
    y=y(t)\\
    z=z(t)
\end{cases}
\] 
con $t \in [t_1, t_2]$. Quindi si può riscrivere:
\[r(t) = x(t)i + y(t)j + z(t)k\]
usando la notazione vettoriale che abbiamo usato in precedenza:
\[r(t) = (x(t), y(t), z(t))\]
\dfn{
}
{
    Sia $I \subset \mathbb{R}$ un intervallo. Si dice \textbf{arco di curva continua} o \textbf{cammino} in
    $\mathbb{R}^m$ una funzione $r: I \rightarrow \mathbb{R}^m$ che sia continua, nel senso che le sue componenti sono funzioni continue.
}
\noindent
Se si pensa alla variabile $t$ come al tempo, allora $r(t)$ è la posizione del punto materiale al tempo $t$.

\begin{figure}[H]
\centering
\begin{tikzpicture}
    \begin{axis}[
        domain=-3:3,
        samples=50,
        xlabel={$x$},
        ylabel={$f(x)$},
        grid=both,
        width=10cm,
        height=7cm,
      ]
      \addplot[blue, thick] {x^3 - 3*x};
    \end{axis}
  \end{tikzpicture}
\end{figure}
\dfn{}
{
    Un arco di curva continua si dice \textbf{chiusa} se $r(a) = r(b)$ con $I = [a,b]$ (cioè punto iniziale e punto finale coincidono).
    Si dice \textbf{semplice} se non ripassa mai dallo stesso punto. Si dice \textbf{piana} se esiste un piano che contiene il suo sostegno.\\
    Si dice \textbf{regolare} se l'arco di una curva $r: I \mapsto \mathbb{R}^m$ tale che $r \in C^1(I)$ e $r'(t) \neq 0$ per ogni $t \in I$. 
}
Quindi il vettore derivato esiste in ogni punto, varia con continuità e non è mai nullo.
Di conseguenza, per le curve regolari è ben definito il \textbf{versore tangente}:
\[T(t) = \frac{r'(t)}{|r'(t)|}\]
che inoltre dipende con continuità da $t$.

\subsection{Lunghezza di un arco di curva}

Sia $r: [a,b] \mapsto \mathbb{R}^m$ la parametrizzazione di un arco di curva $\gamma$ continuo e consideriamo una \textit{partizione}:
\[\mathcal{P} = \{a = t_0,t_1,t_2,\dots,t_{n_1},t_n = b\}\]
dell'intervallo $[a,b]$ con $t_0 < t_1 < \dots < t_n$ non necessariamente equispaziati.
Alla partizione $\mathcal{P}$ risulta associata la poligonale inscritta in $\gamma$ costituita da $n$ segmenti di estremi $R(t_{i-1})$ e $R(t_i)$.
Sia $\mathcal{L}(P)$ la lunghezza di tale poligonale, da cui si ha:
\[\mathcal{L}(P) = \sum_{i=1}^n |r(t_i) - r(t_{i-1})|\]
Siccome intuitivamente il segmento è la linea più breve che congiunge due punti, l'idea è che $\mathcal{L}(\mathcal{P})$ approssimi la lunghezza di $\gamma$;
per cui facendo variare in tutti i modi possibili la partizione $\mathcal{P}$ e prendendo l'estremo superiore
delle $\mathcal{L}(\mathcal{P})$ si ottenga la lunghezza dell'arco di curva $\gamma$. Si ha perciò la seguente definizione:
\dfn{}
{
    Si dice che $\gamma$ è \textbf{rettificabile} se
    \[sup \; \mathcal{L}(\mathcal{P}) = \mathcal{L}(\gamma) < \infty\]
    dove il $\sup$ è preso su tutte le partizioni $\mathcal{P}$ dell'intervallo $[a,b]$. In tal caso si dice \textbf{lunghezza} di $\gamma$.
}
\thm{}
{
    Sia $r: [a,b] \mapsto \mathbb{R}^m$ la parametrizzazione di un arco di curva $\gamma$ regolare. Allora $\gamma$ è rettificabile e:
    \[\mathcal{L}(\gamma) = \int_{a}^{b} |r'(t)|dt\]
}
\noindent
Una classe particolare di curve piane è data dalle curve che si ottengono come grafici di funzioni di una variabile, ossia $y = f(x)$ per $x \in [a,b]$. 
In tal caso la curva può essere scritta in forma parametrica ponendo:
\[
\begin{cases}
    x = t\\
    y = f(t)
\end{cases}
\]
per $t \in [a,b]$. Una curva espressa come grafico di una funzione $f$ ha le seguenti proprietà:
\begin{enumerate}
    \item è continua se e solo se $f$ è continua in $[a,b]$;
    \item è regolare se e solo se $f$ è derivabile in $[a,b]$.
    \item è regolare a tratti se e solo se $f$ è a tratti derivabile in $[a,b]$.
    \item non è mai chiusa
    \item è sempre semplice
\end{enumerate}
Nel caso di una curva espressa come grafico di una funzione, per calcolare la lunghezza della curva si usa la formula:
\[\mathcal{L}(\gamma) = \int_{a}^{b} \sqrt{1 + f'(t)^2}dt = \int_{a}^{b} ||(1, f'(t))||dt\]
\textbf{Curve piane in forma polare}: una forma alternativa per descrivere una curva piana è data dalle curve in forma polare.
Una curva in forma polare è data da un'equazione del tipo:
\[\rho = f(\theta),\; \; \; \; \; \; \theta \in [\theta_1, \theta_2]\]
\[\begin{cases}
    x = \rho\cos{\theta}\\
    y = \rho\sin{\theta}
\end{cases}\]
da cui possiamo osservare che:
\[r'(\theta) = (f'(\theta)\cos{\theta} - f(\theta)\sin{\theta}, f'(\theta)\sin{\theta} - f(\theta)\cos{\theta})\]
da cui 
\[|r'(\theta)| = \sqrt{f'(\theta)^2 + f(\theta)^2}\]


\subsection{Il parametro arco}

 Si osserva che la lunghezza di un arco di una curva $r(\tau)$ per $\tau \in [t_0, t]$ è una funzione di $t$ e si ha 
\[s(t) = \int_{a}^{t} |r'(\tau)|d\tau\]
se si è in grado di calcolare esplicitamente tale funzione e poi invertirla, esprimendo $t$ in funzione di $s$, allora è possibile riparametrizzare la curva in funzione del parametro $s$ detto
\textbf{parametro arco} o \textbf{ascissa curvilinea}. Il vantaggio è che $s$ è esattamente lo spazio percorso dal punto mobile quando il parametro passa da 0 a $s$. 

\subsection{Integrali di linea}

\dfn{Integrale di linea}
{
    Sia $r : [a,b] \mapsto \mathbb{R}^m$ un arco di curva regolare di sostegno $\gamma$ e sia $f$ una funzione a valori reali, definita su un sottoinsieme $A \subset \mathbb{R}^m$ contenente
    $\gamma$, cioè $f: \mathbb{R}^m \mapsto \mathbb{R}$ con $\gamma \subset A$. Allora si dice integrale di linea (di prima specie) di $f$ lungo $\gamma$:
    \[\int_{\gamma} f \; ds = \int_{a}^{b} f(r(t))|r'(t)|dt\]
}
\noindent
\textbf{Significato geometrico dell'integrale di linea di prima specie: } se $m = 2$, $f$ è positiva e continua e $\gamma$ è parametrizzata con il parametro arco, il precedente integrale ha un'interessante interpretazione geometrica. 
Infatti sia $S$ la superficie in $\mathbb{R}^3$ formata dai segmenti che congiungono i punti di $\gamma$ con il grafico di $f$.
Visto che $\gamma$ è parametrizzata con il parametro arco, se pensiamo di sviluppare in un piano $(x,y)$ la superficie $S$ in modo da far coincidere
$\gamma$ con l'asse delle $x$ allora il parametro arco coincide con l'ascissa $x$ e l'integrale $\int_{\gamma} f ds$ rappresenta l'area della superficie sviluppata e quindi l'area della superificie originale.
\\
\textit{L'integrale di $f$ di prima specie lungo $\gamma$ è invariante per parametrizzazioni equivalenti ed anche per cambiamento di orientazione su $\gamma$}.

\begin{figure}[H]
    \centering
\begin{tikzpicture}[scale=2]
  % Draw coordinate axes
  \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$};
  \draw[->] (0,-1.5) -- (0,1.5) node[above] {$y$};

  % Draw closed curve C (a circle of radius 1, starting at (1,0))
  \draw[thick, red, ->] (1,0) arc (0:360:1);
  
  % Mark an element on the curve with a blue arrow indicating d\vec{r}
  \draw[blue, ->] (0.7,0.7) -- +(0.2,0.1) node[above] {$d\vec{r}$};

  % Draw a sample vector from the field (green arrow) on the curve
  \draw[green!70!black, ->] (0.7,0.7) -- +(0.3,0) node[right] {$\vec{F}$};

  % Add a label for the curve
  \node at (1.2,-0.8) {$C$};

  % Write the line integral expression below the diagram
  \node at (0,-1.8) {$\displaystyle \int_C \vec{F}\cdot d\vec{r}$};
\end{tikzpicture}
\end{figure}

\ex{}
{
    Nel piano $\mathbb{R}^2$ consideriamo la funzione a due variabili $f(x,y) = x^2 + y^2$ e vogliamo determinare l'integrale di linea di $f$ lungo l'arco di curva $\gamma$ di equazioni parametriche:
    \[\begin{cases}
        x = t\\
        y = t 
    \end{cases}
    \]
    con $t \in [0,1]$. La curva $\gamma$ è un segmento di retta che congiunge i punti $(0,0)$ e $(1,1)$.
    La parametrizzazione di $\gamma$ è data da $r(t) = (t,t)$ con $t \in [0,1]$. Calcoliamo l'integrale di linea di $f$ lungo $\gamma$:
    \begin{align*}
        \int_{\gamma} f(x,y) ds &= \int_{a}^{b} f(x(t), y(t)) \sqrt{(x')^2 + (y')^2} dt\\
        &= \int_{0}^{1} (t^2 + t^2)\sqrt{1^2 + 1^2} dt\\
        &= \int_{0}^{1} 2t^2\sqrt{2} dt\\
        &= \frac{2\sqrt{2}}{3} [t^3]^1_0 = \frac{2\sqrt{2}}{3}\\
    \end{align*}
    \noindent
    Il significato geometrico dell'integrale di linea di $f$ lungo $\gamma$ è l'area della superficie formata dai segmenti che congiungono i punti di $\gamma$ con il grafico di $f$.


    \begin{figure}[H]
        \centering
    \begin{tikzpicture}
        \begin{axis}[
          view={60}{30},
          xlabel={$x$},
          ylabel={$y$},
          zlabel={$f(x,y)$},
          domain=0:1,
          samples=20,
          zmin=0, zmax=2.5,
          colormap/viridis,
          ]
          % Superficie: f(x,y)=x^2+y^2 per (x,y) in [0,1]x[0,1]
          \addplot3[
            surf,
            domain=0:1,
            y domain=0:1,
            opacity=0.6,
          ]
          {x^2 + y^2};
      
          % Curva gamma: x=t, y=t, z = 2t^2, t in [0,1]
          \addplot3[
            thick,
            red,
            samples=50,
            domain=0:1,
          ]
          ({x}, {x}, {2*x^2});
          % Etichetta per la curva
          \node[red] at (axis cs:0.5,0.5,1) {$\gamma$};
        \end{axis}
      \end{tikzpicture}
    \end{figure}
    \textbf{Osservazione}: Se ruotassimo la curva $\gamma$ attorno all'asse $z$ di $-\frac{\pi}{4}$, otterremo un normalissimo integrale con una funzione ad una variabile. 
    L'integrale equivalente sarebbe:
    \[\int_0^{\sqrt{2}} x^2 dx \]
    \begin{figure}[H]
    \centering 
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$},
            ylabel={$y$},
            axis lines=middle,
            domain=0:2,
            samples=100,
            xtick={0,1.414},
            xticklabels={$0$, $\sqrt{2}$},
            ytick=\empty,
            width=8cm,
            height=6cm,
          ]
          % Tracciamo la curva y = x^2
          \addplot [name path=curve, blue, very thick] {x^2};
          
          % Definiamo l'asse y=0 (il bordo inferiore per il riempimento)
          \addplot [name path=axis, draw=none] {0};
          
          % Riempimento dell'area sottesa alla curva tra x=0 e x=sqrt(2)
          \addplot [blue!30, opacity=0.5] fill between[
              of=curve and axis,
              soft clip={domain=0:1.414}
          ];
        \end{axis}
      \end{tikzpicture}
    \end{figure}
}

\pagebreak

\section{Calcolo infinitesimale per funzioni reali a più variabili}

\subsection{Grafico, linee di livello e domini}

\dfn{}
{
    Sia $f: A \subset \mathbb{R}^n \mapsto \mathbb{R}$ una funzione a due variabili. Si dice \textbf{grafico} di $f$ l'insieme:
    \[G_f = \{(x,f(x)) : x \in A\} \subseteq \mathbb{R}^{n+1}\]
}
Un altro modo per rappresentare graficamente una funzione reale a più variabili reali è tramite gli insiemi di livello.
\dfn{}
{
    Sia $f: A \subset \mathbb{R}^n \mapsto \mathbb{R}$ una funzione a più variabili. Allora gli insiemi di livello (o ipersuperfici di livello)
    per una funzione reale di più variabili sono gli insiemi
    \[x \in \mathbb{R}^n : f(x) = c\]
    al variare di $c \in \mathbb{R}$.
}
Se $n = 2$ si parla di linee di livello alla superficie di equazione $z = f(x,y)$, 
\[\{(x,y) \in \mathbb{R}^2 : (x,y) \in A\}\]
mentre se $n = 3$ si parla di superfici di livello alla funzione di tre variabili $f(x,y,z)$.
\[\{(x,y,z) \in \mathbb{R}^3 : (x,y,z) \in A\}\]
Le curve di livello rappresentano il luogo dei punti dove $f$ ha valore costante, 
quindi danno informazioni sulla funzione implicitamente. Ad esempio sono linee che vengono tracciate
sulle carte topografiche ad indicare luoghi che hanno la stessa altitudine oppure nelle carte delle previsioni del tempo
ad indicare le isobare, cioè i luoghi che hanno la stessa pressione atmosferica.

\ex{}
{
    Sia $f(x,y) = x^2 + y^2$. Questa superficie prende il nome di \textbf{paraboloide}. È una funzione definita su tutto $\mathbb{R}^2$ ed è sempre
    positiva o nulla. Il paraboloide ha simmetria radiale. 
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                view={10}{10},
                axis equal,
                xlabel={$x$},
                ylabel={$y$},
                zlabel={$z$},
                colormap/cool,
                domain=-2:2,
                y domain=-2:2,
                samples=30,
                samples y=30,
                mesh/ordering=x varies,
                z buffer=sort
              ]
              \addplot3[
                surf,
              ]
              {x^2 + y^2};
            \end{axis}
        \end{tikzpicture}
    \end{figure}
}
\noindent
Se pensiamo di andare a selezionare la precedente superficie con il piano $y = 0$ otteniamo 
una parabola di equazione $z = x^2$.
Naturalmente anche le funzioni a più variabili sono definite all'interno di un dominio naturale che è 
il pià grande sottoinsieme di $\mathbb{R}^n$ dove ha senso scrivere la $f$. 
\ex{}
{
    Trovare il dominio della seguente funzione: 
    \[f(x,y) = \sqrt{\tan{xy}}\sin{e^{\sqrt{xy}}}\]
    Si osserva che la radice quadrata è ben definita se il suo argomento è definito o nullo. 
    L'esponenziale è ben definito ovunque così come la funzione seno; la funzione tangente invece è ben 
    definita se il suo argomento è diverso da $\frac{\pi}{2} + k\pi$ con $k \in \mathbb{Z}$. Dunque le condizioni
    da porre sono:
    \[
    \begin{cases}
        \tan{xy} \ge 0\\
        xy \neq \frac{\pi}{2} + k\pi, k \in \mathbb{Z}\\
        xy \ge 0 
    \end{cases} \Longrightarrow
    \begin{cases}
        k\pi \le xy < \frac{\pi}{2} + k\pi, k \in \mathbb{Z}\\
        xy \neq \frac{\pi}{2} + k\pi, k \in \mathbb{Z}\\
        xy \ge 0
    \end{cases}
    \]
    Quindi la condizione da porre è:
    \[k\pi \le xy < \frac{\pi}{2} + k\pi, k \in \mathbb{N}\]

}

\subsection{Limiti e continuità}

\dfn{}
{
    Data ua successione $\{x_k\}_{k=1}^{\infty}$ di punti in $\mathbb{R}^n$ e un punto $x_0 \in \mathbb{R}^n$ si dice che:
    \begin{center}
        $x_k \rightarrow x_0$ per $k \rightarrow \infty$ se $|x_k - x_0| \rightarrow 0$ per $k \rightarrow \infty$ 
    \end{center}
    Si dice \textbf{intorno sferico di centro} $x_0 \in \mathbb{R}^n$ e raggio $r > 0$ l'insieme:
    \[U_r(x_0) = \{x \in \mathbb{R}^n : |x - x_0| < r\}\]
    $r$ si dice raggio dell'intorno; $x_0$ si dice centro dell'intorno.
}
\dfn{Definizione successionale di limite o limiti per successioni}
{
    Sia $f : \mathbb{R}^n \mapsto \mathbb{R}$ definita almeno in un intorno sferico di $x_0 \in \mathbb{R}^n$
    escluso al più il punto $x_0$ stesso. Sia $L \in \mathbb{R}^*$ dove $\mathbb{R}^* = \mathbb{R} \cup {\pm \infty}$. Allora diremo che:
    \[\lim_{x \rightarrow x_0} f(x) = L\]
    \begin{center}
        $\Updownarrow$\\
        $\forall \{{x_k}\}_{k=1}^{\infty}$ di punti tale che $x_k \rightarrow x_0$ per $k \rightarrow \infty$ (con $x_k \neq x_0$ per ogni $k$) si ha che $\lim_{k \rightarrow \infty}f(x_k) = L$ 
    \end{center}
}
Più in generale si ha la seguente:
\dfn{Definizione topologica di limite}
{
    Sia $f : \mathbb{R}^n \mapsto \mathbb{R}$ definita almeno in un intorno sferico di $x_0 \in \mathbb{R}^n$ (escluso al più $x_0$ stesso e sia $L \in \mathbb{R}$). Si dice che:
    \[\lim_{x \rightarrow x_0} f(x) = L\]
    \begin{center}
        $\Updownarrow$\\
        $\forall \varepsilon > 0$, $\exists \delta > 0$ tale che se $0 < |x - x_0| < \delta$ allora $|f(x) - L| < \varepsilon$. 
    \end{center}
    Si dice che
    \[\lim_{x \rightarrow x_0} f(x) = +\infty (- \infty)\]
    \[\Updownarrow\]
    \[\forall M > 0, \exists \delta > 0 \text{ tale che se } 0 < |x - x_0| < \delta \text{ allora } f(x) > M \; (f(x) < -M)\]
}
\dfn{Continuità}
{
    Si dice che $f : \mathbb{R}^n \mapsto \mathbb{R}$ è continua in $x_0$ se 
    \[\lim_{x \rightarrow x_0} f(x) = f(x_0)\]
}
\noindent
Come conseguenza del teorema sui limiti valgono i teoremi della continuità della somma, del prodotto, del quoziente di funzioni continue (quando hanno senso e il denominatore non si annulla)
e della composizione di funzioni continue.

\subsection{Analisi delle forme di indeterminazione}

Se $\lim_{x \rightarrow x_0} f(x) = L$ allora significa che $f$ si avvicina a $L$ indefinitamente quando la distanza
tra $x$ e $x_0$ si avvicina a zero \textit{indipendentemente} da come si avvicina $x$ a $x_0$. Quindi in generale si distinguono due casi:

\begin{itemize}
    \item Il limite esiste.
    \item Il limite non esiste.
\end{itemize}
\noindent
Se il limite esiste allora l'esistenza del limite va dimostrata; in particolare, può essere utile passare a coordinate polari.
Infatti in questo caso, per esempio $n = 2$ si riesce a mettere in evidenza la dipendenza di $f(x,y)$ dalla distanza tra $(x,y)$ e $(0,0)$ attraverso 
$\rho = \sqrt{x^2 + y^2}$. A questo punto è indispensabile che una volta operata la trasformazione, la $f(\rho, \theta)$ non dipenda più da $\theta$.
Vale il seguente criterio generale per funzione di due variabili:
\dfn{Criterio generale}
{
    Per dimostrare $f(x,y) \rightarrow L$ se $(x,y) \rightarrow (0,0)$ è sufficiente riuscire a scrivere una maggiorazione del tipo:
    \[|f(\rho, \theta) - L| \le g(\rho) \; \; \; g(\rho) \rightarrow 0\]
    L'essenziale è dunque che $g$ non dipenda da $\theta$.
} 
\noindent
Se $(x,y) \rightarrow (x_0, y_0) \neq (0,0)$ il criterio si può ancora applicare con $\rho = \sqrt{(x-x_0)^2  + (y - y_0)^2}$ cioò si pone:
\[x = x_0 + \rho \cos{\theta}\]
\[y = y_0 + \rho \sin{\theta}\]
Naturalmente, non riuscire a dimostrare questa maggioranza \textbf{non significa} che il limite NON esiste.
Il criterio può essere generalizzato al caso $n$ variabili. 
\ex{}
{
    Si calcoli:
    \[\lim_{(x,y) \rightarrow (1,0)} \frac{y^2\log{x}}{(x-1)^2 + y^2}\]
    \begin{itemize}
        \item Soluzione 1: Ricordiamo che
        \[f \rightarrow 0 \Longleftrightarrow |f| \rightarrow 0\]
        A questo punto dunque osservando che $\frac{y^2}{z^2 + y^2} \le 1$ per ogni $(y,z) \in \mathbb{R}^2$:
        \begin{align*}
            0 &\le \lim_{(x,y) \rightarrow (1,0)} \frac{y^2|\log{x}|}{(x-1)^2 + y^2}\\
            &= \lim_{(z,y) \rightarrow (0,0)} \frac{y^2|\log{(z+1)}|}{z^2 + y^2}\\
            &\le \lim_{(z,y) \rightarrow (0,0)} |\log{(z+1)}| = 0
        \end{align*}
        \item Soluzione 2: Si può anche passare a coordinate polari ponendo $x= 1 + \rho \cos{\theta}, y = \rho \sin{\theta}$. Si ottiene 
        \[\lim_{\rho \rightarrow 0} \frac{\rho^2 \sin^2{\theta} \log{1 + \rho \cos{\theta}}}{\rho^2} = \lim_{\rho \rightarrow 0} \rho \sin^2{\theta}\cos{\theta} = 0\]
        Poiché $\sin^2{\theta}\cos{\theta}$ è una quantità limitata in modulo da 1.  
    \end{itemize}
}
I limiti possono essere usati anche nello studio della continuità delle funzioni a più variabili.
\ex{}
{
    Data la funzione:
    \[f(x,y) = \begin{cases}
        \frac{y^2}{x}\; \; \; \; x \neq 0\\
        0 \; \; \; \; x = 0
    \end{cases}\]
    \begin{enumerate}
        \item Si stabilisca se $f$ è continua $(0,0)$.
        \item Si stabilisca se è continua in $D := \{(x,y) \in \mathbb{R}^2 : |y| \le x \le 1\}$
    \end{enumerate}
    $(1)$ Per $x \neq 0$ la funzione è continua. Per concludere occorre analizzare la continuità della funzione
    data lungo la retta $x = 0$. Per fare questo bisogna calcolare il limite per $x \rightarrow 0$ con $y$ qualunque
    di $f(x,y)$ e verificare se esso è uguale a 0 che è il valore assunto lungo l'asse delle $y$. Questo è falso dato che consideriamo la curva
    $y = \sqrt{x}$ allora $f(x,y)$ vale constantemente 1 e dunque non può tendere a $0$ al tendere di $x \rightarrow 0$. Dunque la funzione data non è continua in $(0,0)$. \\
    $(2)$ Usando la condizione imposta dal dominio $D$ si ottiene:
    \[0 \le \lim_{x \rightarrow 0} \frac{y^2}{x} \le \lim_{x \rightarrow 0} x = 0\]
    Non dobbiamo considerare i valori assoluti, perché dato il dominio sappiamo sicuramente che $x \ge 0$. 
    A questo punto il teorema del confronto permette di concludere che il limite considerato esiste e fa $0$, dunque la funzione è continua in $D$.
    Notiamo tramite questo esempio che una funzione può essere discontinua in un punto mentre una sua restrizione ad un sottoinsieme del dominio,
    si può escludere un insieme rilevante di curve lungo le quali calcolare i limiti. 
}
\noindent
Per mostrare invece il limite \textbf{non esiste} per $x \rightarrow x_0$ invece, è sufficiente determinare due curve passanti
per $x_0$ lungo le quali la funzione tende a due limiti diversi. 
\ex{}
{
    Determinare il dominio di definizione della funzione:
    \[f(x,y) = x^{-1}[\sin^2{x} + y^2]\tan{(e^{x+y})}\]
    e studiarne il comportamento per $x \rightarrow 0$ e $y \rightarrow 0$.\\
    Innanzitutto la funzione $\tan{(e^{x+y})}$ è definita per $e^{x+y} \neq \frac{\pi}{2} + k\pi$ con $k \in \mathbb{Z}$. Se $k \in \mathbb{Z}^-$ questo è sicuramente verificato quindi basterà imporre:
    \[e^{x+y} \neq \frac{\pi}{2} + k\pi \; \; \; \; k \in \mathbb{Z}^+ \cup \{0\}\] 
    Inoltre il denominatore non deve essere nullo, quindi:
    \[D := \{(x,y) \in \mathbb{R}^2\; | \; x + y \neq \log{\left(\frac{\pi}{2} + k\pi\right)} \; k \in \mathbb{Z}^+ \cup \{0\} \; \wedge \; x \neq 0\}\]
    Per quanto riguarda il comportamento per $(x,y) \rightarrow (0,0)$, dimostriamo che il limite:
    \[\lim_{(x,y) \rightarrow (0,0)} \frac{(sin^2{x} + y^2)\tan{(e^{x+y})}}{x}\]
    non esiste. A tale scopo basta trovare due curve passanti per l'origine lungo le quali il limite tende a due valori diversi.\\
    Si ha ad esempio, sulla curva $y = \sqrt{x}$:
    \[\lim_{x \rightarrow 0} f(x, \sqrt{x}) = \frac{(\sin^2{x} + x)\tan{(e^{x+\sqrt{x}})}}{x} = \tan{1} \]
    D'altra parte restringiamo sulla curva $y = x$ e calcoliamo il limite:
    \[\lim_{x \rightarrow 0} f(x,x) = \frac{(\sin^2{x} + x^2)\tan{(e^{2x})}}{x} = \sin{x} + x\]
    Questo basta per concludere che il limite non esiste.
}

\section{Calcolo differenziale per funzioni reali a più variabili}

\subsection{Nozioni topologiche di base}

Si ricordi la definizione di intorno sferico già introdotto precedentemente (3.3).
\dfn{}
{
    Sia $E \subseteq \mathbb{R}^n$. Un punto $x_0 \in E$ si dice: 
    \begin{itemize}
        \item \textbf{interno} ad $E$ se $\exists U_r(x_0) \subseteq E$
        \item \textbf{esterno} ad $E$ se $\exists U_r(x_0) \subseteq \mathbb{R}^n \backslash E$
        \item \textbf{di frontiera} per $E$ se $\forall U_r(x_0), U_r(x_0) \cap E \neq \emptyset$ e $U_r(x_0) \cap (\mathbb{R}^n \backslash E) \neq \emptyset$
    \end{itemize} 
}
\dfn{}
{
    Un insieme $E \subseteq \mathbb{R}^n$ si dice \textbf{aperto} se ogni suo punto è interno all'insieme;
    si dice \textbf{chiuso} se il complementare è aperto. 
}
\noindent
L'unione di una famiglia qualsiasi (anche infinita) di aperti e l'intersezione di una famiglia finita di aperti sono aperti.
L'unione finita di chiusi e l'intersezione qualsiasi (anche infinita) di chiusi è ancora un chiuso.
Non è difficile rendersi conto che l'unione qualsiasi di due chiusi può non essere un chiuso e l'unione qualsiasi di due aperti può non essere un aperto.
Sia $f : \mathbb{R}^n \mapsto \mathbb{R}$ una funzione definita e continua in tutto $\mathbb{R}^n$. Allora gli insiemi:
\[\{x \in \mathbb{R}^n : f(x) > 0\}\]
\[\{x \in \mathbb{R}^n : f(x) < 0\}\]
\[\{x \in \mathbb{R}^n : f(x) \neq 0\}\]
sono insieme aperti; invece:
\[\{x \in \mathbb{R}^n : f(x) \ge 0\}\]
\[\{x \in \mathbb{R}^n : f(x) \le 0\}\]
\[\{x \in \mathbb{R}^n : f(x) = 0\}\]
sono chiusi.
\ex{}
{
    Gli insiemi di livello di una funzione $f(x,y)$ sono insiemi del tipo $f(x,y) = C$ e quindi sono chiusi se $f$ è continua!
}

\subsection{Derivate parziali, derivabilità e piano tangente}

Come abbiamo imparato da Analisi 1, la derivata di una funzione in un punto ha un interessante significato geometrico: essa rappresenta la pendenza della retta tangente al grafico della funzione in quel punto.
inoltre la funzione è derivabile se e soltanto se è ben definita la retta tangente in quel punto.
Cercheremo ora di estendere questo concetto a funzioni a più variabili.
Innanzitutto la definizione di derivata per una funzione a più variabili vista come limite del rapporto incrementale non è più applicabile, 
intanto non è ovvio che cosa sia un incremento in più dimensioni e secondariamente anche se si arrivasse ad una definzione accettabile, non si sa cosa voglia dire dividere per un vettore (la controparte di $h$ nella definizone scalare di derivata).
La presenza di più variabili richiede di considerare le derivate parziali, che sono le derivate rispetto ad una variabile tenendo fisse le altre.
\dfn{}
{
    Sia $f : A \subseteq \mathbb{R}^2 \mapsto \mathbb{R}^n$ una funzione di due variabili. La quantità (se esiste):
    \[\frac{\partial f}{\partial x}(x_0, y_0) \lim_{h \rightarrow 0} \frac{f(x_0 + h, y_0) + f(x_0, y_0)}{h}\]
    si chiama derivata parziale rispetto ad $x$. calcolata nel punto $(x_0, y_0)$. Analogamente sarà lo stesso per $y$.  
}
\dfn{}
{
    Il vettore che ha come componenti le derivate parziali di $f$ rispetto a $x$ e $y$ si chiama \textbf{gradiente} di $f$ e si indica con:
    \[\nabla f(x,y) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)\]
}
\dfn{}
{
    Una funzione $f : A \subseteq \mathbb{R}^2 \mapsto \mathbb{R}$ si dice \textbf{derivabile} in un punto $(x_0, y_0) \in A$ se esiste in quel punto esistono tutte le derivate parziali; $f$ si dice derivaibile in tutto $A$ 
    se è derivabile in ogni punto di $A$.
}
\noindent
In alcuni casi, quando si applicano le derivate parziali ci si può ricondurre alle regole di derivazione che si possono usare per funzioni a singola variabili trattendo le altre variabili come costanti.
Ma in alcuni casi non è possibile e bisogna ricondurci alla definizione di rapporto incrementale.
\ex{}
{
    Sia $f(x,y) = y\sqrt{x}$. Calcolare se esiste la derivata parziale rispetto a $x$ in $(0,0)$.
    Si avrebbe:
    \[\frac{\partial f}{\partial x} (x,y) = \frac{y}{2\sqrt{x}} = \frac{0}{0}\]
    ci troviamo davanti ad una forma di indecisione che potrebbe farci pensare che la derivata non esista. L'unica via percorribile è quella di calcolare il limite:
    \[\frac{\partial f}{\partial x}(0,0) = \lim_{h \rightarrow 0} \frac{f(h,0) - f(0,0)}{h} = \lim_{h \rightarrow 0} \frac{0 - 0}{h} = 0\]
    Osserviamo come il limite esista e sia uguale a $0$. Contrariamente da quello che si possa pensare $0/h$ non è una forma di indecisione perché al numeratore si ha uno zero "secco" mentre al denominatore troviamo $h$
    che è una quantità che almeno prima del processo del limite è \textit{diverso da zero}. Detto ciò la quantità $\frac{0}{h}$ è sempre 0 e quindi dopo il passaggio al limite non si ha nessun problema.
}
\ex{}
{
    Sia $f$: 
    \[f(x,y) = \begin{cases}
        x^2 + y^2 \; \; \; \; (x,y) \neq (0,0)\\
        1 \; \; \; \; \; \; \;  \; \; \; \; \; \;  \; (x,y) = (0,0)
    \end{cases}\]
    Calcolare (se esiste) $f_x(0,0)$.\\
    Un errore grave e molto comune è dire: $f$ in $(0,0)$ vale $1$ e quindi la derivata parziale rispetto a $x$ vale $0$.
    Questo ragionamento è errato perché non si può calcolare la derivata parziale in un punto se non si conosce il comportamento della funzione in un intorno di quel punto.
    Dunque calcoliamo il limite:
    \[\frac{\partial f}{\partial x}(0,0) = \lim_{h \rightarrow 0} \frac{f(h,0) - f(0,0)}{h} = \lim_{h \rightarrow 0} \frac{h^2 + 0^2 - 1}{h} = \lim_{h \rightarrow 0} \frac{h^2 - 1}{h}\]
    E quest'ultimo limite non esiste perché se si fa il passaggio al limite si ottiene $-\infty$ se $h \rightarrow 0^+$ e $+\infty$ se $h \rightarrow 0^-$. Dunque 
    la derivata parziale di $f$ in $(0,0)$ non esiste e dunque $f$ non è derivabile nella direzione dell'asse delle $x$.
}
\noindent
Intuitivamente si poteva ragionare anche in questo modo: la funzione dell'esempio precedente non è continua quindi ci si aspetta che non sia derivabile.
Tuttavia non è sempre così, ci sono funzioni che sono continue ma non derivabili in un punto. Quindi il comportamento di funzioni a due variabili assume situazioni che non hanno una controparte in una dimensione.
\ex{}
{
    Sia $f$ la funzione di due variabili definita da: 
    \[f(x,y) = \begin{cases}
        \frac{xy}{x^2 + y^2} \; \; \; \; (x,y) \neq (0,0)\\
        0 \; \; \; \; \; \; \;  \; \; \; \; \; (x,y) = (0,0)
    \end{cases}\]
    Calcolare le derivate parziali in $(0,0)$. Per lo stesso motivo dell'esempio precedente, utilizziamo la definizione di derivata parziale in un altro punto. Si ha:
    \[\frac{\partial f}{\partial x} (0,0) = \lim_{h \rightarrow 0} \frac{f(0 + h, 0) + f(0,0)}{h} = 0\]
    \[\frac{\partial f}{\partial y} (0,0) = \lim_{h \rightarrow 0} \frac{f(0, 0 + h) + f(0,0)}{h} = 0\]
    Quindi le derivate parziali in $(0,0)$ esistono e sono uguali a $0$.\\
    D'altra parte è facile notare che la funzione non è continua in $(0,0)$ perché basta prendere la retta $y = x$ lungo la quale $f$ vale $1/2$ e quindi abbiamo trovato un esempio di funzione
    che \textit{è derivabile ma non continua} in un punto.
}

\subsection{Piano tangente}

In una dimensione sappiamo che la derivabilità è equivalente all'esistenza della retta tangente a una superficie. 
Ci poniamo il problema di estendere questo concetto a più variabili, quindi di vedere se esiste il piano tangente. È intuitivo pensare che se tale piano esiste,
deve essere "tangente" alla funzione in quel punto e quindi "approssimare" almeno localmente la funzione a cui è tangente (per esempio $y = \sin{x}$ per $x$ vicino a 0 è approssimata dalla sua retta tangente $y=x$ e infatti si ha che $\sin{x} \sim x$ se $x \rightarrow 0$).
Facciamo il seguente ragionamento: supponiamo di avere la nostra superficie $z = f(x,y)$ e consideriamo il piano verticale $y = y_0$; sezioniamo la superficie con il piano: troveremo la curva data da $z = f(x,y_0)$ (con $y = y_0$ perché una curva in $\mathbb{R}^3$ è sempre individuata da due equazioni). 
È facile immaginare che la retta tangente a tale curva, chiamiamola $r_1$, apparterrà al piano tangente che stiamo considerando. Analogamente,
se sezioniamo la superficie con il piano verticale $x = x_0$ troveremo una curva individuata dalle equazioni $x = x_0$ e $z = f(x_0,y)$; la retta tangente a questa curva, chiamiamola $r_2$, apparterrà al piano tangente che stiamo considerando.
Quindi quali sono le equazioni di $r_1$ e $r_2$? Tenendo conto che le curve sono funzioni di una sola variabile e che la derivata (parziale) nel punto coincide 
con il coefficiente angolare della retta tangente si avrà:
\[
r_1 \begin{cases}
    z = f(x_0,y_0) + \frac{\partial f}{\partial x}(x_0,y_0)(x - x_0)\\
    y = y_0
\end{cases}
\]
\[
r_2 \begin{cases}
    z = f(x_0,y_0) + \frac{\partial f}{\partial y}(x_0,y_0)(y - y_0)\\
    x = x_0
    \end{cases}
\]
quindi il piano che coincide entrambe le rette sarà:
\begin{equation}
    z = f(x_0,y_0) + \frac{\partial f}{\partial x}(x_0,y_0)(x - x_0) + \frac{\partial f}{\partial y}(x_0,y_0)(y - y_0)
\end{equation}
La domanda che ci facciamo è la seguente: siamo sicuri che questo piano così come l'abbiamo costruito partendo da un ragionamento intuitivo 
sia effettivamente tangente alla nostra superficie nel senso di approssimarla localmente? La risposta purtroppo è negativa come mostra il prossimo esempio:
\ex{}
{
    Sia $f$ la funzione di due variabili definita da: 
    \[f(x,y) = \begin{cases}
        \frac{xy}{x^2 + y^2} \; \; \; \; (x,y) \neq (0,0)\\
        0 \; \; \; \; \; \; \;  \; \; \; \; \; (x,y) = (0,0)
    \end{cases}\]
    Abbiamo visto prima come calcolare le derivate parziali in $(0,0)$ e che esse sono uguali a $0$, dunque il presunto 
    piano tangente coincide con il piano $z = 0$ cioè il piano orizzontale $xy$. Ma come abbiamo già rimarcato la funzione
    non è continua nell'origine, addirittura lungo la bisettrice $y = x$ la funzione vale $1/2$ e quindi non è possibile
    che il piano $z = 0$ non sia tangente alla funzione data nel senso che non approssima la funzione localmente. 
    Quindi la derivabilità non è sufficiente per garantire l'esistenza del piano tangente nè la continuità.
}

\subsection{Differenziabilità}

Se una funzione $f$ è differenziabile accadrà che \textit{l'incremento di $f$ sarà uguale all'incremento calcolato 
lungo il piano tangente più un infinitesimo di ordine superiore rispetto alla lunghezza dell'incremento (h,k)} della variabili indipendentemente.
In simboli:
\begin{equation}
    \underbrace{f(x_0 + h, y_0 + k) - f(x_0,y_0)}_{\text{incremento di $f$}} = \underbrace{\frac{\partial f}{\partial x} (x_0,y_0) \overbrace{h}^{(x-x_0)} + \frac{\partial f}{\partial x}(x_0, y_0) \overbrace{k}^{(y-y_0)}}_{\text{incremento calcolato lungo il piano tangente}} + \underbrace{o(\sqrt{h^2 + k^2})}_{\text{infinitesimo di ordine superiore}}
\end{equation}
\noindent
dove per definizione $o(\sqrt{h^2 + k^2})$ è tale che:
\[\lim_{(h,k) \rightarrow (0,0)} \frac{o(\sqrt{h^2 + k^2})}{\sqrt{h^2 + k^2}} = 0\]

\dfn{}
{
    Se vale (4.4.1) allora diremo che $f$ è \textbf{differenziabile} in $(x_0,y_0)$.
}
\noindent
Se $f$ è \textbf{differenziabile} in $(x_0,y_0)$ allora è $f$ è derivabile in $(x_0,y_0)$ e continua in $(x_0,y_0)$.
Ricordiamo ancora che se $f$ è continua non implica che $f$ differenziabile. 
\dfn{}
{
    Se $f$ è differenziabile in $(x_0,y_0)$, si dice \textbf{differenziale} di $f$ in $(x_0,y_0)$ l'applicazione lineare di $df(x_0,y_0) : \mathbb{R}^2 \mapsto \mathbb{R}$ definita da $df(x_0,y_0) : (h,k) \mapsto \nabla f(x_0,y_0) \cdot (h,k)$
}
La quantità $\nabla f(x_0,y_0) \cdot (h,k)$ rappresenta l'incremento della funzione nel passare da $(x_0,y_0)$ a $(x_0 + h, y_0 + k)$ lungo il piano tangente di $f$ in $(x_0,y_0)$.
\thm{}
{
    Siano $f : A \subseteq \mathbb{R}^2 \mapsto \mathbb{R}$ con $A$ aperto e $(x_0,y_0) \in A$. Supponiamo che le derivate parziali esistano in un intorno di $(x_0,y_0)$ e siano continue in $(x_0,y_0)$. Allora $f$ è differenziabile in $(x_0,y_0)$ e in particolare,
    se le derivate parziali di $f$ esistono e sono continue in $A$ allora $f$ è differenziabile in ogni punto di $A$. Quindi vale l'implicazione:
    \[f \in \mathcal{C}^1 \Longrightarrow f \text{ differenziabile in } A\]
}

\end{document}