\documentclass[a4paper]{article}
\usepackage{import}
\input{../../setup.sty}

\onehalfspacing
\title{Algoritmi}
\author{Università di Verona\\Imbriani Paolo -VR500437\\Professor Roberto Segala}

\begin{document}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{UniversityofVerona.png}
    \label{fig:centered-image}
\end{figure}

\maketitle

\pagebreak

\tableofcontents

\pagebreak

\section{Introduzione agli algoritmi}


Ci sono diverse definizioni di 'algoritmo' ma quella più semplice per definirlo è una sequenza di istruzioni volta a risolvere un problema. In maniera più semplice, possiamo definirlo come una ricetta. Le ricette sono delle istruzioni per creare dei piatti: queste istruzioni sono precise tuttavia possono avere anche delle varianti. Il problema non è trovare la ricetta, ma capire quale sia quella gisuta da utilizzare in base al problema posto.

\vspace{1em}
\noindent
In questo corso impararemo come \textit{decidere}. Studiare i diversi approcci e il metodo migliore per affrontare un problema.
Capiremo come confrontare gli algoritmi fra di loro.

\subsection{Complessità}

Classifichiamo gli algoritmi in base alla loro complessità ovvero quanto tempo ci mettono per essere completati. Il nostro obiettivo è quello non di misurare il tempo in sè (poiché può dipendere dal tipo di macchina che utilizziamo) 
che impiega un programma a finire ma piuttosto capire il numero di istruzioni elementari che vengono impiegate per risolvere il problema.

\vspace{1em}
\noindent
La complessità non è un numero bensì una funzione, poiché mappa la dimensione del problema in numero di istruzioni da eseguire. Per esempio, quando si parla di dimensione di una matrice ci si viene più comodo vedere il numero di colonne e righe piuttosto che contare il numero di elementi all'interno della matrice. La dimensione del problema è un insieme di oggetti, tipicamente un insieme di numeri che ci permette di avere un idea chiara di capire quanto sia grande il problema. Se riusciamo a misurarlo bene, potremmo facilitarci la vita nel risolvere il problema.

\subsection{Complessità dei costrutti e ordini di grandezza}

È doveroso stare attenti a cosa moltiplichiamo: se moltiplichiamo due vettori, il numero di operazioni da eseguire è esattamente $n$. Mentre se fossero orgnanizzate in matrici quadrate con lunghezza del lato $\sqrt{n}$ la sua complessità andrebbe ad aumentare a $O(n\sqrt{n})$.
Come si rappresentano le istruzioni di un programma?
Se sono in serie:
\[\begin{matrix}
  I_1 & c_1(n)\\
  I_2 & c_2(n)\\
  \vdots\\
  I_l & c_l(n)
\end{matrix}\]
Dove la complessità totale allora sarà:
\[\sum_{i=1}^l c_i(n)\]
oppure all'interno di un costrutto \textbf{if-else:}
\[\begin{matrix}
  \text{if cond} & c_{\text{cond}}(n)\\
  I_2 & c_1(n)\\
  \text{else}\\
  I_l & c_2(n)
\end{matrix}\]
In questo caso, come faccio a sapere la complessità totale di questi istruzioni? Per capirlo, studiamo il caso nel \textit{worst case scenario} ovvero nel caso peggiore. A volte vengono mostrati anche i casi migliore ma di solito si calcola il tempo peggiore. Quindi ci interessa specialmente il \textit{limite superiore} dell'algoritmo, quindi piuttosto che dire che la complessità è esattamente \textbf{uguale} questo numero, invece noi diremo che è \textbf{minore o uguale} del numero calcolato.

\[C(n) = c_{\text{cond}}(n) + max(c_1(n), c_2(n))\]
Mentre per un while loop?
\[\begin{matrix}
    \text{while cond} & c_{cond}(n)\\
    I & c_0(n)
\end{matrix}\]
Sia $m$ limite superiore numero di iterazioni che esegue l'algoritmo. La complessità di questo algoritmo quindi sarà:
\[C(n) = c_{cond}(n) + m(c_{cond}(n) + c_0(n))\]
Proviamo ora a calcolare la molteplicazioni tra matrici.
Siano due matrici $A$ e $B$ rispettivamente con dimensione $n$ x $m$ e $m$ x $l$.
\begin{lstlisting}
For i <- 1 to n
    For j <- 1 to l
        C[i][j] <- 0
        For k <- 1 to m
            C[i][j] += A[i][j] * B[k][j]
\end{lstlisting}
Avere un risultato del tipo $5m + 1$ (riguardo il for più interno) è inutile perché non ci da informazioni realmente utili sulla complessità dell'algoritmo.
Ad ogni modo contando tutti i for, avremo un risultato del tipo:

\[n(5ml + 4l + 2) + n + 1 = 5nml + 4nl + 3n + 1\]
Ci sono alcuni elementi in questa operazioni che non influiscono realmente sul risultato finale. Indi per cui, possiamo anche ometterlo all'interno del calcolo della complessità di un algoritmo.
In realtà ciò che ci interessa realmente nel risultato finale è:
\[5\colorbox{yellow}{\textbf{nml}}+ 4nl + 3n + 1\]
Infatti $nml$ è capace di dirci tutto sulla complessità dell'algoritmo e da cosa dipende.
Quando si parla di ordine di grandezza si parla in realtà del comportamento asintotico della funzione calcolata.
\begin{definition}
\[f \in O(g)  \Longleftrightarrow \exists c > 0, \,\, \exists \overline{n}, \,\, \forall n \ge \overline{n} \, | \, f(n) \le cg(n)\]
\end{definition}
\begin{figure}[H]
    \centering
    \caption{$f \in O(g)$}
\begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel = $x$,
        ylabel = {$\textcolor{blue}{f(n)}, \textcolor{red}{cg(n)}$},
        legend pos=north west,
        domain=2:8,
        samples=200,
        xmin=2, xmax=8, ymin=-5, ymax=30,
        width=12cm,
        height=8cm
    ]
        % Plot f(x) = exp(x) / 5
        \addplot[color=red, thick] {(exp(x))/5};

        % Plot g(x) = (30*ln(x)) / (sin(x) + 3)
        \addplot[color=blue, thick] {(30*ln(x))/(sin(deg(x)) + 3)};

        % Add a vertical line at the intersection point
        \addplot[mark=none, black, dashed, thick] coordinates {(4.8, 0) (4.8, 24)};
        \node at (axis cs:4.2,25) [anchor=west] {$\overline{n}$};

    \end{axis}
\end{tikzpicture}
\end{figure}

\begin{definition}
  \[f \in \Omega(g) \Longleftrightarrow \exists c > 0, \,\, \exists \overline{n}, \,\, \forall n \ge \overline{n} \, | \, f(n) \ge cg(n)\]
  Che rispettivamente è l'inverso della funzione O grande.
\end{definition}

\begin{figure}[H]
    \centering
    \caption{$f \in \Omega(g)$}
\begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel = $x$,
        ylabel = {$\textcolor{blue}{f(n)}, \textcolor{red}{cg(n)}$},
        legend pos=north west,
        domain=2:8,
        samples=200,
        xmin=2, xmax=8, ymin=-5, ymax=30,
        width=12cm,
        height=8cm
    ]
        % Plot f(x) = exp(x) / 5
        \addplot[color=blue, thick] {(exp(x))/5};

        % Plot g(x) = (30*ln(x)) / (sin(x) + 3)
        \addplot[color=red, thick] {(30*ln(x))/(sin(deg(x)) + 3)};

        % Add a vertical line at the intersection point
        \addplot[mark=none, black, dashed, thick] coordinates {(4.8, 0) (4.8, 24)};
        \node at (axis cs:4.2,25) [anchor=west] {$\overline{n}$};

    \end{axis}
\end{tikzpicture}
\end{figure}
\begin{definition}
  \[f \in \Theta(g) \Longleftrightarrow f \in O(g)  \, \wedge \, f \in \Omega(g)\]
\end{definition}

\begin{figure}[H]
    \centering
    \caption{$f \in \Theta(g)$}
\begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel = $x$,
        ylabel = {$\textcolor{red}{c_1g(n)}, \textcolor{blue}{c_2g(n)}, \textcolor{green!75!black}{f(n)}$},
        legend pos=north west,
        domain=2:8,
        samples=200,
        xmin=3, xmax=8, ymin=10, ymax=40,
        width=12cm,
        height=8cm
    ]
        \addplot[color=blue, thick] {(exp(x))/5};
        \addplot[color=red, thick] {(30*ln(x))/(sin(deg(x)) + 3)};
        \addplot[color=green!75!black, thick] {30*ln(x) - 23.5};


        % Add a vertical line at the intersection point
        \addplot[mark=none, black, dashed, thick] coordinates {(4.78, 0) (4.78, 23.65)};
        \node at (axis cs:4.6,25) [anchor=west] {$\overline{n}$};

    \end{axis}
\end{tikzpicture}
\end{figure}



\ex{}{
Questa affermazione vale?
\[n \in O(2n)\]
Sì, perché esiste un $c$ che soddisfa $n \le c2n$.\\\\
Quest'altra affermazione vale?
\[2n \in O(n)\]
Sì perché esiste un $c$ che soddisfa $2n \le 2n$.
}
\ex{}
{
Questa affermazione vale?
\[f \in O(g) \Longleftrightarrow g \in \Omega(g)\]
Proviamo a dimostrare. Sappiamo che esiste un $c$ e $\overline{n}$ tale che \(\forall n > \overline{n} \; f(n) \le cg(n)\). Isoliamo la g:
\[g(n) \ge \frac{1}{c} f(n)\]
Esiste quindi un c' che soddisfa la disuguaglianza.
\[c' = \frac{1}{c}\]
}
\begin{examplebox}{Esempio 3}

\[f_1 \in O(g),
f_2 \in O(g) \Longrightarrow f_1 + f_2 \in O(g)
\]
Dobbiamo dimostrare che esiste $c_1$ e $c_2$, $\overline{n}_1$ e $\overline{n}_2$ tali che:
\[\forall n > \overline{n}_1 \; | \; f_1(n) \le c_1g(n)\]
\[\forall n > \overline{n}_2 \; | \; f_2(n) \le c_2g(n)\]
Quindi
\[\overline{n} = max(\overline{n}_1, \overline{n}_2)\]
\[f_1(n) + f_2(n) \le (c_1 + c_2)g(n)\]

\end{examplebox}

\begin{examplebox}{Esempio 4}
    \[f_1 \in O(g_1), f_2 \in O(g_2) \; | \; f_1f_2 \in O(g_1g_2)\]
Quindi esiste $c_1$ e $c_2$, $\overline{n}_1$ e $\overline{n}_2$ tali che:
\[\forall n > \overline{n}_1 \; | \; f_1(n) \le c_1g(n)\]
\[\forall n > \overline{n}_2 \; | \; f_2(n) \le c_2g(n)\]
\[f_1(n)f_2(n) \le c_1g_1(n)c_2g_2(n)\]
\[f_1(n)f_2(n) \le c_1c_2g_1(n)g_2(n)\]
Quindi:
 \[c = c_1c_2\] \[\overline{n} = max(\overline{n}_1, \overline{n}_2)\]
\end{examplebox}

\subsection{Ordini di grandezza per le funzioni}

L'algoritmo di ricerca $A$ termina entro n. Immaginiamo che l'algoritmo A sia il seguente:
\begin{lstlisting}
For i <- 0 to length(a) - 1
    if a[i] = x
        ret i
ret -1
\end{lstlisting}
Capiamo che la sua complessità è uguale a:
\[A \in O(n)\]
Per appurarci che la stima di complessità sia accurata dobbiamo controllare che $A \in \Omega(g)$ vuol dire che esiste uno schema di input tale per cui se $g(n)$ è il numero di passi necessari per risolvere l'istanza $n$, allora $g \in \Omega(f)$. Se riusciamo a fare questo allora $g \in \Theta(f)$ e quindi possiamo dire che la stima è accurata.
\\\\
$P \in O(f)$ vuol dire che il problema $P$ riesco a risolvere in tempo $f$. Supponiamo per assurdo che esista un algoritmo riesca a capire se l'elemento si trova nell'array o no. Possiamo capire velocemente che per contraddizione, essendo che esiste almeno un elemento dove l'algoritmo non ha guardato, siamo certi del malfunzionamento dell'algoritmo e quindi non è vero che esiste una stima di complessità più bassa di $f$.


Ora andiamo ad affrontare i tipi di algoritmi che si definiscono di "ordinamento".
\begin{definition}
\textbf{Input}: Sequenza $(a_1, ..., a_n)$ di oggetti su cui è definita una relazione di ordinamento (in questo caso ordinamento per confronti).
\\\\
\textbf{Output}: Permutazione $(a_1', ..., a_n')$ di $(a_1, ..., a_n)$ t.c. $\forall \, i > j, \; \; a_i' \le a_j'$.
\end{definition}
Andiamo ora a vedere i diversi tipi di algoritmi di ordinamento.

\subsection{insertion$\_$sort (A)}

In questo caso la $j$ sarà chiamata variabile "invariante" ovvero che mantiene una proprietà che continua a valere nel run-time dell'algoritmo. In questo caso, la $j$ è invariante perchè tutti gli oggetti "a sinistra" di essa saranno considerati già ordinati. (Parte da 2 perché abbiamo deciso che la posizione all'interno dell'array parte da 1).

\begin{lstlisting}
for j <- 2 to length[A]
    key <- A[j]
    i <- j - 1
    while i > 0 and A[j] > key
        A[j+1] <- A[j]
        j--
    a[j+1] <- key
\end{lstlisting}
Ora dobbiamo capire quale sia la complessità di questo algoritmo. Se le cose vanno bene, l'algoritmo potrebbe terminare in $O(n)$. Tuttavia, seppur giusto, non è preciso. Infatti, nel peggiore dei casi, l'algoritmo termina in $O(n^2)$. Infatti il caso peggiore di input che mi può arrivare è un array ordinato al contrario con complessità uguale a:
\[1 + 2 + 3 + ... + n = \frac{n(n+1)}{2} \in \Theta(n^2)\]
\textit{Questo algoritmo, quanto spazio di memoria usa in più rispetto allo spazio occupato dai dati? }
\\\\
Lo spazio di memoria di questo algoritmo rimane costante a prescindere dalla dimensione del problema. Un algoritmo del genere si dice che \textit{\textbf{ordina in loco}} se la quantità di memoria extra è costante. Si parla di \textit{\textbf{stabilità}} di un algoritmo di ordinamento quando l'ordine relativo di elementi uguali non viene scambiato dall'algoritmo. Quindi:
\[\text{Se } a_i = a_j \, i < j, \text{mantiene l'ordinamento}\]


\section{Concetto di "Divide et impera"}

\subsection{Fattoriale e funzioni ricorsive}

\begin{lstlisting}
Fatt(n)
    if n = 0
        ret 1
    else
        ret n * fatt(n - 1)
\end{lstlisting}

L'argomento della funzione ci fa capire la complessità dell'algoritmo:
\[
  T(n) = \begin{cases}
    1 & \text{se } n = 0 \\
    T(n - 1) + 1 & \text{se } n > 0
  \end{cases}
\]
Con problemi ricorsivi si avrà una complessità con funzioni definite ricorsivamente.
Questo si risolve induttivamente:
\[
  \begin{aligned}
    T(n) & = 1 + T(n-1)\\
         & = 1 + 1 + T(n-2)\\
         & = 1 + 1 + 1 + T(n-3)\\
         & = \underbrace{1 + 1 + \ldots + 1}_{i} + T(n-i)\\
  \end{aligned}
\]
La condizione di uscita è: \( n-i = 0 \quad n = i \)
\[
\begin{aligned}
         & = \underbrace{1 + 1 + \ldots + 1}_{n} + T(n-n)\\
         & = n + 1 = \Theta(n)
\end{aligned}
\]
Questo si chiama passaggio iterativo.

\begin{examplebox}{Esempio 1}
  \[
    T(n) = 2T\left(\floor*{\frac{n}{2}}\right) + n
  \]
  Questa funzione si può riscrivere come:
  \[
  T(n) = \begin{cases}
    \text{Costante} & \text{se } n < a \\
    2T\left(\floor*{\frac{n}{2}}\right) + n & \text{se } n \ge a
  \end{cases}
  \]

  \vspace{1em}
  \noindent
  Se la complessità fosse già data bisognerebbe soltanto verificare se è corretta.
  Usando il metodo di sostituzione:
  \[
    T(n) = cn \log n
  \]
  sostituiamo nella funzione di partenza:
  \[
    \begin{aligned}
      T(n)  & = 2T\left(\floor*{\frac{n}{2}}\right) + n\\
            & \le 2c\left(\floor*{\frac{n}{2}}\right) \log \floor*{\frac{n}{2}} + n\\
            & \le \cancel{2} c \frac{n}{\cancel{2}} \log \frac{n}{2} + n\\
            & = cn \log n - cn \log 2 + n\\
            & \stackrel{?}{\le} cn \log n \quad \text{se } n- cn \log 2 \le 0\\
    \end{aligned}
  \]
  \[
    c \ge \frac{n}{n \log 2} = \frac{1}{\log 2}
  \]
  Il metodo di sostituzione dice che quando si arriva ad avere una disequazione
  corrispondente all'ipotesi, allora la soluzione è corretta se soddisfa una certa ipotesi.
\end{examplebox}

\begin{examplebox}{Esempio 2}
  \[
    T(n) = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1 \quad \in O(n)
  \]
  \[
  T(n) \le cn
  \]
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) + c\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) + 1\\
         & = cn + 1 \stackrel{?}{\le} cn
  \end{aligned}
  \]
  Il metodo utilizzato non funziona perchè rimane l'1 e non si può togliere in alcun modo.
  Per risolvere questo problema bisogna risolverne uno più forte:
  \[
  T(n) \le cn - b
  \]
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) -b + c\left(\ceil*{\frac{n}{2}}\right) -b + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) - 2b + 1\\
         & = cn - 2b + 1 \stackrel{?}{\le} cn - b\\
         & = \underbrace{cn - b} + \underbrace{1 - b}_{\le 0} \le cn - b \quad \text{se } b \ge 1\\
  \end{aligned}
  \]
  Se la proprietà vale per questo problema allora vale anche per il problema iniziale
  perchè è meno forte.
\end{examplebox}

\begin{examplebox}{Esempio 3}
  \[
    \begin{aligned}
      T(n) & = 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right) + n\\
           & = n + 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right)\\
           & = n + 3 \left( \left\lfloor \frac{n}{4} \right\rfloor + 3T
           \left( \left\lfloor \frac{\left\lfloor \frac{n}{4} \right\rfloor}{4} \right\rfloor
           \right)  \right)\\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2 T
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor \right)\\
           & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor + 3T \left(
           \left\lfloor \frac{\left\lfloor \frac{n}{4^2} \right\rfloor}{4} \right\rfloor
           \right)  \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2
           \left\lfloor \frac{n}{4^2} \right\rfloor + 3^3 T
           \left( \left\lfloor \frac{n}{4^3} \right\rfloor \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{i-1}
           \left\lfloor \frac{n}{4^{i-1}} \right\rfloor + 3^i T
           \left( \left\lfloor \frac{n}{4^i} \right\rfloor \right)
    \end{aligned}
  \]


  Per trovare il caso base poniamo l'argomento di T molto piccolo:
  \[
    \begin{aligned}
      \frac{n}{4^i} & < 1\\
      4^i & > n\\
      i & > \log_4 n
    \end{aligned}
  \]
  L'equazione diventa:
  \[
    \begin{aligned}
      & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{\log_4 n - 1}
      \left\lfloor \frac{n}{4^{\log_4 n - 1}} \right\rfloor + 3^{\log_4 n} c\\
    \end{aligned}
  \]


  Si può togliere l'approssimazione per difetto per ottenere un maggiorante:
  \[
  \begin{aligned}
    & \le n \left( 1 + \frac{3}{4} + \left( \frac{3}{4} \right)^2 + \ldots +
    \left( \frac{3}{4} \right)^{\log_4 n-1} \right) + 3^{\log_4 n} c\\
    & \le n \left( \sum_{i=0}^{\infty} \left( \frac{3}{4} \right)^i \right) + c 3^{\log_4 n}\\
  \end{aligned}
  \]
  Per capire l'ordine di grandezza di \( 3^{\log_4 n} \) si può scrivere come:
  \[
    3^{\log_4 n} = n^{\left( \log_n 3^{\log_4 n} \right) } = n^{\log_4 n \cdot \log_n 3}
    = n^{\log_4 3}
  \]
  Quindi la complessità è:
  \[
  \begin{aligned}
    & = O(n) + O(n^{\log_4 3})\\
  \end{aligned}
  \]
  Si ha che una funzione è uguale al termine noto della funzione originale e l'altra
  che è uguale al logaritmo dei termini noti. Se usassimo delle variabili uscirebbe:
  \[
    \begin{aligned}
      T(n) & = a T \left( \left\lfloor \frac{n}{b} \right\rfloor \right) + f(n)\\
           & = O(f(n)) + O(n^{\log_b a})
    \end{aligned}
  \]
\end{examplebox}



\subsection{Master Theorem o Teorema dell'esperto}

Data una relazione di occorrenza di questa forma:

\[T(n) = a T \left( \left\lfloor \frac{n}{b} \right\rfloor \right) + f(n)\]

Distinguiamo tre casi:

\begin{enumerate}
    \item \[f(n) \in O(n^{\log_ba - \epsilon}) \Longrightarrow T(n) \in \Theta(n^{\log_ba})\]
    \item \[f(n) \in \Theta(n^{\log_ba}) \Longrightarrow T(n) \in \Theta(f(n)\log n)\]
    \item \[f(n) \in \Omega(n^{\log_ba + \epsilon} ) \Longrightarrow T(n) \in \Theta(f(n))\]
\end{enumerate}

\begin{examplebox}{Esempio 1}
    \[T(n) = 9T\left(\frac{n}{3}\right) + n\]
    \[a = 3, b = 3, f(n) = n\]
    Basta che trovo un $\epsilon$ che mi dia $n$.
    \[n^{\log_b a} = n^{\log_3 9} = n^2 * n^{-\frac{1}{2}}\]
    In questo caso $\epsilon = n^{-\frac{1}{2}}$ e ci troviamo nel \textbf{PRIMO CASO} e la soluzione è $T(n) \in \Theta(n^2)$.
\end{examplebox}

\begin{examplebox}{Esempio 2}
       \[T(n) = T\left(\frac{2n}{3}\right) + 1\]
    \[a = 1, b = \frac{3}{2}, f(n) = n^0\]
    \[n^{\log_b a} = n^{\log_{\frac{3}{2}} 1} = n^0\]
    Ci troviamo nel \textbf{SECONDO CASO} e la soluzione è $T(n) \in \Theta(\log n))$
\end{examplebox}

\begin{examplebox}{Esempio 3}
    \[T(n) = 3T\left(\frac{n}{4}\right) + n \log n\]
       \[a = 3, b = 4, f(n) = n \log n\]
       Ci troviamo nel \textbf{TERZO CASO} quindi basta qualsiasi valore di $\epsilon$ basta che sia contenuto tra $\log_3 4 \le \epsilon \le 1$. La soluzione è $T(n) \in \Theta(n\log n)$.
\end{examplebox}

\begin{examplebox}{Esempio 4}
    \[T(n) = 2T\left(\frac{n}{2}\right) + n \log n\]
       \[a = 2, b = 2, f(n) = n \log n\]
    \[n\log n \in \Omega(n^{1 + \stackrel{?}{\epsilon}}) \]
    \[\log n \in \Omega(n^\epsilon) \text{ NON VALE}\]
    Poiché un logaritmo è sempre più piccolo di un polinomio.
    Questo è un caso dove il teorema \textit{non} si applica
\end{examplebox}


\subsection{Merge Sort (A, n)}

Questo algoritmo di ordinamento \textit{ricorsivo} utilizza il concetto di \textit{divide et impera}.

Concettualmente, un merge sort funziona come segue:

\begin{enumerate}
    \item \textbf{Dividi} l'array non ordinato in n sottoarray, ognuno contenente un elemento (un array di un elemento è considerato ordinato).
    \item \textbf{Unisci} ripetutamente i sottoarray per produrre nuovi sottoarray ordinati finché non ne rimane solo uno. Questo sarà l'array ordinato.
\end{enumerate}
La sua complessità considerando il merge con complessità lineare risulta:
\[T(n) = 2T\left(\frac{n}{2}\right) + n\]
Utilizzando il \textbf{Master Theorem} e cadendo del \textit{secondo caso} possiamo confermare che il risultato è:
\[= \Theta(n \log n)\]

\begin{lstlisting}[language=Scala]
// A e' l'array mentre p ed r sono rispettivamente l'indice di partenza e di arrivo
mergeSort(A, p, r) // O(n log n)
    if (p < r)
        q <- floor((p+r)/2)
        mergeSort(A, p, q)
        mergeSort(A, q+1, r)
        merge(A, p, q, r)
\end{lstlisting}
\begin{lstlisting}[language=Scala]
merge(A, p, q, r)
    i <- 1
    j <- p
    k <- q+1
    // Ordina gli elementi di A in B
    // O il lato sinistro ha finito
    while(j <= q or k <= r) // O(n)
        if j <= q and (k > r or A[j] <= A[k])
            B[i] <- A[j]
            j++
        else
            B[i] <- A[k]
            k++
        i++
    // Copia gli elementi di B in A
    for i <- 1 to r-p+1  // O(n)
        A[p+i-1] <- B[i]
\end{lstlisting}
L'algoritmo è \textbf{stabile} poiché non vengono scambiati gli elementi uguali. Tuttavia non ordina \textbf{in loco} poiché utilizza uno spazio di memoria aggiuntivo.
\subsection{Heap}
L'Heap è un albero semicompleto (ogni nodo ha 2 figli ad ogni livello tranne l'ultimo  che è completo solo fino ad un certo unto) in cui i nodi contengono oggetti con relazione di ordinamento.\\\\
\textbf{\textit{Proprietà Heap:}}

\[\forall \text{ nodo, il contenuto è } \ge \text{ del contenuto dei figli}\]

\begin{figure}[H]
    \centering

    \begin{forest}
for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=1.5em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1.5cm % Spaziatura verticale tra i livelli
}
  [16, edge label={node[right, xshift=7pt] {1}}
    [14, edge label={node[above, yshift=7pt] {2}}
      [8, edge label={node[above, yshift=7pt] {4}}
        [1, edge label={node[above, yshift=7pt] {8}}]
        [2, edge label={node[above, yshift=7pt] {9}}]
      ]
      [9, edge label={node[above, yshift=7pt] {5}}
        [3, edge label={node[right, xshift=7pt] {10}}]
      ]
    ]
    [10, edge label={node[above, yshift=7pt] {3}}
      [2, edge label={node[above, yshift=7pt] {6}}]
      [3, edge label={node[above, yshift=7pt] {7}}]
    ]
  ]
\end{forest}
    \label{fig:enter-label}
\end{figure}
La complessità dell'algoritmo è in base al numero di livelli dell'albero.\\\\
$\longrightarrow$ albero con $n$ livelli:
\[\# \text{Nodi} = 2^0 + 2^1 + 2^2 + ... + 2^{n - 1} = \frac{1 - 2^n}{1-2} = 2^n - 1\]
$\longrightarrow$ albero con $n$ nodi:
\[\# \text{Livelli} = \log_2n\]
\[\#\text{Foglie} = \frac{n}{2}\]
Le foglie di un albero sono la metà dei nodi dell'albero.

\begin{lstlisting}[language=Scala]
extractMax(H) // O(log n)
    h[1] <- H[H.heap_size()]
    H.heap_size()--
    heapify(H, 1)
\end{lstlisting}

\begin{lstlisting}[language=Scala]
heapify(H, 1) // O(log n)
    l <- left[i] //
    r <- right[i]
    if(l < h.heap_size() AND H[l] > H[i]
        largest <- l
    else
        largest <- i
    if m <= h.heap_size() and H[r] > H[largest]
        largest <- r
    if largest != i
        swap(H[i], H[largest])
        heapify(H, largest)
\end{lstlisting}
Creare uno heap da un array:
\begin{lstlisting}[language=Scala]
buildHeap(A)
    A.heap_size() <- length[A]
        for i <- length[A]/2 down to 1
            heapify(A, i)
\end{lstlisting}
Immagino tutte le foglie come heap con un solo nodo. L'indice del primo nodo che non è heap corrisponde a $\frac{length(A)}{2}$ su questo allora chiamo \texttt{heapify}.

\pagebreak

\subsection{Heapsort}
\begin{lstlisting}[language=Scala]
HeapSort(A) // O(n log n)
    buildHeap(A)
    for i <- length(A) to 1
        scambia(A[1], A[i])
        heapsize(A)--
        heapify(a, i)
\end{lstlisting}
L'Heap Sort è un algoritmo che lavora \textbf{in loco} tuttavia non è \textbf{stabile}. Tuttavia riusciamo a fare una stima migliore e più corretta?
\[n \log i = \sum_{i=1}^n \log i = \log \prod_{i=1}^n i = \log n! = \Theta(\log n^n) = \Theta(n \log n)\]
In questo caso, essere accurati non aiuta, ma abbiamo avuto la certezza che non esiste una stima migliore.
\subsection{Quicksort}
Come funziona l'algoritmo?
\begin{enumerate}
    \item Dividi prima l'array in due parti. (Partizione)
    \item  Devi essere sicuro che tutti gli elementi di sinistra siano $\le$ di quelli di destra. Ricorsivamente ordina la parte sinistra e la parte destra.
    \item A questo punto l'array è ordinato.
\end{enumerate}

\begin{lstlisting}[language=Scala]
quickSort(A, p, r)
    if (p < r)
        q <- partition(a, p, r)
        quickSort(A, p, q)
        quickSort(A, q+1, r)
\end{lstlisting}

\begin{lstlisting}[language=Scala]
partition(A, p, r) // O(n)
    x <- A[p] // Elemento Perno
    i <- p-1
    k <- r+1
    while true
        repeat j-- until a[j] <= x
        repeat i++ until a[i] >= x
        if i < j
            scambia(a[i], a[j])
        else
            ret j
\end{lstlisting}
Scegliamo un elemento a caso in base a quello comparato rispetto all'elemento perno tale che:
\[sx \le perno \le dx\]
Questo algoritmo non è \textbf{stabile} ma lavora \textbf{in loco}.
La sua complessità?
\[T(n) = T(\text{partition}) + T(q) + T(n-q)\]
Se il quicksort è perfettamente diviso in due, allora la sua complessità è $O(n \log n)$.
Se invece l'array è già ordinato la sua equazione di ricorrenza sarà:
\[= n + T(1) + T(n-1) = \colorbox{yellow!50!white}{$\Theta(n^2)$}\]
Tuttavia non ci aspettiamo che questo caso sia frequente e quindin nella stra grande maggioranza dei casi allora:
\[T(n) = n + T(cn) + T((1-c)n)\]
Un equazione di questo tipo sappiamo che ha come complessità $\Theta(n \log n)$.
\begin{lstlisting}[language=Scala]
rand_Partition(A, p, r)
    i <- rand(p .. r) // Ora l'elemento perno e' un elemento a caso
    scambio(A[p], A[i]
    ret partition(A, p, r)
\end{lstlisting}

\begin{align*}
    T(n) &= n + \frac{1}{n}(T(1) + T(n-1) + \frac{1}{n}(T(2) + (T-2)) + \; \dots \; + \\
    & \frac{1}{n}(T(n-2) + T(2)) + \frac{1}{n}(T(n-1) + T(1) = \\
    &= n + \frac{1}{n} \sum_{i}(T(i) + T(n-i))\\
    &= n + \frac{2}{n} \sum_{i} T(i) \in \colorbox{green!30!white}{$O(n \log n$)}
\end{align*}
Qualsiasi algoritmo che\textit{ lavora per confronti }deve fare almeno $O(n \log n)$.

\section{Algoritmi di ordinamento in tempo lineare}

\subsection{Algoritmi non basati su confronti}

\subsubsection{Counting Sort}
Tuttavia possiamo trovare algoritmi che come tempo
di esecuzione hanno tempo lineare. Come?
Non lavorando a \textbf{confronti}.
\\\\
Come ordinare $n$ numeri con valori da 1 a $k$?
\begin{lstlisting}[language=Scala]
countingSort(A, k)
    for i <- 1 to k
        C[i] <- 0
    for j <- 1 to len(A)
        C[A[j]]++
    for i <- 2 to k
        C[i] <- C[i-1]+C[i]
    for j <- len(A) down to 1
        B[C[A[j]]] <- a[j]
        C[A[J]]--
\end{lstlisting}
La complessità di questo algoritmo è $O(n + k)$ dove $n$ è la lunghezza dell'array e $k$ e il range di valori.

\subsubsection{Radix Sort}

Il radix sort è un algoritmo di ordinamento che ordina gli elementi
confrontando i singoli bit. Quello che fa è ordinare per la cifra meno
significativa, poi per la seconda cifra meno significativa e così via.


\begin{lstlisting}[language=Scala]
radixSort(A, d) // O(d(n+k))
  for i <- 1 to d
      countingSort(A, n)
\end{lstlisting}
La complessità di questo algoritmo è \[\Theta(d(n+k))\]dove $d$ è il numero di cifre e $k$ è il range di valori.
Se si vuole invece ordinare $n$ valori da 1 a $n^2 - 1$, le costanti nascoste all'interno
del $\Theta$ sono molto alte e quindi non è un algoritmo efficiente. Tuttavia
si posson rappresentare i numeri in base $n$ e quindi ottenere un algoritmo lineare.

\subsubsection{Bucket Sort}

Il Bucket Sort è un algoritmo di ordinamento applicabile quando la distribuzione dei dati è nota. Quindi
su un array di $n$ elementi \textbf{distribuiti uniformemente} su $[0, 1)$, si può dividere l'intervallo in $n$ sottointervalli
con probabilità $\frac{1}{n}$ e poi ordinare i singoli sottointervalli chiamate anche \textit{"Bucket"}.
Infatti se i dati sono distribuiti uniformemente allora la complessità dell'algoritmo è lineare: \[\Theta(n)\]Questo
perché in ogni bucket ci si aspetta un valore costante e quindi indipendente dal valore di $n$.
\\\\Il caso pessimo però è quando tutti gli elementi ricadono nello stesso bucket. La probabilità che questo accada è molto bassa
infatti è: \[\underbrace{\frac{1}{n} \ast \frac{1}{n} \ast \dots \ast \frac{1}{n}}_{n-1}  = \frac{1}{n^{n-1}}\]e la sua complessità diventa: \[O(n^2)\]
\\\\
Sia \(X_{ij}\) la variabile aleatoria che vale:
\[
\begin{cases}
  1 & \text{Se l'elemento } i \text{ va nel bucket } j\\
  0 & \text{altrimenti}
\end{cases}
\]
Per esprimere il numero di elementi nel bucket $j$ si ha:
\[
  N_j = \sum_{i} X_{ij}
\]
La complessità di questo algoritmo quindi può essere espressa come:
\[
  C = \sum_j {N_j}^2
\]
Dove il valore atteso è:
\[E[C] = E\left[ \sum_j {N_j}^2\right] = \sum_j E[{N_j}^2] = \sum_j\left(Var(N_j) - E[N_j]^2\right)\]
Dove $E[N_j]$ è:
\[E[N_j] = \sum E[X_{ij}] = \sum_{j=1}^n \frac{1}{n} = 1\]
\[Var[N_j] = \sum Var(X_{ij}) = \sum \frac{1}{n} * \left(1 - \frac{1}{n}\right) = 1 - \frac{1}{n}\]
E quindi possiamo svolgere il calcolo precedente dove:

\begin{align*}
  \sum_j \left(Var(N_j) - E[N_j]^2\right) &=  \sum_j \left(\left(1 - \frac{1}{n}\right) + 1\right) \\ &= \sum_j 2 - \frac{1}{n}\\  &= 2n - 1
\end{align*}
Le distribuzioni possono essere arbitrarie ma basta che tutti i bucket abbiano la stessa probabilità.
Prendiamo:
\[n_1, ..., n_2, ... , n_l\]
\[\frac{1}{n_1}, \, ... \,  , \frac{1}{n_2}, \, ... \, , \frac{1}{n_l}\]
La \textit{turing-riduzione} è un algoritmo che riduce un problema ad un altro problema.


\section{Algoritmi di selezione}
Dato in input un array \( A \) di oggetti su cui è definita una relazione di ordinamento
e un indice \( i \) compreso tra \( 1 \) e \( n \) (\( n \) è il numero di oggetti
nell'array), l'output dell'algoritmo è l'oggetto che si trova in posizione \( i \)
nell'array ordinato.
\begin{lstlisting}[language=Scala]
selezione(A, i)
  ordina(A) // O(n log n)
  return A[i]
\end{lstlisting}
Quindi la complessità di questo algoritmo nel caso peggiore è \( O(n \log n) \)
(limite superiore). È possibile selezionare un elemento in tempo lineare? Analizziamo
un caso particolare dell'algoritmo di selezione, ovvero la ricerca del minimo (o del massimo).


\subsection{Ricerca del minimo o del massimo}
\vspace{1em}
\noindent
In tempo lineare si può trovare il minimo e il massimo
di un array:
\begin{lstlisting}[language=Scala]
minimo(A)
  min <- A[1]
  for i <- 2 to length[A]
    if A[i] < min
      min <- A[i]
  return min
\end{lstlisting}
trovare il minimo equivale a trovare \texttt{selezione(A, 1)} e trovare il massimo
equivale a trovare \texttt{selezione(A, n)}. Si può però andare sotto la complessità
lineare?

\vspace{1em}
\noindent
Per trovare il massimo (o il minimo) elemento \( n \) di un array bisogna fare
\textbf{almeno} \( n-1 \) confronti perchè bisogna confrontare ogni elemento con
l'elemento massimo (o minimo) trovato per poter dire se è il massimo (o minimo).
Di conseguenza, non è possibile avere un algoritmo per la ricerca del massimo (o minimo)
in cui c'è un elemento che non "perde" mai ai confronti (cioè risulta sempre il più
grande) e non viene dichiarato essere il più grande (o più piccolo).

\vspace{1em}
\noindent
\textbf{Dimostrazione}:
Per dimostrarlo si può prendere un array in cui l'elemento \( a \) non perde mai ai
confronti, ma l'algoritmo dichiara che il massimo è l'elemento \( b \). Allora si rilancia
l'algoritmo sostituendo l'elemento \( a \) con \( a = \text{\texttt{max(b+1,a)}} \) e si
ripete l'algoritmo con questo secondo array in cui \( a \) è l'elemento più grande. Si ha
quindi che i confronti in cui \( a \) non è coinvolto rimangono gli stessi e i confronti
in cui \( a \) è coinvolto non cambiano perchè anche prima \( a \) non perdeva mai ai
confronti, di conseguenza l'algoritmo dichiarerà che il massimo è \( b \) e quindi
l'algoritmo non è corretto, dimostrando che non esiste un algoritmo che trova il massimo
in meno di \( n-1 \) confronti.

\vspace{1em}
\noindent
Abbiamo quindi trovato che la complessità del massimo (o minimo) nel caso migliore è
\( \Omega(n) \) (limite inferiore) e nel caso peggiore è \( O(n) \) (limite superiore).
Di conseguenza la complessità è \( \Theta(n) \).

\subsubsection{Ricerca del minimo e del massimo contemporaneamente}
Si potrebbe implementare unendo i 2 algoritmi precedenti:
\begin{lstlisting}[language=Scala]
min_max(A)
  min <- A[1]
  max <- A[1]
  for i <- 2 to length[A]
    if A[i] < min
      min <- A[i]
    if A[i] > max
      max <- A[i]
  return (min, max)
\end{lstlisting}
Questo algoritmo esegue \( n-1 + n-1 = 2n-2 \) confronti.

\begin{itemize}
  \item \textbf{Limite inferiore}: Potenzialmente ogni oggetto potrebbe essere il minimo
    o il massimo. Sia \( m \) il numero di oggetti potenzialmente minimi e \( M \) il
    numero di oggetti potenzialmente massimi. Sia \( n \) il numero di oggetti nell'array.
    \begin{itemize}
      \item All'inizio \( m+M = 2n \) perchè ogni oggetto può essere sia minimo che
        massimo.
      \item Alla fine \( m+M = 2 \) perchè alla fine ci sarà un solo minimo e un solo
        massimo.
    \end{itemize}
    Quando viene fatto un confronto \( m+M \) può diminuire.
    \begin{itemize}
      \item Se si confrontano due oggetti che sono potenzialmente sia minimi che massimi,
        allora \( m+M \) diminuisce di \( 2 \) perchè:
        \[
          a < b
        \]
        \( b \) non può essere il minimo e \( a \) non può essere il massimo e si perdono
        2 potenzialità.

      \item Se si confrontano due potenziali minimi (o massimi), allora \( m+M \)
        diminuisce di \( 1 \) perchè:
        \[
          a < b
        \]
        \( b \) non può essere il minimo e si perde 1 potenzialità.
    \end{itemize}
    Un buon algoritmo dovrebbe scegliere di confrontare sempre 2 oggetti che sono
    entrambi potenziali minimi o potenziali massimi.

    \vspace{1em}
    \noindent
    Due oggetti che sono potenzialmente sia minimi che massimi esistono
    se \( m+M > n+1 \) perchè se bisogna distribuire n potenzialità ne avanzano
    due che devono essere assegnate a due oggetti che hanno già una potenzialità.
    Quindi fino a quando \( m+M \) continua ad essere almeno \( n+2 \) si riesce a
    far diminuire \( m+M \) di 2 ad ogni confronto.

    Questa diminuzione si può fare \( \left\lfloor \frac{n}{2} \right\rfloor \) volte,
    successivamente \( m+M \) potrà calare solo di 1 ad ogni confronto.

    \vspace{1em}
    \noindent
    Successivamente il numero di oggetti rimane:
    \[
      \begin{cases}
        n+1 & \text{se } n \text{ è dispari}\\
        n & \text{se } n \text{ è pari}
      \end{cases}
    \]
    \begin{itemize}
      \item \( n \) dispari:
        \[
          \begin{aligned}
            &n+1 - 2 + \left\lfloor \frac{n}{2} \right\rfloor\\
            &= n-1 + \left\lfloor \frac{n}{2} \right\rfloor\\
            &= \left\lfloor \frac{3}{2}n \right\rfloor - 1\\
            &= \left\lceil \frac{3}{2}n \right\rceil - 2\\
          \end{aligned}
        \]

      \item \( n \) pari:
        \[
          \begin{aligned}
            &n - 2 + \left\lfloor \frac{n}{2} \right\rfloor \\
            &= n-2 + \frac{n}{2}\\
            &= \frac{3}{2}n - 2\\
            &= \left\lceil \frac{3}{2}n \right\rceil -2
          \end{aligned}
        \]
    \end{itemize}
    Quindi la complessità è \( \Omega(\left\lceil \frac{3}{2}n \right\rceil -2) = \Omega(n)
    \) (limite inferiore). Meglio di così non si può fare, ma non è detto che esista
    un algoritmo che raggiunga questo limite inferiore.
\end{itemize}
Un algoritmo che raggiunge il limite inferiore è il seguente:
\begin{enumerate}
  \item Dividi gli oggetti in 2 gruppi:
    \[
      \underbrace{
        \underbrace{
          \begin{aligned}
          &a_1\\
          &a_2\\
          &\vdots\\
          &a_{\left\lfloor \frac{n}{2} \right\rfloor}
          \end{aligned}
        }_{\text{Potenziali minimi}}
        \quad
        \underbrace{
          \begin{aligned}
        &b_1\\
        &b_2\\
        &\vdots\\
        &b_{\left\lceil \frac{n}{2} \right\rceil}
          \end{aligned}
        }_{\text{Potenziali massimi}}
      }_{\text{Potenziali sia minimi che massimi}}
    \]

  \item Confronta \( a_i \) con \( b_i \), supponendo \( a_i < b_i \) (mette a sinistra
    i più piccoli e a destra i più grandi). Una volta aver fatto il confronto possiamo swappare
     gli elementi nella loro apposita sezione.

  \item Cerca il minimo degli \( a_i \) e cerca il massimo dei \( b_i \):

  \item Sistema l'eventuale elemento in più (se l'array è dispari)
\end{enumerate}

\subsection{Randomized select}
Si può implementare un algoritmo che divide l'array in 2 parti allo stesso modo
in cui viene effettuata la \texttt{partition} di quick sort:
\begin{lstlisting}[language=Scala]
// A: Array
// p: Indice di partenza
// r: Indice di arrivo
// i: Indice che stiamo cercando (compreso tra 1 e r-p+1)
randomized_select(A, p, r, i)
  if p = r
    return A[p]
  q <- randomized_partition(A, p, r)
  k <- q - p + 1 // Numero di elementi a sinistra
  // Controlla se l'elemento cercato e' a sinistra o a destra
  if i <= k
    return randomized_select(A, p, q, i) // Cerca a sinistra
  else
    return randomized_select(A, q+1, r, i-k) // Cerca a destra
\end{lstlisting}
\begin{itemize}
  \item
    Se dividessimo sempre a metà si avrebbe:
    \[
      T(n) = n + T\left(\frac{n}{2}\right) = \Theta(n) \text{ (terzo caso del teorema dell'esperto)}
    \]

  \item Mediamente:
    \[
      \begin{aligned}
        T(n) &= n + \frac{1}{n} T \left( max(1,n-1) \right) + \frac{1}{n} T \left( max(2,n-2) \right)
        + \dots\\
             &= n + \frac{2}{n} \sum_{i=\frac{n}{2}}^{n-1} T \left( i \right)\\
      \end{aligned}
    \]
    La complessita media è lineare.

    Si esegue un solo ramo, che nel caso pessimo è quello con più elementi. La risoluzione
    è la stessa del quick sort.
\end{itemize}
Esiste un algoritmo che esegue la ricerca in tempo lineare anche nel caso peggiore?

Si potrebbe cercare un elemento perno più ottimale, cioè che divida l'array in
\textbf{parti proporzionali}:
\begin{enumerate}
  \item Dividi gli oggetti in \( \left\lfloor \frac{n}{5} \right\rfloor \) gruppi di
    5 elementi più un eventuale gruppo con meno di 5 elementi.

  \item Calcola il mediano di ogni gruppo di 5 elementi (si ordina e si prende l'elemento
    centrale). \( \Theta(n) \)

  \item Calcola ricorsivamente il mediano \( x \) dei mediani
    \[
      T\left(\left\lceil \frac{n}{5} \right\rceil\right)
    \]

  \item Partiziona con perno \( x \) e calcola \( k \) (numero di elementi a sinistra).
    \( \Theta (n) \)

  \item Se \( i<k \) cerca a sinistra l'elemento \( i \), altrimenti cerca a destra
    l'elemento \( i-k \). La chiamata ricorsiva va fatta su un numero di elementi
    sufficientemente piccolo, e deve risultare un proporzione di \( n \), quindi
    ad esempio dividere in gruppi da 3 elementi non funzionerebbe.
    \[
    T(?)
    \]
    \[
      \begin{aligned}
        m_1 \;\;&\to\;\; m_2 \;\;&\to\;\; m_3 \;\;&\to\;\; \color{red}\underset{x}{m_4} \;\;&\to\;\; \color{green!50!black}m_5
        \;\;&\to\;\; \color{green!50!black}m_6 \;\;&\to\;\; \color{blue}m_7\\
             &&&& \downarrow \quad&\qquad \downarrow &\downarrow \;\;\\
             &&&& \color{green!50!black}m_{5,4} &\qquad \color{green!50!black}m_{6,4} & \color{blue}m_{7,4} \\
             &&&& \downarrow \quad&\qquad \downarrow &\\
             &&&& \color{green!50!black}m_{5,5} &\qquad \color{green!50!black}m_{6,5} & \\
      \end{aligned}
    \]
    Gli elementi verdi sono maggiori dell'elemento \( x \) e ogni elemento verde avrà
    2 elementi maggiori di esso (tranne nel caso del gruppo con meno di 5 elementi
    rappresentato in blu).
    \[
      3 \cdot  \left(\underbrace{\left\lceil \frac{1}{2} \left\lceil \frac{n}{5} \right\rceil  \right\rceil}_{\text{
          verdi + blu + rosso
      }} - \underbrace{2}_{\text{rosso + blu}} \right)
      = \frac{3}{10} n - 6
    \]
    Da ogni parte si hanno almeno \( \frac{3}{10} n - 6 \) elementi, quindi
    al massimo si hanno \( n - \left( \frac{3}{10} n - 6 \right) = \frac{7}{10} n + 6 \)
    elementi.

    \vspace{1em}
    \noindent
    Quindi abbiamo trovato \( T(?) \):
    \[
      T(n) = \Theta (n) + T\left(\left\lceil \frac{n}{5} \right\rceil\right) + T\left(\frac{7}{10} n + 6\right)
    \]
    Applichiamo il metodo di sostituzione \( T(n) \le cn \):
    \[
      \begin{aligned}
        T(n) &\le n + c \left\lceil \frac{n}{5} \right\rceil + c \left( \frac{7}{10}
        n + 6\right) \end{aligned}
    \]
    Tuttavia non sappiamo se $\left(\frac{7}{10}n + 6\right)$ sia $< n$. Scopriamo che la disuguaglianza è vera
    solo se $n > 20$. Quindi per risolvere il problema ci basta scegliere un $\overline{n} > 20$. Le costanti quindi
    sono abbastanza alte.
    \[
        \begin{aligned}
             & \le n + c + n \frac{c}{5} + \frac{7}{10} cn + 6c \\
             &= \frac{9}{10}cn + 7c + n \\
             & \stackrel{?}{\le} cn
      \end{aligned}
    \]
    \[
      \begin{aligned}
        &=  cn + \left(- \frac{1}{10}cn + 7c + n\right) \le cn \\
        &\stackrel{\text{sse}}{\Longleftrightarrow} \left(n + 7c - \frac{1}{10}cn\right)\le 0\\
      \end{aligned}
    \]
    Dove l'equazione di ricorrenza diventa:
    \[T(n) \le \Theta(n) + c\left\lceil\frac{n}{5}\right\rceil + c\left(\frac{7}{10} + 6\right)\]
    Quindi \( T(n) \le cn \) e quindi \( T(n) \in \Theta(n) \). Quindi è un algoritmo ottimo.
    Il problema è che le costanti sono così alte che nella pratica è meglio il \texttt{randomized\_select}.
\end{enumerate}
Esistono modi per strutturare meglio le informazioni nel calcolatore
per trovare l'elemento cercato in tempo $O(\log n)$? Dobbiamo trovare delle rappresentazione
che ci permettono di rispondere a delle domande in tempo logaritmico.

\pagebreak

\section{Strutture dati}

Una struttura dati è un modo per organizzare i dati in modo da poterli manipolare in modo efficiente.

\subsection{Stack o Pila}
\begin{definition}
  Uno stack è una struttura dati che permette di inserire e rimuovere elementi in modo LIFO (Last In First Out).
  \begin{itemize}
      \item \texttt{push(x)}: Inserisce l'elemento \( x \) nello stack.
      (Produce un oggetto che corrisponde allo stack originale a cui è stato aggiunto l'elemento).
      \item \texttt{pop(S)}: Rimuove l'elemento in cima allo stack. (Produce un oggetto che corrisponde all'elemento originale a cui è stato rimosso l'elemento).
      \item \texttt{top(S)}: Restituisce l'elemento in cima allo stack.
      \item \texttt{new()}: Crea uno stack vuoto.
      \item \texttt{isEmpty(S)}: Restituisce \texttt{true} se lo stack è vuoto, \texttt{false} altrimenti.
  \end{itemize}
\end{definition}

Da queste operazioni si possono definire certe proprietà come:

\begin{itemize}
  \item top(push(S,x)) = x
  \item pop(push(S,x)) = S
\end{itemize}
Quindi un modo per definire uno stack tramite le operazioni che andiamo a fare sull'oggetto stesso:

\begin{center}
\texttt{Push(Push(Push(Empty(), $x_1$) $x_2$) $x_3$) ...}
\end{center}

\subsection{Queue o Coda}

\begin{definition}
  Una coda è una struttura dati che permette di inserire e rimuovere elementi in modo FIFO (First In First Out).
  \begin{itemize}
      \item \texttt{enqueue(x)}: Inserisce l'elemento \( x \) in coda alla coda. $\longrightarrow$ O(1) se ci tiene salvata il puntatore alla fine dell'array
      \item \texttt{dequeue(Q)}: Rimuove l'elemento in testa alla coda. $\longrightarrow$ O(1)
      \item \texttt{head(Q)}: Restituisce l'elemento in testa alla coda.
      \item \texttt{new()}: Crea una coda vuota.
      \item \texttt{isEmpty(Q)}: Restituisce \texttt{true} se la coda è vuota, \texttt{false} altrimenti.
  \end{itemize}
\end{definition}

\subsection{Binary Tree o Albero binario}
\begin{definition}
  Un albero binario è una struttura dati che permette di organizzare i dati in modo gerarchico. Ogni nodo ha al massimo 2 figli.
  \begin{itemize}
      \item \texttt{new()}: Crea un albero vuoto.
      \item \texttt{root(T)}: Restituisce la radice dell'albero.
      \item \texttt{left(T)}: Restituisce il sottoalbero sinistro.
      \item \texttt{right(T)}: Restituisce il sottoalbero destro.
      \item \texttt{key(T)}: Restituisce la chiave del nodo.
      \item \texttt{isEmpty(T)}: Restituisce \texttt{true} se l'albero è vuoto, \texttt{false} altrimenti.
    \end{itemize}

\end{definition}

\begin{figure}[H]
    \centering

    \begin{forest}
for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=1.5em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1.5cm % Spaziatura verticale tra i livelli
}
  [16, edge label={node[right, xshift=7pt] {1}}
    [14, edge label={node[above, yshift=7pt] {2}}
      [8, edge label={node[above, yshift=7pt] {4}}
        [1, edge label={node[above, yshift=7pt] {8}}]
        [2, edge label={node[above, yshift=7pt] {9}}]
      ]
      [9, edge label={node[above, yshift=7pt] {5}}
        [3, edge label={node[right, xshift=7pt] {10}}]
        [7, edge label={node[above, yshift=7pt] {11}}]
      ]
    ]
    [10, edge label={node[above, yshift=7pt] {3}}
      [2, edge label={node[above, yshift=7pt] {6}}]
      [3, edge label={node[above, yshift=7pt] {7}}]
    ]
  ]
\end{forest}
 \label{fig:albero_binario}
 \caption{Esempio di albero binario}
\end{figure}
\noindent
La profondità di un albero binario è il logaritmo in base 2 del numero di nodi.

\[P(n) = 1 + P\left(\left\lceil\frac{n-1}{2}\right\rceil\right) = \Theta(\log_2 n)\]

\subsection{Binary Search Tree o Albero binario di ricerca}

\begin{definition}
  Un albero binario di ricerca è un albero binario in cui per ogni nodo \( x \) valgono le seguenti proprietà:
  \begin{itemize}
      \item Tutti i nodi nel sottoalbero sinistro di \( x \) hanno chiavi minori di \( x \).
      \item Tutti i nodi nel sottoalbero destro di \( x \) hanno chiavi maggiori di \( x \).
  \end{itemize}
\end{definition}


\begin{figure}[H]
  \centering

  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=1.5em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1.5cm % Spaziatura verticale tra i livelli
}
[20, edge label={node[right, xshift=7pt] {1}}
  [15, edge label={node[above, yshift=7pt] {2}}
    [8, edge label={node[above, yshift=7pt] {4}}
      [4, edge label={node[above, yshift=7pt] {8}}]
      [9, edge label={node[above, yshift=7pt] {9}}]
    ]
    [19, edge label={node[above, yshift=7pt] {5}}
      [16, edge label={node[right, xshift=7pt] {10}}]
      [23, edge label={node[above, yshift=7pt] {11}}]
    ]
  ]
  [30, edge label={node[above, yshift=7pt] {3}}
    [26, edge label={node[above, yshift=7pt] {6}}]
    [35, edge label={node[above, yshift=7pt] {7}}]
  ]
]
\end{forest}
\caption{Esempio di albero binario di ricerca}
\end{figure}


\noindent
Per cercare un elemento all'interno dell'albero binario di ricerca si può fare in \textit{tempo logaritmico}.
Infatti la sua complessità è $\Theta(\log n)$. Tuttavia nel caso peggiore devo scorrere tutto l'albero e quindi
l'algoritmo finisce ad aveere $O(n)$. Le funzioni che potremmo implementare per questa struttura potrebbe essere:
\begin{itemize}
  \item \texttt{search(T, k)}: Cerca la chiave \( k \) nell'albero \( T \).
  \item \texttt{insert(T, k)}: Inserisce la chiave \( k \) nell'albero \( T \). $\Longrightarrow \Theta(\log n)$ 
  \item \texttt{extract(T, k)}: Estrae la chiave \( k \) dall'albero \( T \). $\Longrightarrow \Theta(\log n)$ 
\end{itemize}
\noindent
Potremmo essere capaci di inserire un elemento nella giusta posizione in tempo logaritmico se l'albero è sbilanciato?
No, infatti se l'albero è sbilanciato la complessità diventa lineare. Tuttavia ci sono dei workaround ma serviranno degli alberi particolari.
\\\\
Una volta che un nodo viene rimosso o aggiunto non puoi essere certo che l'albero sia ancora ribilanciato. Quindi quello che bisogna fare
è ribilanciare l'albero ogni volta che si rimuove o si inserisce un nodo. 

\subsection{Liste doppiamente concatenate}

Come faccio ad eliminare un nodo all'interno della lista evitando di scrivere una lunga sintassi per riassegnare i puntatori dell'elemento prima e dell'elemento dopo?
Utilizzo le sentinelle.

\subsubsection{Sentinelle}

Inserisco due nodi speciali all'inizio e alla fine della lista. Questi nodi speciali non contengono dati e sono chiamati sentinelle. 
Questi nodi speciali mi permettono di eliminare un nodo in modo più semplice. A volte l'efficienza non è la cosa più importante, infatti ci sono alcuni pezzi di codice
che vogliamo ottimizzare nel migliore dei modi. Per tutti gli altri siamo disposti a perdere un po' di memoria. Proprio come le sentinelle. 


\subsection{RB-Tree o Albero rosso-nero}

\begin{definition}
Perché si chiamano red black? Questi sono normali alberi di ricerca, devono rispettare alcune proprietà:

\begin{itemize}
  \item Ogni nodo è rosso o nero.
  \item Ogni foglia è necessariamente nera.
  \item Figli di un rosso sono necessariamente neri.
  \item Ogni cammino radice-fopglia ha lo stesso numero di nodi neri.
\end{itemize}
\end{definition}

\begin{figure}[H]
  \centering

  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=1.5em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=0.2cm
}
[26, 
  [17, 
    [14,
      [10, 
      [7
        [3]
      ]
      [12]
      ]
      [10]
    ]
    [21 
      [19
        [20]
      ]
      [23]
    ]
  ]
  [41
    [30
      [28]
      [38
        [33]
        [39]
      ]
    ]
    [47]
  ]
]
\end{forest}
\caption{Esempio di albero binario}
\end{figure}
\noindent
La proprietà 1 e 3 sono rispettate mentre la proprietà 2. Per sistemare questo problema potremmo bilanciare l'albero aggiungendo alle foglie rossi dei nodi 
"NIL" che saranno neri. In questo modo la proprietà 2 è rispettata.

\begin{figure}[H]
  \centering

  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=1.5em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=0.2cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
[26, 
  [17, 
    [14,
      [10, 
      [7
        [3
          [\text{NIL}]
          [\text{NIL}]
        ]
        [\text{NIL}]
      ]
      [12
        [\text{NIL}]
        [\text{NIL}]
      ]
      ]
      [10
        [\text{NIL}]
        [\text{NIL}]
      ]
    ]
    [21 
      [19
        [20
        [\text{NIL}]
        [\text{NIL}]
        ]
      ]
      [23
        [\text{NIL}]
        [\text{NIL}]
      ]
    ]
  ]
  [41
    [30
      [28
      [\text{NIL}]
      [\text{NIL}]
      ]
      [38
        [33
        [\text{NIL}]
        [\text{NIL}]
        ]
        [39
        [\text{NIL}]
        [\text{NIL}]
        ]
      ]
    ]
    [47
      [\text{NIL}]
      [\text{NIL}]
    ]
  ]
]
\end{forest}
\caption{RB Albero bilanciato con i NIL}
\end{figure}
\noindent
Il concetto principlae di questo tipi di alberi è quello della \textbf{Black Height} (bh(X) dove X è una foglia) cioè il
numero di nodi neri che si incontrano lungo un cammino dalla radice ad una foglia.

\begin{lemma}
  Per ogni nodo $X$ il sottoalbero radicato in $X$ ha almeno $2^{bh(X)} - 1$ nodi.
\end{lemma}
Proviamo a dimsotrare il lemma:
\begin{proof}
  Dimostriamo il lemma per induzione. Per $bh(X) = 0$ il lemma è vero. Supponiamo che il lemma sia vero per tutti i nodi.
 Allora i figli di $X$ hanno $bh(X) = k-1$. Per ipotesi induttiva:

  \begin{figure}[H]
    \centering
  
    \begin{forest}
  for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
    [x
      [a]
      [b]
    ]
  \end{forest}
  \end{figure}
  dove $bh(a) \ge bh(X) - 1$ e $bh(b) \ge bh(x) - 1$
  \begin{align*}
  \# nodi &\ge 2^{bh(a)} - 1 + 2^{bh(b)} - 1 + 1\\
  &\ge 2^{bh(X)-1} - 1 + 2^{bh(X)-1} \cancel{- 1 + 1}\\
  &\ge 2 \cdot 2^{bh(X) - 1} - 1\\
  &= 2^{bh(X)} - 1 \; \; \; \square
  \end{align*}
\end{proof}
Prendiamo un albero RB di altezza $h$, possiamo dire che l'altezza di una foglia sia uguale a 0. 
Quindi l'altezza di un nodo che non sia una foglia sia uguale al numero di tutti i nodi che incontro lungo il cammino meno uno.
$bh(X) \ge \frac{h}{2}$
L'altezza nera è almeno la metà dell'altezza dell'albero. Quindi l'altezza dell'albero è al massimo il doppio dell'altezza nera.
\[\# \text{nodi interni} \ge 2^{\frac{h}{2}} - 1\]
\begin{align*}
  2^{\frac{h}{2}} &\le n + 1\\
  \frac{h}{2} &\le \log_2(n+1)\\
  h &\le 2 \log_2(n+1)
\end{align*}
Come faccio a decidere se il nodo inserito deve essere rosso o nero?
In questo caso, decidiamo che di base il nodo inserito sarà rosso. Se il padre del nodo inserito è rosso per la regola numero 2, allora 
quella parte di albero non sarà un RB-Albero creando un anomalia. L'oggetto RB-Albero in questo momento ha un puntatore che punta ad un nodo X 
che potrebbe essere un nodo rosso figlio di un nodo rosso. Se l'albero presenta un'anomalia, dobbiamo eliminarla. Posso decidere se propagare l'anomalia 
verso l'alto in modo che risolvo l'anomalia per tutta la profondità dell'albero. 
In questo caso ci viene in aiuto l'algoritmo di rotazione destra o sinistra che ribalincierà l'albero:
\begin{examplebox}{Esempio di rotazione dx e sx}
\begin{figure}[H]
    \centering
    \begin{forest}
  for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
    [y
      [x
        [$\alpha$]
        [$\beta$]
      ]
      [$\gamma$]
    ]
  \end{forest}
\end{figure}
\noindent
Ruota-dx $\rightarrow$:
\begin{figure}[H]
  \centering
  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
  [x
    [$\alpha$
    ]
    [y
      [$\beta$]
      [$\gamma$]
    ]
  ]
\end{forest}
\end{figure}
\end{examplebox}
\noindent
Pseudocodice dell'algoritmo della rotazione:
\begin{lstlisting}[language=Scala]
//Gli passiamo il nodo che ha l'anomalia insieme all'intero albero
// la funziona p[x] prende il parent del nodo x
while != root && color(P[x]) = red
  if p[x] = left[p[p[x]]]
    y <- right[p[p[x]]]
    if color[y] = red
      color(p[p[x]]) <- red
      color[y] <- black
      color[p(x)] <- black
      x <- p[p[x]]
    else
      if x = right[p[x]]
        x <- p[x]
        rotation-sx(x)
    
      color[p[x]] <- black
      color[p[p[x]]] <- red
      rotation-dx(p[p[x]])
      x <- root
\end{lstlisting}  

\begin{figure}[H]
  \centering
  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
  [11
    [2, red
      [1]
      [7
        [5, red
          [4(x), red]
        ]
        [8(y), red]
      ]
    ]
    [14]
  ]
\end{forest}
\end{figure}
\noindent
L'idea è quella di spostare l'anomalia in alto. Se il padre del nodo inserito è rosso, allora 
il nonno del nodo inserito sarà nero. Quindi posso cambiare colore al nonno 
e dare al nonno l'anomalia e così via, fino ad arrivare alla radice.
\begin{figure}[H]
  \centering
  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
  [11
    [2, red
      [1]
      [7(x), red
        [5
          [4, red]
        ]
        [8]
      ]
    ]
    [14(y)]
  ]
\end{forest}
\caption{Il padre e lo zio del nodo anomalo diventano nero e il nonno rosso}
\end{figure}
\noindent
Ruotiamo a sinistra 
\begin{figure}[H]
  \centering
  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
  [11
    [7, red
      [2(x), red
        [1]
        [5
          [4, red]
        ]
      ]
      [8]
    ]
    [14(y)]
    ]
\end{forest}
\caption{Rotazione a sinistra e l'ordine dei neri non è cambiato}
\end{figure}
Si cambiano i colori e si effettua una rotazione a destra:
\begin{figure}[H]
  \centering
  \begin{forest}
for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
  [7
    [2, red
      [1
      ]
      [5
        [4, red]
      ]
    ]
    [11, red
      [8]
      [14]
    ]
    ]
\end{forest}
\caption{Rotazione a sinistra e l'ordine dei neri non è cambiato}
\end{figure}
\noindent
Come faccio \textit{per l'estrazione di un elemento} ora? Se voglio eliminare un nodo rosso, non è un problema perché non c'è nessuna anomalia.
Tuttavia se voglio eliminare un nodo nero, allora devo segnarmi l'anomalia perché mi servirebbe un nero in più per fare in modo
che ogni cammino radice-foglia abbia lo stesso numero di nodi neri. L'anomalia si crea e un nodo diventa "doppiamente nero" poiché devo contenere per due neri se voglio che rimanga un RB-Albero
L'algoritmo che ribilancia l'albero anch'esso cerca di spostare l'anomalia verso l'alto ed è diviso in diversi casi:
\begin{enumerate}
  \item Se il fratello del nodo è rosso. Sappiamo che DEVONO esistere il padre, lo zio e i nipoti grazie alle regole dell'RB-Tree.
  \begin{figure}[H]
    \centering
    \begin{forest}
  for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b
      [a (x)]
      [d, red
        [c]
        [e]
      ]
  ]
  \end{forest}
  \end{figure}
  \noindent
  allora il padre del nodo diventa rosso e il fratello diventa nero. Si effettua una rotazione a sinistra su B
  \begin{figure}[H]
    \centering
    \begin{forest}
  for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [d
      [b, red
        [a(x)]
        [c]
      ]
      [e]
  ]
  \end{forest}
  \end{figure}
  \item Se il fratello del nodo è nero andiamo a controllare i nipoti e sia C che E sono neri. (Il simbolo "?" davanti al nome 
  del nodo vuol dire che può essere di qualsiasi colore)
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b?
      [a(x)]
      [d
        [c]
        [e]
      ]
  ]
  \end{forest}
  \end{figure}
  Togliamo 1 nero ad A e D e aggiungilo a B
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b?(x)
      [a]
      [d, red
        [c]
        [e]
      ]
  ]
  \end{forest}
  \end{figure}
  \item Non sappiamo che colore abbia B perché D è nero e assumiamo che uno dei nipoti (C) sia rosso ed E è nero.
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b??
      [a(x)]
      [d
        [c, red
          [$\alpha$]
          [$\beta$]
        ]
        [e
          [$\gamma$]
          [$\delta$]
        ]
      ]
  ]
  \end{forest}
  \end{figure}
  Si scambiano colore D e C e fai una rotazione a destra su D
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b??
      [a(x)]
      [c
        [$\alpha$]
        [d, red
          [$\beta$]
          [e
            [$\delta$]
            [$\gamma$]
          ]
        ]
      ]
  ]
  \end{forest}
  \end{figure}
  \item Se il nipote destro è rosso e il nipote sinistro è di qualsiasi colore:
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [b?
      [a(x)]
      [d
        [c?]
        [e, red]
      ]
  ]
  \end{forest}
  \end{figure}
\end{enumerate}
\noindent Proviamo ad implementare tramite codice e applicarlo sull'albero:
\begin{lstlisting}[language=Scala]
  color[d] <- color[b]
  color[b] <- black
  color[e] <- black
  rotation-sx(b)

  x <- root
\end{lstlisting}
\begin{figure}[H]
  \centering
  \begin{forest}
  for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
[d?
    [b
      [a]
      [c?]
    ]
    [e]
]
\end{forest}
\end{figure}
Abbiamo presentato quattro scenari diversi e per ciascuno di questi scenari abbiamo fatto una trasformazione 
e ci siamo assicurati ceh quest'ultima mantenga le proprietà dell'RB-Tree. Dobbiamo anche assicurarci che non ci siano nodi rossi figli di rossi.
\\
Riassiumiamo in breve questi casi prendendo come riferimento i nipoti del nodo anomalo:
\begin{itemize}
  \item C nero, E nero (caso 2)
  \item \colorbox{red!40!white}{C rosso}, E nero (caso 3)
  \item C nero, \colorbox{red!40!white}{E rosso} (caso 4)
  \item \colorbox{red!40!white}{C rosso, E rosso} (caso 4)
\end{itemize}
Tutti i casi sono esaustivi poiché abbiamo coperto ogni caso. Il caso 1 è il caso in cui D è rosso.
\begin{itemize}
  \item Il caso 3 e il caso 4 sono casi terminali perché portano al termine dell'algoritmo.
  \item Il caso 2 non termina ma porta l'anomalia in alto.
  \item Il caso 1 porta l'anomalia in \textbf{basso} e questo è un gran problema. Dopo il caso 1 non può verificarsi nuovamente il caso 1. Se è seguito da un caso 3 e 4 allora l'algoritmo termina.
  Se invece è seguito dal caso 2 nonostante il caso 2 non sia un algoritmo che termina, \textbf{l'algoritmo termina} perché l'anomalia è stata spostata in alto e B era rosso.
  Quindi anche questo algoritmo termina in tempo $O(\log n)$.
\end{itemize}

\subsubsection{Estrazione in un RB-Albero}

Se nell'albero binario posso andare a sinistra o a destra per ogni passaggio, ora sicuramente so che la complessità del mio algoritmo in un RB-Albero è $O(\log n)$.

Per sapere la posizione effettiva della mia radice, posso aggiungere alla mia struttura del nodo una proprietà \textit{size} che mi permette di sapere quanti nodi ci sono sotto al nodo x. 
Per sapere la posizione effettiva di ogni nodo, posso utilizzare la seguente formula:
\[
  size(left(x)) + 1
\]
Lo pseudocode per l'estrazione di un nodo in un RB-Albero è il seguente:
\begin{lstlisting}[language=Scala]
// X e' la lista
// i e' la posizione del nodo da estrarre
Select(x, i)
  r <- size(leftx) + 1
  if r = i
    return x
  else if(r > i)
    ret select[left[x], i]
  else 
    ret select[right[x], i - r]
\end{lstlisting}
Tuttavia una volta aver estratto il valore, devo aggiornare la proprietà \textit{size} di tutti i nodi che sono stati toccati.
\begin{examplebox}{Esempio}
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [x
      [y
        [$\alpha$]
        [$\beta$]
      ]
      [$\gamma$]
  ]
  \end{forest}
  \end{figure}
  \noindent
  Proviamo a ruotare e poi cambiare la proprietà \textit{size} in questa maniera:
  \begin{figure}[H]
    \centering
    \begin{forest}
    for tree={
    draw, % Disegna i nodi
    circle, % Forma dei nodi
    minimum size=2em, % Dimensione minima dei nodi
    inner sep=1pt, % Spaziatura interna dei nodi
    s sep=1cm, % Spaziatura verticale tra i livelli
    scale=0.8
  }
  [y
      [$\alpha$]
      [x
        [$\beta$]
        [$\gamma$]
      ]
  ]
  \end{forest}
  \end{figure}
  \noindent
  \[
  size(x) \leftarrow size(left[x]) + size(right[x]) + 1
  \]
  La size di y dopo la rotazione è esattamente la size di x prima della rotazione.
  Gli unici che cambiano la size quindi, sono x e y e la complessità di questo algoritmo
  è O(1) e quindi è costante.
\end{examplebox}

\subsubsection{Campi aggiuntivi}
Si possono aggiornare dei campi aggiuntivi all'interno della struttura dell'RB-Albero per velocizzare alcuni algoritmi (come abbiamo visto per size)
ma dobbiamo prestare attenzione a mantenere la complessità delle operazioni inalterate.
\begin{theorem}
  Sia $F$ un campo aggiuntivo, se 
  \[\exists f. \; \forall x F[x] = f(key[x], f[left[x]], f[right[x]], key[left[x]], key[right[x]])\]
  allora $F$ mantenibile in tempo logaritmico. 
  \\
  Cioè se il campio aggiuntivo è calcolabile utilizzando le operazioni definite in $f$ allora la complessità di $F$ rimane inalterata.
\end{theorem}
\textbf{Osservazione. } Se siamo in un Albero AVL possiamo aggiungere un campo aggiuntivo tuttavia il teorema appena enunciato non varrà poiché l'inserimento e la rimozione impiega tempo $O((\log n)^2)$.
Perché deve svolgere le rotazioni (che impiegano tempo logaritmico) per ogni nodo all'interno del cammino radice-foglia.

\begin{examplebox}{Esempio 1}
  Se volessi aggiungere il campo "Rank(x)" che ritorna la posizione che occupa quel nodo in un ordinamento crescente all'interno dell'albero.
  \begin{lstlisting}[language=Scala]
// x: Nodo da cui si parte per la selezione
// y: Nodo dove bisogna trovare la 
Rank(x, y) 
  count = size[left[x]] + 1
  // finche' x non e' la radice
  while(x != root)
    // Se sei figlio destro allora aggiungo la size del sottoalbero sinistro e il padre
    if (x == right[parent[x]]) 
      return count + size[left[parent[x]]] + 1
    // altrimenti x diventa il suo parent e vai sopra
    x = parent[x]
  
  return count
\end{lstlisting}

\end{examplebox}
\begin{examplebox}{Esempio 2}
  Se si volessero inserire gli elementi di un RB-Albero in un array ordinato, si potrebbe usare un algoritmo di visita in-order dell'albero. 
  Esistono diversi modi per visitare un albero:
  Lo pseudocodice è il seguente:
  \begin{lstlisting}[language=Scala]
// x: Nodo da cui si parte la visita
// i: indice dell'array in cui inserire x
// prima si scrive la radice, poi il sottoalbero sinistro e poi il sottoalbero destro
pre_visit(x, i)
  if x != null
    visit(x)
    pre_visit(left[x], i)
    pre_visit(right[x], i)
  
// la radice si trova al centro e a sinistra si trovo il sottoalbero sinistro e a destra il sottoalbero destro
in_visit(x, i) 
  if x != null
    invisit(left[x], i)
    visit(x)
    invisit(right[x], i)

post_visit(x, i)
  if x != null
    post_visit(left[x], i)
    post_visit(right[x], i)
    visit(x)
  \end{lstlisting}
\noindent
Possiamo dimostrare che la visita ha complessità $O(n)$. Per dimostrarlo possiamo usare l'equazione di ricorrenza:
\begin{align*}
  T(n) &= T(n_{left}) + T(n_{right}) + 1\\
  &= n_{left} + n_{right} + 1 = n
\end{align*}
\noindent
Supponiamo per induzione che $T(n_{left})$ e $T(n_{right})$ sono rispettivamente $n_{left}$ e $n_{right}$.
Quindi possiamo visitare i nodi degli RB-Alberi in \textbf{tempo lineare}.
\end{examplebox}
Se volessi inserire gli elementi di un RB-Albero in un array ordinato, posso usare la visita in-order dell'albero. 
Tuttavia la sua complessità è $O(n\log n)$ poiché facendo una 
sequenza di inserimento:
\[1 + \log 1 + \log 2 + \dots + \log n = \log n! =  \Theta(n \log n)\]
Ci chiediamo se possiamo costruire un RB-Albero partendo da un array in un tempo più basso di $O(n \log n)$.
Se è così, allora otteniamo un array ordinato e questo vuol dire che riesco a scrivere un algoritmo di ordinamento con complessità inferiore di $O(n \log n)$.
\begin{lstlisting}[language=Scala]
sort(A)
  T <- build_tree(A)
  B <- in_visit(T)
\end{lstlisting}
Tuttavia avevamo già dimostrato che non è possibile fare un ordinamento in tempo inferiore a $O(n \log n)$. 
Riusciamo a scrivere una funzione "buildtree" che dato un array ordinato mi costruisce un RB-Albero
\begin{lstlisting}[language=Scala]
build_tree(A)
  if A = []
    return null
  else
    m <- A.length / 2
    x <- new Node(A[m])
    left[x] <- build_tree(A[0, m-1])
    right[x] <- build_tree(A[m+1, A.length])
    return x
\end{lstlisting}
Proviamo ora a scrivere un algoritmo che costruisce un nodo e prende in input due stringhe prese dalla previsita e dalla invisita.
\begin{lstlisting}[language=Scala]
buildtree(p, i)
  if(length(i) == 0) {
    return null
  } else {
    x <- add[p[0]]
    newP <- i.indexOf(p[0])
    left(x) <- buildtree(P[1 .. newP], I[0 .. newP-1])
    right(x) <- buildtree(P[newP+1 .. p.length], I[newP+1 .. I.length])
  }
\end{lstlisting}
Prendiamo come esempio le due stringhe:
\begin{align*}
  PREvisit &= \text{[A, B, D, E, C, C, G, H, I]}\\
  INvisit &= \text{[E, D, B, F, A, G, C, H, I]}
\end{align*}
\begin{figure}[H]
  \centering
  \begin{forest}
  for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=2em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
[A, green
    [B, red
      [D, red
        [E, red]
        [, no edge, draw=none]
      ]
      [F, red]
    ]
    [C, blue
      [G, blue]
      [H, blue
        [, no edge, draw=none]
        [I, blue]
      ]
    ]
    ]
\end{forest}
\end{figure}
\noindent
Non è possibile unire due RB-Alberi in tempo logaritmico in modo deterministico perché:
\begin{itemize}
  \item L'operazione richiede spesso una manipolazione globale della struttura.
  \item Il bilanciamento degli RB-Alberi impone vincoli che possono richiedere tempo lineare rispetto alla dimensione degli alberi coinvolti.
\end{itemize}

\subsection{Heap Binomiale}

Un heap binomiale è una collezione di alberi binomiali. Un albero binomiale di dimensione $0$ è un singolo nodo.

\begin{figure}[H]
  \centering
  \begin{forest}
  for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=3em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
[]
\end{forest}
\end{figure}
\noindent
Un albero di dimensione $1$ è un albero binario con un solo figlio.
\begin{figure}[H]
  \centering
  \begin{forest}
  for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=3em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
}
[
  []
]
  \end{forest}
\end{figure}
\noindent
Un albero di dimensione $2$ diventa:
\begin{figure}[H]
\centering
\begin{forest}
for tree={
draw, % Disegna i nodi
circle, % Forma dei nodi
minimum size=3em, % Dimensione minima dei nodi
inner sep=1pt, % Spaziatura interna dei nodi
s sep=1cm, % Spaziatura verticale tra i livelli
scale=0.8
}
[
  [
    []
  ]
  []
]
\end{forest}
\end{figure}
\noindent
Un albero di dimensione $3$ diventa:
\begin{figure}[H]
  \centering
  \begin{forest}
  for tree={
  draw, % Disegna i nodi
  circle, % Forma dei nodi
  minimum size=3em, % Dimensione minima dei nodi
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=1cm, % Spaziatura verticale tra i livelli
  scale=0.8
  }
  [
    [
      [
        []
      ]
      []
    ]
    [
      []
    ]
    []
  ]
  \end{forest}
  \end{figure}
  
Dato un albero di dimensione $i$ posso costruire un albero di dimensione $i+1$ in tempo costante. Questi alberi hanno una serie di proprietà:
\begin{lemma}
  In un albero binomiale di dimensione $k$: 
  \begin{itemize}
    \item ci sono $2^k$ nodi.
    \item L'altezza è $k$.
    \item A profondità $i$ ci sono $\binom{k}{i}$ nodi.
    \item I figli della radice da sx a dx sono radici di alberi binomiali di dimensione $k-1, k-2, \dots, 0$.
  \end{itemize}
\end{lemma}
\noindent
Cerchiamo di dimostrare le proprietà che abbiamo appena elencato:
\begin{proof}
  \textbf{Dimostrazione per induzione: Ci sono $2^k$ nodi}: La proprietà è valida per gli alberi di dimensione 0 ($B_0$) perché ha $2^0 = 1$ nodo.
  supponiamo per induzione che in un albero di dimensione $k$ ($B_k$) ci siano $2^k$ nodi. Dimostriamo che nell'albero di dimensione $k+1$ ($B_{k+1}) $
  formato da due alberi $B_k$ sono dunque presenti:
  \[2^k + 2^k = 2 \cdot 2^k = 2^{k+1}\]
\end{proof}

\begin{proof}
  \textbf{L'altezza è $k+1$}:
  Supponiamo per induzione che in albero di dimensione $k$ ci siano $2^k$ nodi.
  Come è fatto un albero di dimensione $k+1$?
  \noindent
  L'albero di dimensione $k+1$ è formato da un albero di dimensione $k$ e un albero di dimensione $k$.
  Quindi ci sono $2 \cdot 2^k = 2^{k+1}$ nodi.
  L'altezza è $k+1$.
  A profondità $i$ ci sono $\binom{k+1}{i}$ nodi.
  I figli della radice da sx a dx sono radici di alberi binomiali di dimensione $k, k-1, \dots, 0$.
\end{proof}

\begin{proof}
  \textbf{I figli dellla radice da sx a dx sono radici di alberi binomiali di dimensione $k-1, k-2, \dots, 0$}: 
  Per $k=0$ è vero.
  Dato che l'altezza è $k+1$, il figlio sinistro della radice è un albero binomiale di dimensione $k$ e il figlio destro è un albero binomiale di dimensione $k-1$.
\end{proof}

\begin{proof}
  \textbf{A profondità $i$ ci sono $\binom{k}{i}$ nodi}: 
  Supponiamo per induzione che in albero di dimensione $k$ ci siano $\binom{k}{i}$ nodi.
  Come è fatto un albero di dimensione $k+1$?
  \noindent
  L'albero di dimensione $k+1$ è formato da un albero di dimensione $k$ e un albero di dimensione $k$.
  Quindi ci sono $\binom{k}{i} + \binom{k}{i-1}$ nodi.
  Quindi, tramite l'identità di Pascal, otteniamo $\binom{k+1}{i}$ nodi.
  L'identità di Pascal dice che \[\binom{k}{i} + \binom{k}{i-1} = \binom{k+1}{i}\]
  A profondità $i$ ci sono $\binom{k+1}{i}$ nodi.
  I figli della radice da sx a dx sono radici di alberi binomiali di dimensione $k, k-1, \dots, 0$.
\end{proof}
\noindent
Quindi riassumendo: \textit{La Heap Binomiale è una lista di alberi binomiali} dove
\begin{itemize}
  \item I contenuti dei nodi sono oggetti su cui è definita una relazione di ordinamento
  \item Per ogni di dimensione c'è al più un albero binomiale
  \item I vari nodi soddisfano la proprietà di heap (chiave[x] $\leq$ chiave[children[x]])
\end{itemize}
\begin{examplebox}{Esempio 1}
  La complessità di trovare il minimo all'interno di questo Heap Binomiale si basa sul numero di radici. Dove $n$ è il numero di nodi:
  \[k > \log_2 n\]
  \[2^k > 2^{\log_2 n} = n\]
  Quindi la complessità è $O(\log n)$, anzi $\Omega(\log n)$ perché devo scorrere tutti gli alberi.
\end{examplebox}

\begin{examplebox}{Esempio 2}
  Se volessi costruire uno Heap Binomiale con $n$ nodi fissato, è possibile perché essendo ogni albero rappresentabile come $2^k$ 
  posso scomporre il valore $n$ come somma di potenze di due.
  \[n = 2^{k_1} + 2^{k_2} + \dots + 2^{k_m}\]
  Per esempio:
  \[37 = 2^5 + 2^2 + 2^0\]
  Quindi posso costruire uno Heap Binomiale con qualsiasi $n$ nodi.
\end{examplebox}

\subsubsection{Unione di Heap Binomiali}

Supponiamo di voler fare l'unione di due heap binomiali $H_1$ e $H_2$. 

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={circle, draw, minimum size=7mm, inner sep=0pt}, 
    sibling distance=1.5cm, edge from parent/.style={draw, -latex}, level distance=1.2cm]
  
  % Primo Heap Binomiale
  \node[draw=blue] (A) {10}
      child {node[draw=blue] (B) {20}
          child {node[draw=blue] (C) {40}}
          child {node[draw=blue] (D) {50}}
      }
      child {node[draw=blue] (E) {30}};
  
  % Secondo Heap Binomiale
  \node[draw=red, right=6cm of A] (F) {15}
      child {node[draw=red] (G) {25}
          child {node[draw=red] (H) {45}}
      }
      child {node[draw=red] (I) {35}};
  
  % Heap Unito
  \node[draw=green, below=5.5cm of A, xshift=3cm] (J) {10}
      child {node[draw=green] (K) {15}
          child {node[draw=green] (L) {25}
              child {node[draw=green] (M) {45}}
          }
          child {node[draw=green] (N) {35}}
      }
      child {node[draw=green] (O) {20}
          child {node[draw=green] (Q) {50}}
      }
      child {node[draw=green] (R) {30}};
  \node[draw=none, below=1cm of J] {Heap Binomiale Unito};
  
  % Linee orizzontali per radici
  \draw[dashed, gray] (-1,0.8) -- (7,0.8);
  \draw[dashed, gray] (-1,-4.2) -- (7,-4.2);
  
  % Frecce di fusione
  \draw[->, thick] (A.south) .. controls +(down:2cm) and +(up:2cm) .. (J.north west);
  \draw[->, thick] (F.south) .. controls +(down:2cm) and +(up:2cm) .. (J.north east);
  
  \end{tikzpicture}
\end{figure}


Massimo di iterazione massimo $O(\log n)$ e quindi la complessità è $O(\log n)$.
\\
L'inserimento di un nodo non è altro che l'unione di un nodo di dimensione $0$ con l'heap binomiale a cui vogliamo aggiungere il nodo.

\subsubsection{Rimozione di un nodo}
La rimozione di un nodo in uno heap binomiale è un'operazione che può essere eseguita in tempo 
$O(logn)$. Supponendo di voler rimuovere il nodo con la chiave più piccola (che corrisponde sempre alla radice dell'heap), il processo si articola nei seguenti passi:

\begin{itemize}
  \item Identificazione della radice con chiave minima:
La radice con chiave minima può essere trovata scorrendo l'elenco delle radici degli alberi binomiali. Questo richiede tempo 

$O(logn)$, dato che il numero massimo di alberi binomiali in uno heap è proporzionale a 
logn.

\item Rimozione della radice:
Una volta identificata la radice minima, essa viene rimossa. A seguito della rimozione, i sottoalberi figli della radice vengono "scompattati". Ciascuno di questi sottoalberi è un albero binomiale separato, che può essere gestito individualmente.

\item Ricostruzione dello heap:
Per ripristinare la struttura dello heap binomiale, i sottoalberi risultanti vengono uniti tra loro e con il resto dello heap. L'operazione di unione avviene combinando alberi di gradi uguali, procedendo come nell'algoritmo di fusione. Anche questa operazione richiede 
$O(logn)$.

\end{itemize}
In sintesi, l'operazione di rimozione implica un'iterazione sugli alberi binomiali (per identificare la radice minima), seguita dalla fusione dei sottoalberi generati. Entrambe le fasi hanno complessità 
$O(logn)$, rendendo l'intero processo efficiente.



\subsubsection{Diminuzione di una chiave e rimozione di un elemento arbitrario}

Per diminuire la chiave di un nodo in uno heap binomiale, è necessario prestare attenzione a non violare la proprietà di heap. Se la chiave diminuita è minore della chiave del padre, occorre scambiare il nodo con il padre e ripetere questa operazione (nota come \textit{risalita}) fino a quando la proprietà di heap non è più violata. La complessità di questa operazione è \( O(\log n) \), poiché la profondità massima di un albero binomiale è \( O(\log n) \).
\\
La diminuzione della chiave è utile in molte applicazioni, come negli algoritmi di ottimizzazione. Tuttavia, ci sono operazioni più potenti che potrebbero comportare una complessità maggiore.
\\
Per \textbf{rimuovere un nodo arbitrario}, possiamo sfruttare l'operazione di diminuzione della chiave. Riducendo la chiave del nodo a \(-\infty\), il nodo verrà spinto alla radice, rendendo la sua rimozione equivalente a quella della radice, che richiede \( O(\log n) \) tempo.
\\
Quindi, con uno heap binomiale, è possibile eseguire le seguenti operazioni in tempo \( O(\log n) \):

\begin{itemize}
  \item Inserire un nodo
  \item Trovare il minimo
  \item Unire due heap binomiali
  \item Rimuovere la radice
  \item Diminuire una chiave
  \item Rimuovere un nodo arbitrario
\end{itemize}

Tuttavia, alcune operazioni non possono essere eseguite in tempo \( O(\log n) \):

\begin{itemize}
  \item Trovare il massimo
  \item Cercare un elemento arbitrario
\end{itemize}


\subsubsection{Trovare il mediano}

Poiché l'array di partenza non è ordinato, trovare il mediano richiede generalmente un'operazione di tempo lineare, ovvero \(O(n)\). Tuttavia, è possibile ottimizzare questo processo trasformando l'array in un heap binomiale. L'heap binomiale, sebbene non permetta l'accesso diretto al mediano, offre una struttura che può facilitare il processo.
\\
Per trovare il mediano utilizzando un heap binomiale, è necessario seguire alcuni passaggi aggiuntivi. In particolare, dopo aver costruito l'heap binomiale, si può procedere ad estrarre gli elementi in ordine crescente (o decrescente), fino a raggiungere l'elemento centrale. Sebbene l'heap binomiale non fornisca un accesso diretto alla posizione del mediano, l'ordinamento parziale che essa garantisce può ridurre il numero di operazioni necessarie rispetto a un ordinamento completo dell'array.
\\
In alternativa, un approccio come l'algoritmo di selezione del k-esimo elemento potrebbe essere utilizzato in combinazione con l'heap binomiale per trovare il mediano in modo più efficiente. 

\subsubsection{RB-Alberi e Search Closest}

Se in un RB-Albero non dobbiamo cercare la chiave precisa ma la chiave più vicina dobbiamo cercare di utilizzare l'RB Albero
in modo tale da poter usare la potenza del suo ordinamento.
Come chiave di ordinamento uso la chiave x-k ovvera la differenza tra le due misure.

\begin{lstlisting}[language = Scala]
// Primo candidato
// k valore a cui voglio avvicinarmi
// BestCandidate e' un ulteriore candidato
searchClosest(x,k) 
  if x == null
      return null
  else if(key[x] > k)
      bestCandidate = searchClosest(right[x], k)
  else if (key[x] < k)
      bestCandidate = searchClosest(left[x], k)

  if bestCandidate == null ||  (key[x] - k) < (bestCandidate - k) 
    return x
  else 
    return bestCandidate
\end{lstlisting}
Questo algoritmo ha complessità $O(\log n)$ poiché dobbiamo comunque passare per il cammino radice foglia.

\begin{examplebox}{Esempio}
Ora immaginiamo questo problema: Siamo dei venditori di macchine e conteniamo i dati delle macchine all'interno 
di un record dove sono contenuti informazioni come: prezzo di vendita, data di vendita, ecc. Vogliamo trovare la somma dei prezzi delle macchine che abbiamo venduti fino ad una certa data.
Per costruire questo tipo di albero possiamo servirci della possibilità di aggiungere campi aggiuntivi mantenibili in tempo logaritmico.
\begin{lstlisting}[language = Scala]
SearchDate(x, k)
  if(x == null) 
    return null
  else if (date[x] <= k) 
    bestDate = SearchDate(right[x], k)
    return best ?? x //equivalente a scrivere -> best != null ? best : x
  else if (date[x] > k) 
    bestDate = SearchDate(left[x], k)
    return best ?? x
\end{lstlisting}
Quindi ora abbiamo trovato il candidato migliore della data a cui possiamo applicare il rango per trovare la somma dei prezzi delle macchine vendute fino a quella data.

\end{examplebox}

\subsection{Strutture dati facilmente partizionabili}

Vogliamo una struttura dati che sia formata da insiemi disgiunti di oggetti e possega le seguenti operazioni:
\begin{itemize}
  \item \texttt{MakeSet(x)}: Crea un insieme contenente l'oggetto x.
  \item \texttt{Union(x, y)}: Unisce gli insiemi contenenti x e y.
  \item \texttt{FindSet(x)}: Restituisce l'insieme contenente x. Cerca quindi il \textbf{rappresentante} dell'insieme a cui appartiene x.
  Quindi quando voglio capire se due oggetti appartengono allo stesso insieme posso controllare se il risultato di \texttt{FindSet(x)} e \texttt{FindSet(y)} sono uguali.
\end{itemize}
posso organizzare gli oggetti nelle liste concatenate in cui il rappresentante è il primo oggetto
della lista. 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
    every node/.style={draw,minimum size=0.7cm,node distance = 0cm},
    ]
    \node (a) {a};
    \node[above=of a] (at) {};
    \node[below=of a] (ab) {};

    \node[right=1cm of a] (b) {b};
    \node[above=of b] (bt) {};
    \node[below=of b] (bb) {/};
    
    \draw[->] (at.center) -- (a.north);
    \draw[->] (ab.east) -- ++(0.5,0) |- (b.west);
    \draw[->] (bt) -- ++(0,0.8) -| (at);
  \end{tikzpicture}
  \caption{Esempio di una lista concatenata}
\end{figure}
\noindent
in questo modo \texttt{MakeSet(x)} e \texttt{FindSet(x) }hanno complessità costante, ma la \texttt{Union(x,y)}
avrebbe complessità lineare perché devo capire se i due oggetti hanno lo stesso rappresentante, poi scorrere uno dei due insiemi fino alla fine e poi cambiare
il puntatore dell'insieme aggiunto al nuovo rappresentante per risolvere il primo problema che causa linearità potrei aggiungere al rappresentante il puntatore all'ultimo elemento.
\subsubsection{Ottimizzazione della complessità con alberi e unione per rango}
Se vengono effettuate \( m \) operazioni di cui \( n \) sono \texttt{make\_set} (cioè il numero di oggetti, di conseguenza il numero di union possibili),
la complessità nel caso pessimo è \( O(m*n) \). Di tutte queste operazioni, le
\texttt{union} saranno al massimo \( n-1 \) perchè non si possono unire più di
\( n \) insiemi. Tutte le rimanenti saranno operazioni di tempo costante, di
conseguenza un altro limite superiore corretto sarebbe \( O(m+n^2) \). Un limite
superiore migliore si può ottenere calcolando la complessità media di tutte le operazioni
e quindi la complessità finale diventa: \( O(\frac{m+n^2}{m}) = O(1 + \frac{n^2}{m}) \).

\vspace{1em}
\noindent
Per capire se questa complessità è accurata bisogna trovare un insieme di operazioni
da fare che portino ad avere quella complessità.

\begin{enumerate}
  \item Si fanno \( n \) \texttt{make\_set} creando \( n \) insiemi
  \item Si uniscono le coppie di insiemi. Per la prima coppia il costo è \( 1 \),
    per la seconda \( 2 \) e così via. Il costo totale è:
    \[
      1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2} = \Theta(n^2)
    \] 
\end{enumerate}
Per fare meglio di \( \Theta (n ^2) \) bisogna ottimizzare la union, cioè l'aggiornamento
di tutti i puntatori. Si potrebbe unire l'insieme più piccolo a quello più grande,
al posto di unire quello più grande a quello più piccolo e a questo punto bisogna cambiare
soltanto un puntatore, nel caso in cui l'elemento più piccolo abbia un elemento.
Questo metodo è chiamato \textbf{unione per rango}.
Il numero massimo di volte che si può modificare il puntatore al rappresentante dopo
questa tecnica diventa \( \log_2 n \) perchè:
\begin{enumerate}
  \item Se ad un insieme di un elemento viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà minimo 2 elementi:
    \[
    \ge 2
    \] 
  \item Se ad un insieme di due elementi viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà al minimo 4 elementi (perchè sappiamo
    che bisogna unire l'insieme più piccolo ad un altro insieme, di conseguenza se
    sappiamo che l'insieme di due elementi è il più piccolo, l'altro insieme avrà
    al massimo 2 elementi):
    \[
    \ge 4 = 2^2
    \] 

  \item ...

  \item Se ad un insieme di \( i \) elementi viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà al minimo \( 2^i \) elementi:
    \[
    \ge 2^i
    \]
\end{enumerate}
Questa costruzione si chiama \textbf{iterative squaring}.
\noindent
La complessità con l'unione per rango diventa:
\[
\frac{m + n \log n}{m} = 1 + \frac{n \log n}{m} \le  1 + \log n
\] 

\vspace{1em}
\noindent
Per fare meglio si può rappresentare in un modo alternativo gli insiemi, quindi con
degli alberi.
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [a
     [b
       [d]
       [,draw=none,no edge]
     ]
     [c
       [,draw=none,no edge]
       [e]
     ]
    ]
  \end{forest}
  \hspace{1cm}
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [f
     [g
      [h]
     ]
    ]
  \end{forest}
  \caption{Esempio di alberi come insiemi}
\end{figure}
\begin{itemize}
  \item \texttt{make\_set(x)}: si crea un albero con un solo nodo
    \[
      O(1)
    \] 

  \item \texttt{find\_set(x)}: si risale l'albero fino alla radice
    \[
      O(n)
    \]

  \item \texttt{union(x,y)}: si fa diventare la radice di un albero il figlio dell'altra
    \[
      O(n)
    \]
\end{itemize}

\vspace{1em}
\noindent
Come prima si può usare l'unione per rango per migliorare la complessità. Ogni volta
che un nodo aumenta di profondità (grazie all'unione ad un altro albero), il numero
di nodi raddoppia. Facendo l'unione per rango sappiamo che la complessità della ricerca
del rappresentante è \( O(\log n) \) e la complessità dell'unione è \( O(\log n) \).

\vspace{1em}
\noindent
Siccome la \texttt{find\_set} è \( O(\log n) \) siamo comunque peggio di prima, quindi
si può far puntare ogni nodo direttamente al rappresentante, ma questo viene fatto
soltanto quando viene richiamata la \texttt{find\_set}, im modo da non farlo più alle
chiamate successive. L'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
find_set(x):
  if parent(x) == x
    return x
  else
    parent(x) = find_set(parent(x))
    return parent(x)
\end{lstlisting}
oppure più compatto:
\begin{lstlisting}[language=Scala]
find_set(x):
  if parent(x) != x
    x <- find_set(parent(x))
  return parent(x)
\end{lstlisting}
Man mano che vengono eseguite le \texttt{find\_set} la struttura si comprime 
(\textbf{tecnica di compressione dei cammini}):
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [f
      [a]
      [b]
      [c]
      [...]
      [g]
      [h]
    ]
  \end{forest}
\end{figure}
\noindent
Di conseguenza la complessità della \texttt{find\_set} diventa costante.

\vspace{1em}
\noindent
La nuova complessità sapendo di avere \( m \) operazioni di cui \( n \) \texttt{make\_set}
diventa:
\[
O(m \alpha(m,n))
\] 
dove \( \alpha \) è l'inversa della funzione di Ackermann:
\[
  \alpha(m,n) = min \left\{ i \ge 1 , \left| A \left( i, \left\lfloor \frac{m}{n} \right\rfloor > \log n \right)  \right.  \right\}
\] 
\[
  A(i,1) = 2^{2^{2^{2^{{\vdots i}^{2}}}}}
\] 
Siccome la funzione di Ackermann cresce molto velocemente, la sua inversa
cresce molto lentamente. Di conseguenza non si riuscirà mai a superare una certa
costante, quindi la complessità diventa costante.

\subsection{Il concetto di funzione calcolabile}

Le funzioni descritte in maniera \textbf{costruttiva} non sono la totalità delle funzioni
esistenti perché esse sono maggiori della cardinalità stessa dei naturali (Infinito non numerabile).
I fratelli maggiori Robison partendo da assiomi di Peano:
\begin{itemize}
  \item $0$ è un numero naturale
  \item $\forall x \in \mathbb{N}, \; x=x$
  \item $x+0=x$
  \item $x+(y+1)=(x+y)+1$
\end{itemize}
hanno dimostrato che l'insieme delle funzioni costruttive sono ricorsive primitive.
Ackermann aveva capito che la seguente struttura confutava ciò che i Robinson avevano dimostrato:
\begin{align*}
  x &\ast 1 = x\\
  x &\ast (y+1) = x + x \ast y
\end{align*}
oppure
\begin{align*}
  x^1 &= x\\
  x^{y+1} &= x \cdot x^y
\end{align*}
Questo dimostrava come ci fossero funzioni che non erano ricorsive primitive.
\begin{lstlisting}[language=Scala]
// Funzione di Ackermann in Pseudocodice
A(x, y) 
  if x == 0
    return y + 1
  else if y == 0
    return A(x-1, 1)
  else
    return A(x-1, A(x, y-1))
\end{lstlisting}

\subsubsection{Esempio di mantenimento di un campo in un RB-Albero}

\begin{examplebox}{Esempio}
  Immaginiamo di gestire un agenda in cui poter inserire e togliere appuntamenti con inizio e fine, poi anche
  una funzione che preso un intervallo di tempo mi restituisce un appuntamento in cui andrebbe in collisione.

  Usiamo gli RB-Alberi in cui i nodi rappresentano gli appuntamenti. In ogni nodo $x$ teniamo il campo $max[x]$ che mantiene
  il massimo del tempo finale tra tutti i nodi radicati in $x$. Questo campo è mantenibile in tempo logaritmico.

  \begin{lstlisting}[language=Scala]
search(x,i)
  if x == nil || i in x
    return x
  else if left[x] != nil && max[left[x]] > min[i]
    return search(left[x], i)
  else
    return search(right[x], i)
  \end{lstlisting}
\end{examplebox}


\section{Tecniche di programmazione}
Fin'ora abbiamo utilizzato la tecnica del \textbf{divide et impera}, cioè dividere
il problema in parti più piccole e risolverle con lo stesso algoritmo per poi unirle
per ottenere il risultato. Questa non è l'unica tecnica di programmazione esistente,
ce ne sono molte altre, ad esempio la \textbf{programmazione greedy}, cioè prendere
le decisioni il prima possibile e sperare che siano le migliori, oppure la
\textbf{programmazione dinamica}, cioè creare un'infrastruttura prima di poter 
prendere una decisione.

\subsection{Programmazione dinamica}
Prendiamo ad esempio il problema della \textbf{moltiplicazione di matrici}.
Sappiamo che se abbiamo due matrici \( A \) di dimensione \( n \times m \) e \( B \)
di dimensione \( m \times l \), il prodotto tra le due matrici avrà complessità
\( \Theta(nml) \).

Se volessimo moltiplicare 3 matrici: \( A_1 \cdot A_2 \cdot A_3 \) di dimensione:
\[
\begin{aligned}
  A_1 & : 10 \times 100\\
  A_2 & : 100 \times 5\\
  A_3 & : 5 \times 50
\end{aligned}
\] 
possiamo sfruttare la proprietà associativa:
\[
  (A_1 \cdot A_2) \cdot A_3 = A_1 \cdot (A_2 \cdot A_3)
\] 
Solo che il numero di operazioni nei due casi è diverso, quindi per rendere minimo
il numero di operazioni è più conveniente fare: \( (A_1 \cdot A_2) \cdot A_3 \) perchè:
\[
\begin{aligned}
  (A_1 \cdot A_2) \cdot A_3 &\quad 10 \times 100 \times 5 + 10 \times 5 \times 50 = 5000 + 2500 = 7500\\
  A_1 \cdot (A_2 \cdot A_3) &\quad 100 \times 5 \times 50 + 10 \times 100 \times 50 = 25000 + 50000 = 75000
\end{aligned}
\] 

\vspace{1em}
\noindent
Supponiamo di avere un insieme di matrici \( A_1, A_2, \ldots, A_n \), vogliamo trovare
la \textbf{parentesizzazione} ottimale per minimizzare il numero di operazioni.

\vspace{1em}
\noindent
Un modo per affrontare il problema è quello di provare tutte le combinazioni
di parentesi e vedere quale è la migliore.

C'è per forza una moltiplicazione che dovrà essere eseguita per ultima, quindi
distinguiamo \( k \) come il punto in cui verrà fatta l'ultima moltiplicazione.
\[
  (A_1 \cdots A_k) \cdot (A_{k+1} \cdots A_n)
\] 
Il numero modi per moltiplicare le matrici è dato dal numero di modi per moltiplicare
le matrici \( A_1 \dots A_k \) e le matrici \( A_{k+1} \dots A_n \).
\[
  P(n) = \sum_{k=1}^{n-1} P(k) + P(n-k) \quad \in \Omega \left( \frac{4^n}{n^{\frac{3}{2}}} \right) 
\] 
La complessità è troppo alta, quindi provare tutte le combinazioni possibili non è
efficace.

\vspace{1em}
\noindent
Supponiamo che qualcuno ci dica il valore ottimo di \( k \), allora è sicuro che
il modo in cui vengono moltiplicate le matrici \( (A_1 \cdots A_k) \) e 
\( (A_{k+1} \cdots A_n) \) è ottimale:
\[
  \underbrace{\left( A_1 \cdots A_k \right)}_{\text{Ottimo}} \cdot 
  \underbrace{\left( A_{k+1} \cdots A_n \right)}_{\text{Ottimo}}
\] 
Questa tecnica si chiama \textbf{Sottostruttura ottimale}, un problema si può dividere in
problemi della stessa natura, ma più piccoli.

\vspace{1em}
\noindent
Il numero di moltiplicazioni effettuate è dato da:
\[
  N\left( A_1 \cdots A_n \right) =
\] 
\[
  \begin{aligned}
         &= N\left( A_1 \cdots A_k \right) +
         N\left( A_{k+1} \cdots A_n \right) +
         \text{Costo ultima moltiplcazione}
         \\
         &= N\left( A_1 \cdots A_k \right) +
         N\left( A_{k+1} \cdots A_n \right) +
         rows(A_1) \cdot cols(A_k) \cdot cols(A_n)
  \end{aligned}
\] 
Se \( k \) è ottimo, allora \( N\left( A_1 \cdots A_k \right) \) e
\( N\left( A_{k+1} \cdots A_n \right) \) sono ottimi perchè il risultato
viene sommato e se non fossero ottimi il risultato finale non sarebbe ottimo.
Il problema è che non sappiamo quale sia il valore di \( k \) ottimo.

\vspace{1em}
\noindent
Un possibile algoritmo per trovare \( k \) è il seguente:
\begin{lstlisting}[language=Scala]
// P e' un vettore che descrive le matrici come:
//   - P[0] = rows(A1)
//   - P[i] = cols(Ai)
// Quindi Ai ha dimensione P[i-1] x P[i]
// i e' l'indice di partenza
// j e' l'indice di fine
matrix_chain_order(P,i,j)
  if i == j
    return 0

  m <- +inf
  for k <- i to j-1
    m <- min(m,
      matrix_chain_order(P,i,k) +
      matrix_chain_order(P,k+1,j) +
      P[i-1]*P[k]*P[j])
  return m
\end{lstlisting}
Questo algoritmo ha una complessità esponenziale.

\vspace{1em}
\noindent
Cerchiamo di migliorare l'algoritmo, implementando l'\textbf{albero di ricorrenza}, cioè
un alberi i cui nodi rappresentano le chiamate ricorsive dell'algoritmo, identificate
dai parametri \( i \) e \( j \).
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      inner sep=1pt,
      s sep=0.5cm,
    }
    [{(1,n)}
      [{(1,1)(2,n)}
        [{(1,1)(1,1)}]
        [{(2,2)\color{red}(3,n)}
          [{(2,2)(2,2)}]
          [{(3,3)\color{blue}(4,n)}]
        ]
      ]
      [{(1,2)\color{red}(3,n)}
        [{(1,1)(2,2)}]
        [{(3,3)\color{blue}(4,n)}]
      ]
      [{(1,3)\color{blue}(4,n)}]
      [{...}]
      [{(1,n-1)(n,n)}]
    ]
  \end{forest}
  \caption{Albero di ricorrenza}
\end{figure}
\noindent
Notiamo che ci sono molte chiamate ricorsive ripetute, ad esempio \( \color{blue}(4,n) \),
e questo aumenta la complessità dell'algoritmo inserendo istanze del problema che abbiamo
già affrontato. Al massimo ci saranno \( n^2 \) istanze di un problema.

Anzichè risolvere il problema ogni volta che lo incontriamo, possiamo risolvere ogni
singolo problema una sola volta e salvare il risultato:
\begin{lstlisting}[language=Scala]
matrix_chain_order(P)
  n <- length(p) - 1
  for i <- 1 to n // n
    // La moltiplicazione di una sola matrice costa 0 perche' non c'e' nulla da moltiplicare
    M[i,i] <- 0 // Matrice che salva tutte le soluzioni (i,j)
  
  for l <- 2 to n //           n --* Problemi composti da n matrici
    for i <- 1 to n - l + 1 // n --*
      j <- i + l - 1 //            | n^3
      M[i,j] <- +inf //            |
      for k <- i to j - 1 //   n --*
        M[i,j] <- min(M[i,j],
          M[i,k] + M[k+1,j] + P[i-1]*P[k]*P[j])

  return m[1,n]
\end{lstlisting}
Questo ci dice quante moltiplicazioni sono necessarie per moltiplicare tutte le matrici,
però per sapere la posizione dell'ultima moltiplicazione da fare invece si può
riscrivere l'algoritmo precedente come:
\begin{lstlisting}[language=Scala]
matrix_chain_order(P)
  ...
      for k <- i to j - 1
        m <- M[i,k] + M[k+1,j] + P[i-1]*P[k]*P[j]
        if m < M[i,j]
          M[i,j] <- m // Numero moltiplicazioni
          S[i,j] <- k // Indice dell'ultima moltiplicazione
  ...
\end{lstlisting}
La complessità di questo algoritmo è \( O(n^3) \).

\vspace{1em}
\noindent
L'algoritmo precedente scritto in modo iterativo si potrebbe anche scrivere ricorsivamente
e l'idea è che prima di calcolare il risultato si controlla se è già stato calcolato
e si utilizza quello, altrimenti si calcola e si salva il risultato.
\begin{lstlisting}[language=Scala]
matrix_chain_order(P)
  for i <- 1 to n
    for j <- 1 to n
      M[i,j] <- +inf

  matrix_chain_order_aux(P,1,n)


matrix_chain_order(P,i,j)
  if M[i,j] != +inf
    return M[i,j]
  else
    if i == j
      M[i,j] <- 0
    else
      m <- +inf
      for k <- i to j-1
        m <- min(m,
          matrix_chain_order(P,i,k) +
          matrix_chain_order(P,k+1,j) +
          P[i-1]*P[k]*P[j])
      M[i,j] <- m

    return M[i,j]
\end{lstlisting}
Per ogni istanza del problema si calcola una sola volta il risultato, quindi la complessità
è \( O(n^3) \).

\vspace{1em}
\noindent
\begin{definition}
  La tecnica di memorizzare i risultati già calcolati si chiama \textbf{memoizzazione}.
\end{definition}

\vspace{1em}
\noindent
L'algoritmo che data la matrice S, che contiene gli indici delle parentesizzazioni
ottimal e la matrice A, che contiene le matrici da moltiplicare, restituisce la
moltiplicazione ottimale è il seguente:
\begin{lstlisting}[language=Scala]
optimal_multiply(S, A, i, j)
  if i == j
    return A[i]

  return optimal_multiply(S, A, i, S[i,j]) *
         optimal_multiply(S, A, S[i,j] + 1, j)
\end{lstlisting}

\subsection{Programmazione Greedy}

La tecnica di programmazione Greedy, o in italiano \enquote{avida} è una tecnica di programmazione simile a quella dinamica 
ma che non garantisce la soluzione ottimale. L'idea è quella di \textit{prendere la decisione che sembra la migliore in quel momento}, senza 
pensare alle conseguenze future. Questo approccio è molto più veloce rispetto alla programmazione dinamica, ma non sempre garantisce
la soluzione ottimale. A volte ci troveremo davanti a problemi non risolvibili con la programmazione greedy, ma in molti casi
è possibile trovare una soluzione tramite la programmazione dinamica. 
\begin{examplebox}{Selezione di Attività}
  Prendiamo come problema esempio il seguente:
  $S = \{1, ... , n\}$ che è un insieme di attività. Ogni attività ha un tempo di inizio $S_i$ e un tempo di fine $F_i$. Due attività sono in conflitto
  se i loro intervalli hanno un intersezione non vuota. Il mio obiettivo è quello di selezionare un sottoinsieme di attività non in conflitto di cardinalità 
  massima. Scriviamo la lista delle attività con i loro intervalli:
\begin{center}
  

\begin{tabular}{c|c|c}
  N & S & F\\
  \hline
  1 & 3 & 8\\
  2 & 8 & 12\\
  3 & 1 & 4\\
  4 & 2 & 13\\
  5 & 3 & 5\\
  6 & 0 & 6\\
  7 & 5 & 9\\
  8 & 12 & 14\\
  9 & 6 & 10\\
  10 & 5 & 7\\
  11 & 8 & 11
\end{tabular}
\end{center}
\noindent
Un esempio di attività compatabili di cardinalità tre sono la 3 la 7 e la 8, oppure 3, 10, 11. E di cardinalità 4?
Abbiamo la 3, 10, 11 e la 8. E invece cinque? Forse non esistono, ed è difficile da mostrare con una grande quantità di dati.
Proviamo ad ordinare la tabella secondo i tempi di fine delle attività (reinserendo anche il numero delle attività):
\begin{center}
\begin{tabular}{c|c|c}
  N & S & F\\
  \hline
  1 & 1 & 4\\
  2 & 3 & 5\\
  3 & 0 & 6\\
  4 & 5 & 7\\
  5 & 3 & 8\\
  6 & 5 & 9\\
  7 & 6 & 10\\
  8 & 8 & 11\\
  9 & 8 & 12\\
  10 & 2 & 13\\
  11 & 12 & 14
\end{tabular}
\end{center}

\begin{lemma}
  Esiste una soluzione che contiene l'attività uno.
\end{lemma}
Selezionando la prima attività sono sicuro di trovare una soluzione con questa attività. Ora posso eliminare tutte le attività che sono in conflitto con la prima.
Questo lemma ci permette di risolvere il problema ricorsivamente, riapplicando la stessa procedura sulle attività rimanenti e quindi ad un sottoproblema della stessa
natura ma più piccolo. Alla fine dell'algoritmo ottengo il mio insieme di attività compatibili. 
Questo algoritmo è considerato \textbf{greedy} perché \textit{ho selezionato un'attività senza risolvere i sottoproblemi che ne derivano e spero che la mia scelta sia la migliore}.
\begin{proof}
  Sia $A$ una soluzione al problema, cioè un insieme compatibile di cardinalità massima. 
  Se l'attività $1 \in A$ abbiamo finito. Altrimenti sia $k \in A$ l'attività con tempo di $A$ che termina per prima.
  \\
  Allora \[A' = (A - \{k\}) \cup {1}\] è una soluzione. Come faccio a dire che l'attività $1$ è compatibile con tutte le altre attività?
  Questo perché:
  \[f_1 < f_k\] 
  Sia $i \in A - \{k\}$ allora sicuramente $S_i$ è maggiore di $f_k$ e quindi sicuramente anche $f_1 < f_k$.
  Quindi sappiamo che in qualsiasi soluzione ottimale l'attività $1$ è presente.
  Tuttavia nel caso pessimo questo algoritmo ha complessità $O(n^2)$, perché devo eliminare tutte le attività in conflitto con l'attività selezionata.
  \begin{lstlisting}[language=Scala]
greedy_activity_selector(s,f)
ordina s,f per f decrescente
n <- length(s)
A <- {1}
j <- 1
for i <- 2 to n 
    if s[i] >= f[j]
        A <- A U {i}
        j <- i
return A
  \end{lstlisting}
Questo algoritmo ha complessità $O(n\log n)$ questo a causa dell'ordinamento presente nell'algoritmo.
\end{proof}
\end{examplebox}
\noindent
Un altro tipo di problema a cui ci possiamo ricondurre è il seguente: immaginiamoci dei ladri
che possiedono uno zaino di capienza limitata. Ogni oggetto ha un peso e un valore, e il ladro 
deve scegliere quali oggetti mettere nello zaino in modo da massimizzare il valore totale degli oggetti.
Questo problema è chiamato \textbf{problema dello zaino} e può essere risolto con la programmazione greedy.
Questo perché bisogna per forza risolvere tutte le sottoistanze del problema per poter risolvere il problema principale in maniera ottimale.
Questo problema infatti fa parte della classe di problemi \textbf{NP-completi}, cioè problemi per i quali non esiste un algoritmo polinomiale che possa risolverli in maniera ottimale.
Infatti l'unico modo per risolvere il problema dello zaino in maniera ottimale è quello di provare tutte le combinazioni possibili, e questo ha complessità esponenziale, che cresce molto velocemente.


\section{Tabelle di Hash}

Quando per esempio stiamo scrivendo un programma e dobbiamo memorizzare delle variabili, vogliamo che ad ogni variabile
sia associato un indirizzo di memoria. Per esempio:

\begin{center}

\begin{tabular}{|c|c|}
  \hline
  Nome & Indirizzo\\
  \hline
  pippo & 0x0001\\
  pluto & 0x0002\\
  x & 0x0003\\
  \hline
\end{tabular}
\end{center}
Piuttosto che una tabella, non sarebbe comodo creare un array indicizzato da stringhe?
Quale è il problema? Il problema è che il numero di stringhe componibili sono veramente tante, e quindi non possiamo creare un array di dimensione troppo grande.
Ci converebbe trasformare l'insieme delle chiavi delle stringhe in un insieme di indici, e questo è possibile tramite la \textbf{funzione di Hash}.
\[A[i] \Longleftrightarrow A[H(i)]\]
Questo ci permette di trovare un buon compromesso per la dimensione dell'array e il numero di chiavi possibili.\\
Tuttavia se $H(i) = H(j)$ abbiamo un problema, e questo è chiamato \textbf{conflitto}.
Per quanto sufficientemente grande possa creare l'array, non posso evitare che ci siano conflitti.\\
Come posso risolvere i conflitti? Dobbiamo costruire una funzione hash che minimizzi al più possibile i conflitti.

\subsection{Gestione dei conflitti}

Costruiamo un array di $m$ elementi e lo riempiamo con $n$ elementi.
Chiamiamo $\alpha = \frac{n}{m}$ il fattore carico dell'array. Se $H(i)$ è casuale,
in media, quanti elementi ci aspettiamo in una cella $a$ di $b$? Un elemento finisce in una cella $a$ con probabilità $\frac{1}{m}$ e chiamiamo $X_{ia}$ la variabile casuale 
che vale uno se $i$ finisce in $a$. Ho in mano $X_{i1a} ... X_{ina}$ che rappresentano gli elementi che finiscono in $a$.
$X_{i1a} + ... + X_{ina}$ è una variabile casuale che conta gli elementi che fininiscono in $a$. Ma mediamente, quanti elementi mi aspetto di trovare in $a$? 
\[E[X_{i1a} + ... + X_{ina}] = E[X_{i1a}] + ... + E[X_{ina}] = \frac{1}{m} + ... + \frac{1}{m} = \frac{n}{m} = \alpha \]
Quindi ricapitolando: riduciamo l'insieme di chiavi in un insieme di indici tramite la funzione di Hash e poi trasformo l'array ottenuto in una lista concatenata.
Così che anche se si ha $H(i) = H(j)$ ci si può ricondurre all'elemento successivo della lista concatenata.\\
Calcolo $H(i)$ e cerco $B[H(i)]$ e se $B[H(i)] = i$ (dove B è l'array costruito) allora ho trovato l'elemento, altrimenti vado ad aggiungere alla lista l'elemento alla lista concatenata.
\\
\vspace{1em}
\noindent
Quanto costa accedere ad $A$? 
\begin{itemize}
  \item Calcolare $H(i)$ (A(1))
  \item Ricerca nella lista $B[H(i)]$ se $A(i)$ è presente $\alpha$, altrimenti mediamente $\frac{\alpha}{2}$
\end{itemize}
Quindi possiamo dire che accedere ad $A$ ha costo $O(1+\alpha)$ che è costante ed è quindi è come accedere un array.

\subsection{Creazione della funzione Hash}

\subsection{Metodo di divisione}

Definiamo una funzione $h$ in questo modo:

\[h(k) \stackrel{\Delta}{=} k \; \; mod \; \;  m \] 
ma attenzione al valore di $m$! Se $m$ è una potenza di due, allora stiamo scartando
i bit più significativi di $k$. Non ha senso creare una funzione Hash che permette di occupare solo 
una parte dell'array, sarebbe preferibile utilizzare una funzione Hash che permetta di utilizzare tutto l'array.
Meglio utilizzare un $m$ con potenze diverse da due (o da dieci). Vogliamo un valore $m$ che sia capace di incasinare il tutto e i numeri primi fanno al caso nostro.

\subsection{Metodo della moltiplicazione}

Definiamo una funzione $h$ in questo modo:

\[h(k) \stackrel{\Delta}{=} \left(\floor*{m(ka \; \; mod \; \; 1)}\right) \] 
dove $A$ è un numero reale positivo fissato. Cosa sta succedendo qui? Stiamo moltiplicando $k$ per $A$ e poi prendiamo la parte frazionaria di $ka$ e moltiplichiamo per $m$.
Ci esce un numero compreso tra $0$ e $m$ escluso (e quindi che permette di contenere gli elementi dell'array). Quindi il valore della funzione di Hash dipende da $A$.
Tuttavia come si sceglie il valore $A$? Il valore ideale di $A$ è $\frac{\sqrt{5}-1}{2}$, trovato da Knuth.
Questo numero è molto vicino a $0.6180339887$ che è il numero aureo. Questo valore permette di avere una distribuzione uniforme degli elementi nell'array. Questo numero appare
spesso in natura, ad esempio nelle conchiglie, nelle piante, nell'arte, nell'architettura, ecc. Questo numero è anche il rapporto tra due numeri della sequenza di Fibonacci.

\section{Grafi}

\subsection{Definizioni generali}

I grafi sono indubbiamente lo strumento più usato per astrarre problemi concreti e metterli mettere su "carta". 
Un grafo, in maniera molto semplice, non è nient'altro che un insieme di nodi uniti da archi.

\begin{figure}[H]
\centering
\begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
draw,processblue , text=blue , minimum width =1 cm}]
\node[state] (C) {$1$};
\node[state] (A) [above left=of C] {$0$};
\node[state] (B) [above right =of C] {$2$};
\path (A) edge [loop left] node[left] {$1/4$} (A);
\path (C) edge [bend left =25] node[below =0.15 cm] {$1/2$} (A);
\path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
\path (A) edge [bend left =25] node[above] {$1/4$} (B);
\path (B) edge [bend left =15] node[below =0.15 cm] {$1/2$} (A);
\path (C) edge [bend left =15] node[below =0.15 cm] {$1/2$} (B);
\path (B) edge [bend right = -25] node[below =0.15 cm] {$1/2$} (C);
\end{tikzpicture}
\caption{Esempio di grafo}
\end{figure}
\noindent
La cosa interessante che si può fare con i grafi è che potremmo anche modellizzare un labirinto su di essi. 
Mettiamo caso che io voglia rappresentare un labirinto con un grafo.


\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=4cm and 5cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (C) {$1$};
    \node[state] (E) [above=of C] {$5$};
    \node[state] (A) [above left=of C] {$3$};
    \node[state] (F) [below=of A] {$6$};
    \node[state] (B) [above right=of C] {$2$};
    \draw (C) -- (A);
    \draw (C) -- (B);
    \draw (C) -- (E);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (F) -- (C);
    \draw (C) -- (F);
    \draw (A) -- (E);
    \draw (A) -- (F);
    \draw (B) -- (E);
    
  \end{tikzpicture}
  \caption{Esempio di "labirinto" con i grafi}
\end{figure}
\noindent
Se definiamo un punto di ingresso- prendiamo per esempio il nodo $3$ -e un punto di uscita- prendiamo per esempio il nodo $2$ -da questo grafo, 
riusciamo a capire quali connessioni fare per arrivare all'uscita attraversa il minor numero di nodi. 
In questo caso il percorso $[3,1,2]$ o $[3,5,2]$ hanno il percorso minore.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=4cm and 5cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (C) {$1$};
    \node[state] (E) [above=of C] {$5$};
    \node[state] (A) [above left=of C] {$3$};
    \node[state] (F) [below=of A] {$6$};
    \node[state] (B) [above right=of C] {$2$};

    \draw[red] (A) -- (C) -- (B);
  \end{tikzpicture}
\end{figure}
\noindent
Quindi il caso migliore mi consente di attraversare solo tre nodi. 
Ma il caso peggiore? All'apparenza il percorso più lungo è di quattro nodi.
Ma in realtà potrei continuare a girare in cerchio e non arrivare mai alla fine.

\begin{itemize}
  \item Un'arco unisce due nodi. Quando due nodi sono uniti da un arco si dice che i due nodi sono
  \textbf{adiacenti}.
  \item  Un \textbf{cammino} è una sequenza di nodi che sono uniti da archi.
  \item  La \textbf{lunghezza} invece è il numero di archi attraversati oppure la lunghezza della sequenza
  meno uno. 
  \item \textbf{Cammino semplice} è un cammino che parte da un nodo di inizio e arriva a destinazione passando per ogni nodo solo una volta.
  \item \textbf{Un ciclo hamiltoniano} è un ciclo che passa per ogni nodo una sola volta.
\end{itemize}
Tuttavia potrei avere un grafo con degli \textbf{archi pesati} e la definizione del cammino potrebbe cambiare in base alla necessità che ho.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (C) {$1$};
  \node[state] (A) [above left=of C] {$0$};
  \node[state] (B) [above right =of C] {$2$};
  \path (A) edge [loop left] node[left] {$24$} (A);
  \path (C) edge [bend left =25] node[below =0.15 cm] {$3$} (A);
  \path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
  \path (A) edge [bend left =25] node[above] {$3$} (B);
  \path (B) edge [bend left =15] node[below =0.15 cm] {$6$} (A);
  \path (C) edge [bend left =15] node[below =0.15 cm] {$10$} (B);
  \path (B) edge [bend right = -25] node[below =0.15 cm] {$32$} (C);
  \end{tikzpicture}
  \caption{Esempio di grafo con archi pesati}
\end{figure}
\noindent
Nel grafo sopra, si può notare come negli archi ho un verso. Questo significa che se io vado da $A$ a $B$ non posso tornare indietro da $B$ ad $A$.
Questo tipo di grafo è chiamato \textbf{grafo orientato}. Se invece non ci fosse un verso, il grafo sarebbe chiamato \textbf{grafo non orientato}.
Un grafo non orientato è \textbf{connesso} se esiste un cammino tra ogni coppia di nodi.

\thm{}{
  Un grafo orientato è aciclico, se e solo se non contiene cammini infiniti.
}

\begin{proof}
  Sia $G$ un grafo aciclico. Supponiamo per assurdo che contenga un cammino infinito.
  \[v_1, v_2, v_3, ..., v_n, v_{n+1} ...\]
  Se $n$ è il numero di nodi nel grafo, allora devono esistere nodi ripetuti.
\end{proof}

\thm{}{
  Un grafo orientato è ciclico, se e solo se contiene cammini infiniti.
}

\begin{proof}
  \[\neg B \implies \neg A\]
\end{proof}
Possiamo definire il \textbf{grado} di un nodo come la somma tra gli archi uscenti ed entranti.
\thm{}{
  In un grafo aciclico esistono nodi con grado entrante pari a zero.
 }
 \begin{proof}
  Supponiamo per assurdo che dato
  un grafo aciclico non esista un nodo con grado entrante pari a zero. Allora:
  \[v_{n+1} \rightarrow v_n \rightarrow v_{n-1} ... v_3 \rightarrow v_2 \rightarrow v_1\]
  possiamo "andare indietro" fin quanto vogliamo quindi posso costruire sequenze di lunghezza arbitraria.
  Ma di conseguenza vuol dire che esistono nodi ripetuti. Ma ripetuto vuol dire ciclo. E per definizione questo
  non può essere.
 \end{proof} 
Cerchiamo ora di definire il concetto di \textbf{grafo} in maniera formale.
\dfn{Grafo}{
  Un \textbf{grafo} è una coppia $(V,E)$ dove $V$ è un insieme di nodi e $E \subset V \times V$ è un insieme di archi.
}
\noindent
Esiste un arco da $A$ a $B$ se $(A, B) \in E$. Solitamente si rappresenta un arco con la coppia $(x, y)$ oppure con $x \rightarrow y$.
\begin{lemma}
  Se $|V|$ è la cardinalità dei nodi e $|E|$ è la cardinalità degli archi, sicuramente $|E| \leq |V|^2$.
\end{lemma}
\dfn{Cammino}{
  Un \textbf{cammino} è una sequenza di nodi $v_1, v_2, ..., v_l$ tale che $\forall i \in \{1, \dots, l - 1\} (v_i, v_{i+1}) \in E$.
}
\dfn{Cammino semplice}
{
  Un cammino è \textbf{semplice} se ogni nodo di $V$ appare al più di una volta.
}
\dfn{Grado entrante}
{
  Il \textbf{grado entrante} di un nodo $v$ è definito come
  \[d_{in}(v) \stackrel{\Delta}{=} |\{(u, v) \in E\}| = |(V \times \{v\})\cap E|\]
}
\dfn{Grado uscente}
{
  Il \textbf{grado uscente} di un nodo $v$ è definito come
  \[d_{out}(v) \stackrel{\Delta}{=} |(\{v\} \times V) \cap E|\]
}
\dfn{Grado}
{
  \begin{center}
      Grado($v$) $\stackrel{\Delta}{=} |E \cap (V \times \{v\} \cup \{v\} \times V)|$
  \end{center}
}

\subsection{Rappresentanza di un grafo}

Un grafo può essere rappresentato in diversi modi:
\begin{itemize}
  \item Liste di adiacenza
  \item Matrice di adiacenza
\end{itemize}
Creo una lista di nodi dove a loro a volta inserisco una lista di nodi adiacenti.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
    node distance=0cm and 0.5cm
    ]
    \node[draw,rectangle,minimum width=1cm,minimum height=0.5cm] (1) at (0,0) {1};
    \node[draw,rectangle,minimum width=1cm,minimum height=0.5cm] (2) [below=of 1] {2};
    \node[draw,rectangle,minimum width=1cm,minimum height=0.5cm] (3) [below=of 2] {3};
    \node[draw,rectangle,minimum width=1cm,minimum height=0.5cm] (4) [below=of 3] {4};

    \node[draw,rectangle,minimum size=0.5cm] (1-1-1) [right=of 1] {3};
    \node[draw,rectangle,minimum size=0.5cm] (1-1-2) [right=0cm of 1-1-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (1-1-3) [right=0cm of 1-1-2] {};
    \draw[->] (1) -- (1-1-1);
    \node[draw,rectangle,minimum size=0.5cm] (1-2-1) [right=of 1-1-3] {1};
    \node[draw,rectangle,minimum size=0.5cm] (1-2-2) [right=0cm of 1-2-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (1-2-3) [right=0cm of 1-2-2] {};
    \draw[->] (1-1-3.center) -- (1-2-1);
    \node[draw,rectangle,minimum size=0.5cm] (1-3-1) [right=of 1-2-3] {4};
    \node[draw,rectangle,minimum size=0.5cm] (1-3-2) [right=0cm of 1-3-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (1-3-3) [right=0cm of 1-3-2] {};
    \draw[->] (1-2-3.center) -- (1-3-1);
    \node[draw,rectangle,minimum size=0.5cm] (1-4-1) [right=of 1-3-3] {2};
    \node[draw,rectangle,minimum size=0.5cm] (1-4-2) [right=0cm of 1-4-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (1-4-3) [right=0cm of 1-4-2] {\footnotesize/};
    \draw[->] (1-3-3.center) -- (1-4-1);


    \node[draw,rectangle,minimum size=0.5cm] (2-1-1) [right=of 2] {4};
    \node[draw,rectangle,minimum size=0.5cm] (2-1-2) [right=0cm of 2-1-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (2-1-3) [right=0cm of 2-1-2] {\footnotesize/};
    \draw[->] (2) -- (2-1-1);


    \node[draw,rectangle,minimum size=0.5cm] (3-1-1) [right=of 3] {1};
    \node[draw,rectangle,minimum size=0.5cm] (3-1-2) [right=0cm of 3-1-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (3-1-3) [right=0cm of 3-1-2] {\footnotesize/};
    \draw[->] (3) -- (3-1-1);


    \node[draw,rectangle,minimum size=0.5cm] (4-1-1) [right=of 4] {1};
    \node[draw,rectangle,minimum size=0.5cm] (4-1-2) [right=0cm of 4-1-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (4-1-3) [right=0cm of 4-1-2] {};
    \draw[->] (4) -- (4-1-1);
    \node[draw,rectangle,minimum size=0.5cm] (4-2-1) [right=of 4-1-3] {2};
    \node[draw,rectangle,minimum size=0.5cm] (4-2-2) [right=0cm of 4-2-1] {p};
    \node[draw,rectangle,minimum size=0.5cm] (4-2-3) [right=0cm of 4-2-2] {\footnotesize/};
    \draw[->] (4-1-3.center) -- (4-2-1);
  \end{tikzpicture}
  \caption{Rappresentazione per liste di adiacenza}
\end{figure}
\noindent
Mentre con una matrice di adiacenza, posso inserire $1$ nel punto $(i,j)$ se questo arco è contenuto in $E$, $0$ altrimenti.
I vantaggi della lista di adiacenza sono che occupa meno spazio, mentre la matrice di adiacenza è più veloce per trovare se due nodi sono adiacenti, ma occupa più spazio perché contiene esattamente $|V|^2$ elementi.
\begin{table}[H]
  \centering
  \begin{tabular}{c|cccc}
    / & 1 & 2 & 3 & 4 \\
    \hline
    1 & 1 & 1 & 1 & 1 \\
    2 & 0 & 0 & 0 & 1 \\
    3 & 1 & 0 & 0 & 0 \\
    4 & 1 & 1 & 0 & 0 \\
  \end{tabular}
  \caption{Rappresentazione per matrice di adiacenza}
\end{table}
\ex{}{
  Supponiamo di voler rappresentare con un grafo la rete stradale italiana dove gli archi rappresentano il numero di incidenti.
  Si andrebbe a creare una matrice con molte celle vuote e poche celle piene. 
  Questo tipo di matrici sono dette \textbf{sparse}.
}
\noindent
Quindi solitamente si usa liste di adiacenza per grafi sparsi e matrici di adiacenza per grafi densi.
\\
Se volessi calcolare il grado uscente di un nodo in una lista di adiacenza, dovrei scorrere tutta la lista per contare quanti elementi ci sono.
Di conseguenza, un algoritmo che calcola il grado di un nodo in una lista di adiacenza ha complessità $O(out_deg(v))$ dove $out_deg(v)$ è il numero di nodi adiacenti.
Mentre per una matrice di adiacenza, il grado uscente di un nodo si calcolerebbe con complessità $O(V)$ dove passare per ogni possibile nodo.
\begin{lstlisting}[language=Scala]
all_out_deg(G)
  for i <- 1 to |V|
    out_deg(i) <- 0
    for j <- 1 to Adj[V]
      out_deg(i)++
  return out_deg
\end{lstlisting}
L'algoritmo ha complessità pari al numero di archi più il numero di nodi $\Theta(E + V)$.
Mentre per la matrice ha comèlessità $\Theta(V^2)$.

\subsection{Visita di un grafo}

\subsubsection{Visita in ampiezza}

La visita in ampiezza è un algoritmo che permette di visitare tutti i nodi di un grafo a partire da un nodo di partenza.
Anche chiamata \textbf{BFS} (Breadth First Search), l'algoritmo visita tutti i nodi adiacenti al nodo di partenza, poi tutti i nodi adiacenti a questi nodi e così via.
Ovviamente l'algoritmo non visita mai due volte lo stesso nodo.

\begin{figure}[H]
  \centering

  \begin{forest}
for tree={
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=0.5cm % Spaziatura verticale tra i livelli
}
[0
    [1
      [2
        [3]
        [3]
        [3]
      ]
      [2
        [3]
        [3]
        [3]
      ]
    ]
    [1
      [2
        [3]
        [3]
        [3]
      ]
      [2
        [3]
        [3]
        [3]
      ]
    ]
    [1
      [2
        [3]
        [3]
        [3]
      ]
      [2
        [3]
        [3]
        [3]
      ]
    ]
]
\end{forest}
\end{figure}
\noindent
\begin{lstlisting}[language=Scala]
BFS(G, S)
# WHITE se non ho ancora visitato il nodo
# GRAY se ho visitato il nodo ma non ho ancora visitato i suoi vicini
# BLACK se ho visitato il nodo e i suoi vicini
$\forall u \in V[G]$
  color[u] <- WHITE
  d[u] <- $\infty$
  $\pi[u] <- NIL$
color[S] <- GRAY 
d[S] <- 0 # Distanza dal nodo di partenza a se stesso (quindi 0)
Q <- {S} # Inizializzo una coda FIFO che contiene i nodi che 
# ho gia visto ma non ho ancora visitato i vicini
while Q $\neq \emptyset$
  u <- Q.dequeue()
  $\forall v \in Adj[u]$ # Scorro tutti i nodi adiacenti a u
    if color[v] == WHITE
      color[v] <- GRAY
      d[v] <- d[u] + 1
      $\pi[v] <- u
      Q.enqueue(v)
  Q.dequeue()
  color[u] <- BLACK
\end{lstlisting}
Il primo ciclo ha come complessità $\Theta(V)$ poiché bisogna inizializzare tutti i nodi del grafo.
, mentre il secondo ciclo ha complessità $\Theta(E)$ perché dobbiamo attraversare tutti gli archi che sono presenti nel grafo per visitarlo completamente.
Quindi la complessità totale dell'algoritmo è $\Theta(V + E)$. Ora che abbiamo
delle informazioni sui nodi, possiamo calcolare la distanza tra due nodi.
\begin{lstlisting}[language=Scala]
path(u)
  if u != s
    path($\pi[u]$)
  print(u)
\end{lstlisting}
La BFS non solo permette di visitare tutti i nodi di un grafo, ma permette anche di calcolare la distanza tra il nodo S e qualsiasi altro nodo del grafo.
Non solo, ma permette anche di calcolare il cammino più \textit{corto} tra due nodi.
Vogliamo dimostrare che la BFS calcola un albero dei cammini minimi da un nodo di partenza $s$ dove si minimizza la lunghezza (numeri archi).
Abbiamo bisogno dei lemmi che ci torneranno utili per la dimostrazione.
\begin{lemma}
$\forall (u,v) \in E, \delta(s,v) \le \delta(s,u) + 1$ 
\end{lemma}
Questo lemma ci dice che la distanza tra due nodi è minore o uguale alla distanza tra un nodo e un suo vicino più uno. In poche parole, se troviamo un cammino più corto, allora il cammino più lungo non è sicuramente migliore di quello che ho trovato.
\begin{lemma}
Dopo BFS, $d[v] \ge \delta(s,v) \; \; \forall v \in E$
\end{lemma}
Quando eseguo $d[v] = d[u] + 1$ sto analizzando un arco $(u,v)$. Noi sappiamo che $d[v] = d[u] + 1$, ma sappiamo anche che $d[u] \ge \delta(s,u)$. Di conseguenza $d[u] + 1 \ge \delta(u,v) \ge \delta(s,v)$.
\begin{lemma}
In $Q$ ci sono sempre al più due valori di $d$ e $Q$ è ordinata per $d$.
Se 
\[Q = v_1, \dots, v_r \; \; \; d[v_1] \le d[v_2] \le \dots \le d[v_r] \le d[v_1] + 1\]
\end{lemma}
Nel senso che nella coda, i nodi sono ordinati per distanza. Se io prendo un nodo $v$ e lo metto in coda, la sua distanza sarà sempre maggiore o uguale alla distanza del nodo precedente più uno.
\thm{}{
  Per ogni nodo $v \in V$, esiste un momento in cui:
  \begin{itemize}
    \item $v$ è grigio
    \item $\delta(s,v)$ assegnato a $d[v]$
    \item $v \neq s$, $\pi[v] = v$ con $\delta(s,u) = \delta(s,v)-1$
    \item $v$ inserito in coda
  \end{itemize}
}
\begin{proof}
$\forall k$, sia $V_k$ l'insieme dei nodi $v$ con $\delta(s,v) = k$. Per induzione su $k$:
\begin{itemize}
  \item Per $V_0$ c'è solo $s$ che viene inizializzato.
  \item Supponiamo che sia vero per $V_0 ... V_k$ e dimostriamolo per $V_{k+1}$.
  Sia $v \in V_{k+1}$ 
  \begin{itemize}
    \item Quando avremo inserito in coda tutti i nodi di $V_k$ il nodo $v$ non sarà ancora stato visto altrimenti $d[v] \le k+1$ che va contro il lemma (8.3) che avevamo enunciato.
    \item La coda $Q$ è ordinata, quindi analizzeremo la lista di adiacenza di ogni nodo $V_k$ prima di qualsiasi nodo con distanza superiore a $k$.
    \item Tra i nodi di $V_k$ ne esiste uno $v \in V_k$ t.c $(u,v) \in E$.
    \item Quando analizziamo $u$ troviamo $v \in Adj[u]$ ed in quel momento:
    \begin{itemize}
      \item $v$ è grigio
      \item assegniamo $d[v] = d[u] + 1 = k + 1$
      \item $\pi[v]$ prende $v \in V_k$ 
      \item $v$ inserito in coda
    \end{itemize}
  \end{itemize}
\end{itemize}

\end{proof}

\begin{lstlisting}[language=Scala]
BFS(G)
  for u in V[G]
    color[u] <- WHITE
    d[u] <- $\infty$
    $\pi[u] <- NIL$
  count = 0
  for u in V[g]
    if color[u] == WHITE
      BFS(G, u, count)


BFS(G, S, count)
  color[s] <- GRAY
  d[s] <- 0
  $\pi[s] <- NIL$
  while $Q \neq \emptyset$
    u <- Q.dequeue()
    for v in Adj[u]
      if color[v] == WHITE
        color[v] <- GRAY
        counter[v] <- count
        d[v] <- d[u] + 1
        $\pi[v] <- u
        Q.enqueue(v)
    color[u] <- BLACK
\end{lstlisting}
Decorando l'algoritmo con un contatore, posso \textbf{calcolare il numero di componenti connesse} di un grafo.




\subsubsection{Visita in profondità}

La visità in profondità, a differenza della visita in ampiezza, visita un nodo e poi si sposta in profondità, visitando tutti i nodi adiacenti a questo nodo per poi tornare indietro.
Anche chiamata \textbf{DFS} (Depth First Search).


\begin{figure}[H]
  \centering

  \begin{forest}
for tree={
  inner sep=1pt, % Spaziatura interna dei nodi
  s sep=0.5cm % Spaziatura verticale tra i livelli
}
[0
    [1
      [2
        [3
          [5
            [8]
          ]
          [6]
          [7]
        ]
        [4]
      ]
    ]
]
\end{forest}
\end{figure}

\begin{lstlisting}[language=Scala]
DFS(G)
  // per ogni nodo del grafo
  for u in V[G]
    color[u] <- WHITE
    $\pi[u] <- NIL$
  time <- 0
  for u in V[G]
    if color[u] == WHITE
      DFS-Visit(G, u)


DFS-Visit(G, u)
  color[u] <- GRAY
  time <- time + 1
  d[u] <- time
  for v in Adj[u]
    if color[v] == WHITE
      $\pi[v] <- u
      DFS-Visit(G, v)
  color[u] <- BLACK
  time <- time + 1
  f[u] <- time
\end{lstlisting}
\noindent
In questo algoritmo può sembrare che non ci sia l'utilizzo di una coda, come invece è esplicito nella BFS, ma
in questo caso la coda è implicita all'interno di DFS-Visit definita ricorsivamente. 

\ex{}
{
 
Consideriamo un grafo $G$ di nodi $V$ e archi $E$ di questo tipo: tutti i nodi sono bianchi, tranne il nodo $s$ che è grigio perché sarà il nostro punto di partenza e il suo tempo sarà 1.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (W) {W};
    \node[state] (Z) [above=of W] {Z};
    \node[state] (Y) [above left=of W] {Y};
    \node[state] (S) [above right=of W] {S, $1$};
    \node[state] (X) [below=of Y] {X};
    \node[state] (V) [right=of W] {V};    
    \node[state] (U) [right of=V] {U};
    \node[state] (T) [above=of U] {T};


    \draw (Y) -- (X);
    \draw (X) -- (Z);
    \draw (Z) -- (Y);
    \draw (Z) -- (W);
    \draw (W) -- (X);
    \draw (S) -- (W);
    \draw (S) -- (Z);
    \draw (V) -- (W);
    \draw (T) -- (V);
    \draw (V) -- (S);
    \draw (T) -- (U);
    \draw (U) -- (T);


  \end{tikzpicture}
\end{figure}
\noindent
Una volta aver visitato il primo nodo $s$, dobbiamo entrare dentro la funzione DFS-Visit che ci fa visitare tutti i suoi nodi adiacenti.
Nella lista di adiacenza di $s$ abbiamo $W$ e $Z$. Visitando per primo $Z$ cadiamo ricorsivamente dentro la funzione DFS-Visit che ci porta a visitare tutti i nodi adiacenti a $Z$.
Una volta che arriviamo ad un nodo a cui viene completata la lista di adiacenza di nodi non ancora visitati, dobbiamo incrementare $f[u]$ di uno.
Quindi in questo nostro primo caso passeremo per $Z$ che poi entra in $Y$ che a sua volta entra in $X$ che una volta arrivato lì, l'unico suo nodo adiacente è $Z$ che però è già stato visitato.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (W) {W};
    \node[state] (Z) [above=of W, color=gray] {Z, $2$};
    \node[state] (Y) [above left=of W, color=black] {Y, $3/6$};
    \node[state] (S) [above right=of W, color=gray] {S, $1$};
    \node[state] (X) [below=of Y, color=black] {X, $4/5$};
    \node[state] (V) [right=of W] {V};    
    \node[state] (U) [right of=V] {U};
    \node[state] (T) [above=of U] {T};


    \draw (Y) -- (X);
    \draw (X) -- (Z);
    \draw (Z) -- (Y);
    \draw (Z) -- (W);
    \draw (W) -- (X);
    \draw (S) -- (W);
    \draw (S) -- (Z);
    \draw (V) -- (W);
    \draw (T) -- (V);
    \draw (V) -- (S);
    \draw (T) -- (U);
    \draw (U) -- (T);
  \end{tikzpicture}
\end{figure}
\noindent
Tornando a $Z$ dobbiamo ancora visitare le sue liste di adiacenza tra cui $W$. Quindi a tempo $7$ arriviamo in $W$ e a tempo $8$ completiamo la lista di adiacenza di $W$.
Continuiamo finché non torniamo ad $s$ dove anche lui finisce la sua lista di adiacenza e incrementa il tempo di uno.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (W) [color=black] {W, $7/8$};
    \node[state] (Z) [above=of W, color=black] {Z, $2/9$};
    \node[state] (Y) [above left=of W, color=black] {Y, $3/6$};
    \node[state] (S) [above right=of W, color=black] {S, $1/10$};
    \node[state] (X) [below=of Y, color=black] {X, $4/5$};
    \node[state] (V) [right=of W] {V};    
    \node[state] (U) [right of=V] {U};
    \node[state] (T) [above=of U] {T};


    \draw (Y) -- (X);
    \draw (X) -- (Z);
    \draw (Z) -- (Y);
    \draw (Z) -- (W);
    \draw (W) -- (X);
    \draw (S) -- (W);
    \draw (S) -- (Z);
    \draw (V) -- (W);
    \draw (T) -- (V);
    \draw (V) -- (S);
    \draw (T) -- (U);
    \draw (U) -- (T);
  \end{tikzpicture}
\end{figure}
Ora che abbiamo finito con questa prima iterazione, dobbiamo andare avanti con i nodi bianchi rimasti, supponiamo di trovare $T$ come prossimo nodo.
Facciamo lo stesso procedimento per quest'ultima iterazione e otterremo un grafo del genere:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (W) [color=black] {W, $7/8$};
    \node[state] (Z) [above=of W, color=black] {Z, $2/9$};
    \node[state] (Y) [above left=of W, color=black] {Y, $3/6$};
    \node[state] (S) [above right=of W, color=black] {S, $1/10$};
    \node[state] (X) [below=of Y, color=black] {X, $4/5$};
    \node[state] (V) [right=of W, color=black] {V, $12/13$};    
    \node[state] (U) [right of=V, color=black] {U, $14/15$};
    \node[state] (T) [above=of U, color=black] {T, $11/16$};


    \draw (Y) -- (X);
    \draw (X) -- (Z);
    \draw (Z) -- (Y);
    \draw (Z) -- (W);
    \draw (W) -- (X);
    \draw (S) -- (W);
    \draw (S) -- (Z);
    \draw (V) -- (W);
    \draw (T) -- (V);
    \draw (V) -- (S);
    \draw (T) -- (U);
    \draw (U) -- (T);
  \end{tikzpicture}
\end{figure}
In questa maniera abbiamo ottenuto due alberi, uno per ogni componente connessa.
}
\noindent
Tuttavia, c'era un modo migliore per scegliere un nodo iniziale da cui partire e ottenere un albero singolo?
Se fossimo partiti dal nodo $T$ nell'esempio precedente avremmo ottenuto un singolo albero.
\thm{}
{
  $\forall$ coppia di nodi $u,v$ gli intevalli sono disgiunti oppure sono uno sottointervallo dell'altro.
}

\begin{proof}
  Prendendo $u,v$ ci sono due casi. 
  \begin{itemize}
    \item d[u] $<$ d[v]
    \item d[v] $<$ d[u]
  \end{itemize}
  Ipotizziamo che d[u] $<$ d[v] senza perdità di generalità (questo perché dimostrare un caso o un altro è equivalente). 
  Questo vuol dire che $u$ è stato visitato prima di $v$. 
  Distinguiamo due casi:
  \begin{itemize}
    \item Caso $a$: f[u] $<$ d[v], tuttavia se è così, vuol dire che sono disgiunti. [d[u], f[u]] [d[v]]
    \item Caso $b$: d[v] $<$ f[u], se invece è così, vuol dire che $v$ è stato visitato prima di $u$ e quindi $[d[v], f[v]] \subseteq [d[u], f[u]]$ e di conseguenza sono sottointervalli.
  \end{itemize}
\end{proof}

\thm{
}
{
  In una foresta DFS di un grafo $G$, un nodo $v$ discende da $u$ se e solo se al tempo
  $d[u]$, $v$ è raggiungibile da $u$ con un cammino di solo nodi bianchi.  
}

\begin{proof}
  \\
  $\Longrightarrow$ Supponiamo che $v$ discenda da $u$ e sia $w$ un nodo nel cammino da $u$ a $v$ nella foresta DFS.
  Se $w$ discende da $u$ allora per il teorema 8.3.2 $d[u] < d[w] < f[w] < f[u]$.\\
  $\Longleftarrow$ Supponiamo che al tempo $d[u]$ esiste un cammino da $u$ a $v$ fatto di soli nodi bianchi.
  Supponiamo per assurdo che $v$ non discenda da $u$. 
  Sia $w$ il predecessore di $v$ e assumiamo che $w$ discenda da $u$. Allora \[d[u] < d[w] < d[v] < f[v] < f[w] < f[u]\]
\end{proof}
Ci sono alcuni archi nel grafo che possiedono nomenclature diverse in base ai movimenti che fanno. Per esempio, nel grafo di esempio precedente, l'arco $(V, S)$ è un arco di ritorno che noi chiamaremo "backward", mentre l'arco $(S, W)$ è un arco di attraversamento che chiameremo "forward".
Ci sono alcuni archi invece che vanno da un ramo all'altro si chiamano crossing. 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (W) [color=black] {W, $7/8$};
    \node[state] (Z) [above=of W, color=black] {Z, $2/9$};
    \node[state] (Y) [above left=of W, color=black] {Y, $3/6$};
    \node[state] (S) [above right=of W, color=black] {S, $1/10$};
    \node[state] (X) [below=of Y, color=black] {X, $4/5$};
    \node[state] (V) [right=of W, color=black] {V, $12/13$};    
    \node[state] (U) [right of=V, color=black] {U, $14/15$};
    \node[state] (T) [above=of U, color=black] {T, $11/16$};


    \draw (Y) -- (X);  
    \draw (X) -- node[midway, below] {B} (Z);  
    \draw (Z) -- (Y);  
    \draw (Z) -- (W);  
    \draw (W) -- (X);  
    \draw (S) -- node[midway, right] {F} (W);  
    \draw (S) -- node[midway, above] {T} (Z);  
    \draw (V) -- node[midway, below] {C} (W);  
    \draw (T) -- (V);  
    \draw (V) -- node[midway, right] {C} (S);  
    \draw (T) -- (U);  
    \draw (U) -- node[midway, right] {B} (T);  
    
  \end{tikzpicture}
\end{figure}
\noindent
Perché ci servono queste classificazioni?
Gli diamo dei nomi perché ci permettono di capire meglio la struttura del grafo.
\thm{}
{
  Un grafo è aciclico se e solo se la DFS non trova archi all'indietro.
}
\begin{proof}
 Supponiamo che DFS trova un arco $(u, v)$ all'indietro. 
 \begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    

    \node[state] (V) {V};
    \node[state] (U) [below of=V] {U};
    
    \draw (U) -- (V);    
  \end{tikzpicture}
\end{figure}
\noindent
Se è un arco all'indietro vuol dire che $v$ sarà di colore grigio. Ciò vuol dire che $u$ è un discendente di $v$. 
\[d[v] < d[u] < f[u] < f[v]\]
Tuttavia essere grigio nella foresta DFS ed essere discendente di un grigio vuol dire che esiste un cammino che va da $v$ a $u$. 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    

    \node[state] (V) {V};
    \node[state] (U) [below of=V] {U};
    
    \draw (U) -- (V);    
    \path (V) edge [bend left = -15] (U);
  \end{tikzpicture}
\end{figure}
\noindent
Questo arco completa il ciclo, di conseguenza, $G$ è ciclico. \\
Supponiamo che $G$ sia ciclico. Prendiamo un ciclo qualsiasi nel nostro grafo. Sia $u$ il primo nodo del ciclo visitato da DFS. 
Sia $v$ il predecessore di $u$ nel ciclo. Al tempo $d[u]$ tutti i nodi del ciclo tranne $u$ sono bianchi. 
Quindi al tempo $d[u]$ il nodo $v$ è raggiungibile da $u$ con un cammino di soli nodi bianchi. Ma se così è, per il teorema 8.3.3 
$v$ discende da $u$. $v$ sarà visitato quando $u$ è ancora grigio. Ma durante la visita di $v$ si analizzerà l'arco $(v,u)$ e di conseguenza si troverà un arco all'indietro. $\square$
\end{proof}
Qual è la complessità del problema di verificare se un grafo è aciclico? 
Sicuramente non peggio della complessità della DFS. 
Ma quanto è la complessità della DFS?
\begin{itemize}
  \item Inizializzazione: $\Theta(V)$
  \item DFS-Visit: $\Theta(E)$
\end{itemize}
Quindi la complessità totale è $\Theta(V + E)$. Di conseguenza la complessità del problema di verificare se un grafo è aciclico è $O(V + E)$.

\ex{}
{
  \textbf{Esempio da tema d'esame:}
  Burocraziland è un paese che possiede una burocrazia molto complessa. Sei stato assunto come nuovo presidente per risanare le falle
  del governo e devi fare in modo che all'interno dei documenti di questo paese non ci siano circoli di approvazione. 
  Immaginiamo di avere un grafo in cui i nodi sono i documenti e gli archi sono le approvazioni.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
      state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
      
  
      \node[state] (A) {A};
      \node[state] (B) [right of=A] {B};
      
      \path (A) edge [bend right = -15] (B);
    \end{tikzpicture}
  \end{figure}
  In questo caso posso decidere se dire che per avere il documento $A$ mi serve il documento $B$ o viceversa.
  Facciamo che in questo caso, per avere il documento $A$ mi serve il documento $B$.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
      state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
      
      \node[state] (A) {A};
      \node[state] (B) [above right of=A] {B};
      \node[state] (C) [below right of=B] {C};
      \node[state] (D) [below left of=C] {D};
      
      \path (A) edge [bend right = -15] (B);
      \path (B) edge [bend right = -15] (C);
      \path (C) edge [bend right = -15] (D);
      \path (D) edge [bend right = -15] (A);
    \end{tikzpicture}
  \end{figure}
  Se io volessi sapere tutti i documenti che non si possono ottenere a causa di un circolo di approvazione, come potrei fare?
  La soluzione è quella di fare una DFS e vedere se trovo un arco all'indietro. Se trovo un arco all'indietro, allora ho trovato un circolo di approvazione.
  Tuttavia, questa operazione la dovrei fare per ogni singolo nodo nel grafo, quindi la complessità sarebbe $O(V(V + E))$.
  Ma c'è un modo più efficiente per farlo. Osservando che se trovo un arco all'indietro, allora ho trovato un circolo di approvazione, posso fare in modo che se trovo un arco all'indietro, allora \textbf{tutti i nodi che sono raggiungibili da quel nodo} sono parte di un circolo di approvazione.
}

\subsection{Ordinamento topologico}

Ci sono casi in cui abbiamo bisogno di ordinare i nodi di un grafo in modo tale che 
il grafo venga visitato in un certo ordine. 
\ex{}
{
  Immaginiamo di avere un roboto che deve andare a vestire per andare a lavoro.
  Il robot ha bisogno di indossare prima i pantaloni e poi la maglietta, ma non può indossare le scarpe prima di aver indossato i pantaloni.
  In questo momento stiamo ponendo delle restrizioni su come il robot può vestirsi.
  Questo tipo di restrizioni possono essere rappresentate da un grafo orientato in cui i nodi sono gli oggetti da indossare e gli archi sono le restrizioni.
  Quindi per esempio se abbiamo i nodi $P$ (pantaloni), $S$ (scarpe) e $M$ (maglietta), avremo gli archi $(P, S)$ e $(P, M)$.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
      state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
      
      \node[state] (P) {P};
      \node[state] (S) [right of=P] {S};
      \node[state] (M) [below right of=S] {M};
      
      \draw (P) -- (S);
      \draw (P) -- (M);
    \end{tikzpicture}
  \end{figure}
}
\noindent
Il problema non sta nella costruzione del grafo, poiché quello che ci viene dato, 
ma nel trovare un modo per visitare i nodi in modo tale che le restrizioni siano rispettate e 
che il robot possa vestirsi correttamente. In maniera formale: 

\dfn{Ordinamento topologico}
{
  Ordinare i nodi del grafo in modo tale che $\forall (u,v) \in E$ t.c. $u < v$ si chiama
  \textbf{ordinamento topologico} di un grafo orientato.
}
\noindent
Un termine comodo seppur non preciso, può essere "linearizzare" il grafo, poiché quello che andremo a 
sarà quello di disporre il grafo con tutti archi che vanno da sinistra a destra su una linea di nodi.
\begin{lstlisting}[language=Scala]
topological_sort(G)
    usa DFS per calcolare f
    se ciclico set false
    restituisci nodi per f decrescente
\end{lstlisting}
Utilizzare la DFS e le sue proprietà ci aiuta a risolvere il problema di ordinamento topologico.
\thm{}
{
  $\forall (u,v) \in E$, $f[v] < f[u]$ ogni volta che $f$ è il tempo di fine visita
  di una DFS su grafo aciclcico. \textcolor{white}{I pantalonzi vanno prima dei calzonzi.}
}
\begin{proof}
  Sia $(u,v) \in E$ e quando $(u,v)$ viene visitato distinguiamo 3 casi:
  \begin{itemize}
    \item v bianco, v discenda da u, quindi $d[u] < d[v] < f[v] < f[u]$.
    \item v grigio, impossibile perché $G$ è aciclico.
    \item v nero, abbiamo già terminato la visita di $v$ ma non quella di $u$ e quindi $f[v] < f[u]$
  \end{itemize}
\end{proof}

\subsection{Componenti connesse}

Come calcolare le componenti connesse di un grafo data una qualsiasi visita?
Ricordiamo le funzioni che abbiamo imparato negli scorsi capitoli:
\begin{itemize}
  \item MakeSet(x): Crea un insieme contenente l’oggetto x.
  \item Union(x, y): Unisce gli insiemi contenenti x e y.
\end{itemize}

\begin{lstlisting}[language=Scala]
CC(G)
  per ogni V in V[G]
    make_set(v)
  per ogni V E[G]
    union(u,v)
\end{lstlisting}
\noindent
Si può facilmente vedere come alla fine dell'algoritmo si ottengono le connessioni tra i nodi del grafo.
Alla fine dell'algoritmo, ogni nodo sarà parte di un insieme che rappresenta una componente connessa del grafo.
La complessità di questo algoritmo è $O(V + E)$, poiché dobbiamo visitare tutti i nodi e tutti gli archi del grafo.

\subsection{Componenti fortemente connesse}

Le componenti fortemente connesse sono un concetto che si applica ai grafi orientati.
\dfn{Componente fortemente connessa (SCC)}
{
  Una componente fortemente connessa è un sottoinsieme di nodi $C \subseteq V$ tale che $\forall (u,v) \in C$ esiste un cammino da $u$ a $v$ e viceversa.
}

\begin{lstlisting}[language=Scala]
SCC(G)
  Usa DFS per calcolare f (V+E)
  Calcola G^t  (grafo trasposto, il senso degli archi e' invertito) (V+E)
  Usa DFS su G^t esplorando i nodi per f decrescente (V+E)
  Gli alberi della foresta DFS risultante sono le SCC 
\end{lstlisting}
\noindent
Quindi la complessità di questo algoritmo è $O(V + E)$, poiché dobbiamo visitare tutti i nodi e tutti gli archi del grafo.

\begin{lemma}
  Alla fine della DFS, ogni SCC (Strongly Connected Component) è interamente contenuta in un albero.
\end{lemma}

\begin{proof}
  Prendiamo una SCC, ci sarà un nodo $u$ che sarà il primo nodo visitato della SCC.
  Tutti gli altri nodi sono raggiungibili da un cammino di soli nodi bianchi, quindi per il teorema 8.3.3, sono tutti
  discendenti di $u$ e quindi appartengono allo stesso albero.
\end{proof}
\noindent
Ricordiamoci burocraziland (Esempio 8.3) e proviamo a risolvere il problema.
Se abbiamo un nodo $A$ connesso ad un nodo con il verso orientato su $B$ vuol dire che per avere certificato $B$ bisogna
prima avere $A$. Le \textit{conditio sine qua non}, per cui non si può ottenere un documento $x$ sono:
\begin{itemize}
  \item sicuramente se $x$ appartiene un ciclo. 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex, auto, node distance=3cm and 3cm, on grid,
    state/.style={circle, top color=white, bottom color=processblue!20, draw, processblue, text=blue, minimum width=1cm}]
    
    \node[state] (A) {X};
    \node[state] (B) [above right of=A] {B};
    \node[state] (C) [below right of=B] {C};
    \node[state] (D) [below left of=C] {D};
    
    \path (A) edge [bend right = -15] (B);
    \path (B) edge [bend right = -15] (C);
    \path (C) edge [bend right = -15] (D);
    \path (D) edge [bend right = -15] (A);
  \end{tikzpicture}
\end{figure}
\item Anche se $x$ è raggiungibile da un ciclo, allora $x$ non può essere ottenuto.
\end{itemize}
I problemi sono i seguenti:
\begin{itemize}
  \item Stabilire se esistono documenti che non si possono ottenere $\rightarrow$ basta verificare se il grafo è ciclico.
  \item Trovare un esempio di documento non ottenibile $\rightarrow$ quando trovo un arco $(u,v)$ all'indietro restituisco $v$.
  \item Dato $x$ stabilire se è ottenibile $\rightarrow$ lancia DFS Visit da $x$ sul grafo trasposto, restituisci true se trovi un ciclo.
  \item Trovare tutti i documenti NON ottenibili $\rightarrow$ 
  \begin{enumerate}
    \item Lancia DFS e marca tutti i nodi raggiunti da archi all'indietro. (per ogni ciclo nel grafo, ci sta almeno un nodo che viene marcato)
    \item Lancia nuovamente DFS,ma come nodi di partenza, usa quelli marcati. Ora \textit{marca} tutti nodi che raggiungi. Tutti i nodi marcati,
    appartenti ad un ciclo vengono raggiunti e dunque marcati. \textit{Tuttavia vengono marcati tutti e soli i nodi raggiungibili da un ciclo.}
  \end{enumerate}
\end{itemize}
\noindent
\ex{}
{
Vediamo il seguente problema: 
dati $(G,S,F),\; F \subseteq V[G], S \in V[G]$ stabilire se da $S$ esiste un cammino infinito che passa infinite volte
da stati (nodi) di $F$.
Una soluzione potrebbe esseere: guardiamo le SCC del grafo $G$ e costruiamo un nuovo grafo $G'$ i cui nodi sono le
SCC di $G$ arco $(u,v)$ di $G$ con $u \in A$ e $v \in B$.

\noindent
Questo grafo rispetta le seguenti proprietà:
\begin{itemize}
  \item $G$ è aciclico. Questo perché se $G$ fosse ciclico, allora vorrebbe dire che le due SCC collasserebbero in un unico nodo, ma questo non è possibile perché
  ogni SCC è un sottoinsieme di nodi che sono raggiungibili tra loro, quindi se ci fosse un ciclo, allora tutti i nodi sarebbero raggiungibili tra loro.
  \item Ogni cammino infinito resterà definitivamente all'interno di una singola SCC
  \item Affinché il cammino infinito possa contenere infinite istanze di stati di $F$ la SCC in cui si resterà definitviamnte 
  deve contenere almeno uno stato di $F$
\end{itemize}
\noindent
Quindi devo trovvare una SCC raggiungibile da S che 
\begin{itemize}
  \item contiene cammini infiniti 
  \item contiene almeno uno stato di $F$
\end{itemize}
}

\subsection{Alberi di copertura di costo minimo}

\subsubsection{Minimum Spanning Tree (MST)}

Venezia. Un evento atmosferico distrugge tutti i suoi ponti. Tutti i ponti da riparare per ogni ponte c'è un costo specifico di riparazione.
Dobbiamo garantire la raggiungibilità via terra di ciò che era raggiungibile prima. Dobbiamo spendere il meno possibile.
Proporre un algoritmo per segliere i ponti da riparare. 
Per risolvere questo problema potrei mappare il problema in un grafo in cui i nodi sono le isole e gli archi sono i ponti.
Quando vado a vedere la mappa generale dei ponti potrei andare ad eliminare tutti i ponti che non sono necessari per garantire la raggiungibilità tra le isole, come per esempio quegli archi che sono in un ciclo.
Finché il grafo ha cicli, posso eliminare gli archi che sono in un ciclo e il grafo rimarrà connesso. Quindi il grafo che otterremo sarà un grafo aciclico.
Questo oggetto che otteniamo è in realtà \textbf{un albero} di copertura, dove copertura si intende che tutti i nodi del grafo sono raggiungibili da un nodo qualsiasi dell'albero.
\dfn{Taglio di ung grafo}
{
  Il taglio di un grafo $G = (V,E)$ è una \textbf{bipartizione $(S,T)$ di $V$}. Quando parliamo di taglio vuol dire che stiamo 
  dividendo il grafo in deu parti, $S$ e $T$ e a questo punto possiamo distinguere gli archi 
  che vanno da $S$ a $T$ e quelli che vanno da $T$ a $S$ (archi del taglio).
}
\noindent
L'arco del taglio è detto \textbf{sicuro} se tra gli archi del taglio ha un costo minimo.

\begin{lemma}
  Dato un arco sicuro $(u,v)$ per un taglio $(S,T)$  esiste un MST di $G$ che contiene $(u,v)$.
\end{lemma}
\begin{proof}
  Prendi un taglio $(S,T)$ supponiamo che $(u,v)$ sia un arco sicuro per questo taglio e sia $A$ una soluzione.
  Distinguiamo due casi:
  \begin{itemize}
    \item $(u,v) \in A$, allora $A$ è una soluzione valida.
    \item $(u,v) \notin A$, allora esiste un cammino da $u$ a $v$ che non passa per $(u,v)$, quindi possiamo sostituire l'arco $(u,v)$ con questo cammino e ottenere una soluzione valida.
  \end{itemize}
  $(A - \{(s,t)\})) \cup \{(u,v)\}$ è pure un albero di copertura.
\end{proof}
\begin{lstlisting}[language=Scala]
MST(G)
  A <- 0
  finche A non e un albero di copertura 
    sia (S,T) un taglio che non contiene archi di A
    sia (u,v) un arco sicuro per (S,T)
    A <- A + (u,v)
\end{lstlisting}

\subsubsection{Algoritmo di Kruskal}

L'algoritmo di Kruskal è un algoritmo che permette di trovare un albero di copertura di costo minimo in un grafo pesato.

\begin{lstlisting}[language=Scala]
// G e il grafo e w e la matrice dei pesi 
Kruskal(G, w)
  A <- 0
  per ogni v in V[G] make_set(v)
  ordina E[G] per peso non decrescente
  per ogni (u,v) in E[G] preso in ordine
    if find_set(u) != find_set(v) then
      A <- A + (u,v)
      union(u,v)
\end{lstlisting}
\ex{}
{
  Immaginiamo di avere un grafo pesato $G$ con la seguente struttura:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw (C) -- node[above] {$1$} (E);
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw (F) -- node[above] {$5$} (E);
    \draw (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw (A) -- node[above] {$4$} (B);
  \end{tikzpicture} 
  \end{figure}
  \noindent
  Per prima cosa, nell'algoritmo di Kruskal, ogni nodo diventa un insieme singoletto. 
  Subito dopo, ordiniamo gli archi per peso non decrescente, il primo arco che andremo a prendere (ovvero l'arco con il peso minore)
  sarà (C,E) con peso 1. Quando ci chiediamo se \texttt{if find set(u) != find set(v)} 
  ci stiamo chiedendo se C ed E appartengono allo stesso insieme. Essendo singoletti, per forza di cose non lo sono,
  quindi andiamo aggiungere questo arco all'insieme $A$ e unire i due insiemi. 
  Basta prendere un taglio che abbia il nodo $C$ da una parte e il nodo $E$ dall'altra e noi scopriamo che tutti gli archi
  che appartengono a questo taglio, non fanno parte della soluzione perché sono tutti archi che pesano di più o uguali dell'arco $(C,E)$ di peso 1.
  Inoltre, è un arco sicuro e quindi può far parte della soluzione. 
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw[red] (C) -- node[above] {$1$} (E);
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw (F) -- node[above] {$5$} (E);
    \draw (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw (A) -- node[above] {$4$} (B);

    \draw[dashed, thick, green!40!black] (4.3,1) -- (-2,-2.5);

  \end{tikzpicture} 
  \end{figure}
  \noindent
  Ora bisogna considerare il secondo arco con peso minore, che è $(F,D)$ con peso 2.
  Anche in questo caso, facciamo un taglio che ha $F$ da una parte e $D$ dall'altra.
  Non fanno parte dello stesso insieme, quindi possiamo aggiungere l'arco $(F,D)$ all'insieme $A$ e unire i due insiemi.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw[red] (C) -- node[above] {$1$} (E);
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw (F) -- node[above] {$5$} (E);
    \draw[red] (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw (A) -- node[above] {$4$} (B);

    \draw[dashed, thick, green!40!black] (4.3,1) -- (-1,-2.5);
    \draw[dashed, thick, green!40!black] (-0.5, 3) -- (4.3, -1);

  \end{tikzpicture} 
  \end{figure}
  \noindent
  Il prossimo arco è $(A,B)$ con peso 4. Facciamo un taglio che ha $A$ da una parte e $B$ dall'altra.
  Di questo taglio, l'arco $(A,B)$ è sicuro, perché tutti gli archi che attraversano il taglio non sono ancora stati considerati,
  gli archi vengono presi in ordine non decrescente di peso, quindi tutti gli archi che attraversano il taglio hanno un peso maggiore o uguale a 4.
  Uniamo i due insiemi e aggiungiamo l'arco $(A,B)$ all'insieme $A$.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw[red] (C) -- node[above] {$1$} (E);
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw (F) -- node[above] {$5$} (E);
    \draw[red] (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw[red] (A) -- node[above] {$4$} (B);

    \draw[dashed, thick, green!40!black] (4.3,1) -- (-1,-2.5);
    \draw[dashed, thick, green!40!black] (-0.5, 3) -- (4.3, -1);
    \draw[dashed, thick, green!40!black] (-4, 3) -- (-4, -3);
  \end{tikzpicture} 
  \end{figure}
  \noindent
  Il prossimo arco è $(F,E)$ con peso 5. Facciamo un taglio che ha $F$ da una parte e $E$ dall'altra.
  Non sono nello stesso insieme quindi possiamo aggiungere l'arco $(F,E)$ all'insieme $A$ e unire i due insiemi.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw[red] (C) -- node[above] {$1$} (E);
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw[red] (F) -- node[above] {$5$} (E);
    \draw[red] (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw[red] (A) -- node[above] {$4$} (B);

    \draw[dashed, thick, green!40!black] (4.3,1) -- (-1,-2.5);
    \draw[dashed, thick, green!40!black] (-0.5, 3) -- (4.3, -1);
    \draw[dashed, thick, green!40!black] (-4, 3) -- (-4, -3);
    \draw[dashed, thick, green!40!black] (-6, -1) -- (4, -1);
  \end{tikzpicture} 
  \end{figure}
  \noindent
  Il prossimo arco è $(D,E)$ con peso 6. Tuttavia $D$ e $E$ \textbf{NON} sono nello stesso insieme, 
  quindi non possiamo aggiungere l'arco $(D,E)$ all'insieme $A$. Andiamo al prossimo: anche $(C,F)$ con peso 7 
  non può essere aggiunto all'insieme $A$ perché $C$ e $F$ sono nello stesso insieme.
  Subito dopo abbiamo due archi che pesano 8, ovvero $(B,D)$ e $(C,A)$. Facciamo finta che l'algoritmo di ordinamento
  abbia ordinato prima $(C,A)$ e poi $(B,D)$, quindi prendiamo l'arco $(C,A)$.
  Esiste un taglio che sapara $C$ da $A$ e che non separa elementi che stanno nello stesso insieme e quindi 
  possiamo aggiungere l'arco $(C,A)$ all'insieme $A$ e unire i due insiemi.
  Anche l'arco $(B,D)$ poteva andare bene, perché
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) -- node[above] {$7$} (F);
    \draw[red] (C) -- node[above] {$1$} (E);
    \draw[red] (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B);
    \draw[red] (F) -- node[above] {$5$} (E);
    \draw[red] (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D);
    \draw[red] (A) -- node[above] {$4$} (B);

    \draw[dashed, thick, green!40!black] (4.3,1) -- (-1,-2.5);
    \draw[dashed, thick, green!40!black] (-0.5, 3) -- (4.3, -1);
    \draw[dashed, thick, green!40!black] (-4, 3) -- (-4, -3);
    \draw[dashed, thick, green!40!black] (-6, -1) -- (4, -1);
    \draw[dashed, thick, green!40!black] (1, 3) -- (-6.5, -2.5);
  \end{tikzpicture} 
  \end{figure}
  \noindent
  Ora abbiamo un albero di copertura di costo minimo, che è l'insieme $A$.
  Nonostante parliamo di tagli in questo esempio, a livello di codice
  sappiamo già di averli in mano. 
}
Quanto è complesso l'algoritmo di Kruskal?
Ordinare gli archi richiede $O(E \log E)$, mentre l'operazione di unione e find\_set richiede $O(E)$.
Salta fuori che la complessità dell'algoritmo di Kruskal è $O(V + E \log E)$, quindi
l'algoritmo di ordinamento è il collo di bottiglia dell'algoritmo.
Nonostante ciò è comunque un algoritmo molto efficiente e utilizzato in molti contesti essendo 
un algoritmo di tipo greedy, ovvero che prende la soluzione migliore in ogni passo.

\subsubsection{Algoritmo di Prim}
\begin{lstlisting}[language=Scala]
// G grafo
// w matrice dei pesi
// r nodo da cui partiamo
Prim(G, w, r)
  Q <- V[g] // coda di priorita
  per ogni u in V[g]
    key[u] <- inf 
  key[r] <- 0
  pi[r] <- nil
  while Q non e vuota
    u <- extract_min(Q)
    per ogni v in Adj[v]
      if v in Q and w(u,v) < key[v] then
        pi[v] <- u
        key[v] <- w(u,v)
\end{lstlisting}

\ex{}
{
  Proviamo ad eseguire questo algoritmo su questo grafo. Settiamo tutti i nodi con chiave infinita, tranne il nodo $A$ che ha chiave 0.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
    semithick ,
    state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
    draw,processblue , text=blue , minimum width =1 cm}]
    \node[state] (F) {F};
    \node[state] (C) [below left=of F] {C};
    \node[state] (B) [above left =of F] {B};
    \node[state] (D) [above right=of F] {D};
    \node[state] (E) [below right=of F] {E};
    \node[state] (A) [above left=of C] {A};

    \draw (C) node[left, xshift=-11pt, blue] {$\infty$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$\infty$};
    \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$\infty$};
    \draw (C) -- node[below] {$8$} (A);
    \draw (C) -- node[left] {$11$} (B) node[above, yshift=11pt, blue] {$\infty$};
    \draw (F) -- node[above] {$5$} (E);
    \draw (F) -- node[above] {$2$} (D);
    \draw (F) -- node[above] {$9$} (B);
    \draw (B) -- node[above] {$8$} (D);
    \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$\infty$};
    \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);
  \end{tikzpicture} 
  \end{figure}
  Facciamo l'extract min, che ci restituisce il nodo $A$ e andiamo ad aggiornare
tutti i nodi nella lista di adiacenza di $A$. In questo esempio segnerò con le frecce blu i tagli sicuri che costruiranno la soluzione e con le frecce rosse i parent. Andiamo a prendere l'arco $(A,B)$ con peso 4, che è minore di $\infty$ 
e quindi andiamo ad aggiornare la chiave di $B$ a 4 e il suo predecessore a $A$.
Stessa cosa per l'arco $(A,C)$ con peso 8, che è minore di $\infty$ e quindi andiamo ad aggiornare la chiave di $C$ a 8 e il suo predecessore a $A$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (F) {F};
  \node[state] (C) [below left=of F] {C};
  \node[state] (B) [above left =of F] {B};
  \node[state] (D) [above right=of F] {D};
  \node[state] (E) [below right=of F] {E};
  \node[state] (A) [above left=of C] {A};

  \draw (C) node[left, xshift=-11pt, blue] {$8$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$\infty$};
  \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$\infty$};
  \draw (C) -- node[below] {$8$} (A);
  \draw (C) -- node[left] {$11$} (B) node[above, yshift=12pt, blue] {$4$};
  \draw (F) -- node[above] {$5$} (E);
  \draw (F) -- node[above] {$2$} (D);
  \draw (F) -- node[above] {$9$} (B);
  \draw (B) -- node[above] {$8$} (D);
  \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$\infty$};
  \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);

  \draw[blue, yshift=5pt, -latex] (B) -- (A);
  \draw[red, -latex] (C) -- (A);
\end{tikzpicture} 
\end{figure}
Una volta aver visitato le liste di adiacenza di $A$, andiamo a fare l'extract min, che ci restituisce il nodo $B$.
Abbiamo già visto $A$ e quindi non lo consideriamo, mentre andiamo a considerare $D$ che ha chiave 8,
che è minore di $\infty$ e quindi andiamo ad aggiornare la chiave di $D$ a 8 e il suo predecessore a $B$.
Stessa cosa per l'arco $(B,F)$ con peso 9, che è minore di $\infty$ e quindi andiamo ad aggiornare la chiave di $F$ a 9 e il suo predecessore a $B$.
Tuttavia il nodo $C$ ha chiave 8 che è minore di 11 (l'arco $(B,C)$) e quindi non lo consideriamo.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (F) {F};
  \node[state] (C) [below left=of F] {C};
  \node[state] (B) [above left =of F] {B};
  \node[state] (D) [above right=of F] {D};
  \node[state] (E) [below right=of F] {E};
  \node[state] (A) [above left=of C] {A};

  \draw (C) node[left, xshift=-11pt, blue] {$8$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$9$};
  \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$\infty$};
  \draw (C) -- node[below] {$8$} (A);
  \draw (C) -- node[left] {$11$} (B) node[above, yshift=12pt, blue] {$4$};
  \draw (F) -- node[above] {$5$} (E);
  \draw (F) -- node[above] {$2$} (D);
  \draw (F) -- node[above] {$9$} (B);
  \draw (B) -- node[above] {$8$} (D);
  \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$8$};
  \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);

  \draw[blue, yshift=5pt, -latex] (B) -- (A);
  \draw[red, -latex] (C) -- (A);
  \draw[blue, -latex] (D) -- (B);
  \draw[red, -latex] (F) -- (B);
\end{tikzpicture} 
\end{figure}
\noindent
Nel prossimo extract min, possiamo decidere se prendere $C$ o $D$ essendo che pesano uguali. Prendiamo $D$.
Guardando i vicini di D, arriviamo ad $F$ dove 2 è minore di 9 e quindi andiamo ad aggiornare la chiave di $F$ a 2 e il suo predecessore a $D$. Inoltre, la freccia 
che prima collegava F e B viene eliminata perché ora sappiamo che l'arco $(D,F)$ è più conveniente.
Stessa cosa per l'arco $(D,E)$ con peso 6, che è minore di $\infty$ e quindi andiamo ad aggiornare la chiave di $E$ a 6 e il suo predecessore a $D$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (F) {F};
  \node[state] (C) [below left=of F] {C};
  \node[state] (B) [above left =of F] {B};
  \node[state] (D) [above right=of F] {D};
  \node[state] (E) [below right=of F] {E};
  \node[state] (A) [above left=of C] {A};

  \draw (C) node[left, xshift=-11pt, blue] {$8$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$2$};
  \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$6$};
  \draw (C) -- node[below] {$8$} (A);
  \draw (C) -- node[left] {$11$} (B) node[above, yshift=12pt, blue] {$4$};
  \draw (F) -- node[above] {$5$} (E);
  \draw (F) -- node[above] {$2$} (D);
  \draw (F) -- node[above] {$9$} (B);
  \draw (B) -- node[above] {$8$} (D);
  \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$8$};
  \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);

  \draw[blue, yshift=5pt, -latex] (B) -- (A);
  \draw[red, -latex] (C) -- (A);
  \draw[blue, -latex] (D) -- (B);
  \draw[red, -latex] (F) -- (D);
  \draw[red, -latex] (E) -- (D);
\end{tikzpicture} 
\end{figure}
\noindent
Successivamente facciamo l'extract min e prendiamo $F$ che ha due ed è il nodo più piccolo.
Prendiamo $C,F$ e notiamo come $7 < 8$ di consegueza aggiorniamo il suo parent e la sua chiave.
Stessa cosa per $F,E$. 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (F) {F};
  \node[state] (C) [below left=of F] {C};
  \node[state] (B) [above left =of F] {B};
  \node[state] (D) [above right=of F] {D};
  \node[state] (E) [below right=of F] {E};
  \node[state] (A) [above left=of C] {A};

  \draw (C) node[left, xshift=-11pt, blue] {$7$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$2$};
  \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$5$};
  \draw (C) -- node[below] {$8$} (A);
  \draw (C) -- node[left] {$11$} (B) node[above, yshift=12pt, blue] {$4$};
  \draw (F) -- node[above] {$5$} (E);
  \draw (F) -- node[above] {$2$} (D);
  \draw (F) -- node[above] {$9$} (B);
  \draw (B) -- node[above] {$8$} (D);
  \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$8$};
  \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);

  \draw[blue, yshift=5pt, -latex] (B) -- (A);
  \draw[red, -latex] (C) -- (F);
  \draw[blue, -latex] (D) -- (B);
  \draw[blue, -latex] (F) -- (D);
  \draw[red, -latex] (E) -- (F);
\end{tikzpicture} 
\end{figure}
\noindent
Ora facciamo l'extract min e prendiamo $E$ che ha chiave 5.
Vediamo come l'arco $(E,C)$ con peso 1 è minore di 7 e quindi andiamo ad aggiornare la chiave di $C$ a 1 e il suo predecessore a $E$.
Il prossimo extract min è $C$ che è l'unico rimasto, sappiamo che $1$ è un taglio sicuro e quindi viene connesso ad $E$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (F) {F};
  \node[state] (C) [below left=of F] {C};
  \node[state] (B) [above left =of F] {B};
  \node[state] (D) [above right=of F] {D};
  \node[state] (E) [below right=of F] {E};
  \node[state] (A) [above left=of C] {A};

  \draw (C) node[left, xshift=-11pt, blue] {$1$} -- node[above] {$7$} (F) node[right, xshift=11pt, blue] {$2$};
  \draw (C) -- node[above] {$1$} (E) node[right, xshift=11pt, blue] {$5$};
  \draw (C) -- node[below] {$8$} (A);
  \draw (C) -- node[left] {$11$} (B) node[above, yshift=12pt, blue] {$4$};
  \draw (F) -- node[above] {$5$} (E);
  \draw (F) -- node[above] {$2$} (D);
  \draw (F) -- node[above] {$9$} (B);
  \draw (B) -- node[above] {$8$} (D);
  \draw (E) -- node[right] {$6$} (D) node[right, xshift=11pt, blue] {$8$};
  \draw (A) node[right, xshift=11pt, blue] {$0$} -- node[above] {$4$} (B);

  \draw[blue, yshift=5pt, -latex] (B) -- (A);
  \draw[blue, -latex] (D) -- (B);
  \draw[blue, -latex] (F) -- (D);
  \draw[blue, -latex] (E) -- (F);
  \draw[blue, -latex] (C) -- (E);

\end{tikzpicture} 
\end{figure}
\noindent
Abbiamo trovato un albero di copertura di costo minimo, che è l'insieme degli archi con le frecce blu.
}
\noindent
Settare il primo nodo a $0$ va bene solo se sappiamo che il grafo ha solo pesi positivi o comunque sia il primo ad essere
estratto. Ecco perché una chiave come $-\infty$ sarebbe stata più corretta. 
Settare tutti gli altri nodi a $\infty$ ha complessità $O(V)$ e poi $V$ volte l'extract min richiede $O(V \log V)$.


\subsection{Cammini minimi}

In questo caso, non voglio più un albero di copertura che colleghi tutti i nodi, ma voglio trovare il cammino minimo da un nodo di partenza ad un nodo di arrivo.
Un algoritmo per il cammino minimo avrà come input un grafo $G = (V, E)$ (con la sua $w$ che è la matrice dei pesi) e un nodo di partenza $s \in V$ e come output un cammino di costo minimo.
Di cammini minimi ne potrebbero esistere molti, come zero.
Non abbiamo la garanzia che esista un cammino minimo anche se il grafo è connesso, perché ci potrebbero essere dei cicli negativi e questo la rende la soluzione impossibile.

\begin{lemma}
  Sia $P$ un cammino minimo da $s$ a $v$ e sia $u$ il predecessore di $v$ in $p$. 
  \[P: S \rightarrow  u \rightarrow v\]
  Allora il prefisso di $P$ da $S$ a $V$ è un cammino minimo da $S$ a $u$.
\end{lemma}
\noindent
Conseguenza di questo lemma: possiamo rappresentare alcuni insieme di cammini minimi con un albero radicato in $S$ (albero dei cammini minimi).
Quindi possiamo ricollegarci ad un problema di cammini minimi.\\
Partiamo ad una stima iniziale:
\begin{itemize}
  \item Un cammino di costo minimo da $s$ a $_1$ se esiste, ha costo $0$.
  \item Tutti gli altri nodi sono raggiungibili con costo infinito.
\end{itemize}
\begin{lstlisting}[language=Scala]
init(G,s) 
  forall u in V[G]
    d[u] <- inf
    pi[u] <- nil
  d[s] <- 0
\end{lstlisting}
\noindent
Cerchiamo di migliorare questa stima iniziale, tramite il rilassamento degli archi.
\begin{lstlisting}[language=Scala]
relax(u,v,w)
  if d[v] > d[u] + w(u,v) then
    d[v] <- d[u] + w(u,v)
    pi[v] <- u
\end{lstlisting}
\begin{figure}[H]
  \centering


  \begin{tikzpicture}[>=Stealth, node distance=2.8cm, on grid, auto]

    % Nodes
    \node[circle, draw, thick, minimum size=1.2cm] (s) {$S$};
    \node[circle, draw, thick, minimum size=1.2cm, right=of s] (u) {$u$};
    \node[circle, draw, thick, minimum size=1.2cm, right=of u] (v) {$v$};
  
    % Arrows
    \draw[->, thick] (s) -- node[midway, above] {$d[u]$} (u);
    \draw[->, thick] (u) -- node[midway, above] {$w(u,v)$} (v);
    \draw[->, thick, bend left=40] (s) to node[midway, above] {$d[v]$} (v);
  \end{tikzpicture} 
\end{figure}
In poche parole se il percorso alternativo è migliore di quello attuale, allora aggiorniamo il cammino minimo. Aggiorno anche il predecessore di $v$ a $u$.
Se ho un albero dei cammini minimi, allora nessun arco è rilassabile.
\[\forall (u,v \in E[G] \; \; \; d[v] \le d[v] + w(u,v))\]
Se questa condizione è vera (Disequazione di Bellman) allora abbiamo trovato un albero dei cammini minimi.
Come potremmo rilassare gli archi di un grafo in modo da arrivare ad un albero dei cammini minimi?
Proponiamo l'algorimo di Dijkstra, che funziona solo se non esistono archi negativi.
\begin{lstlisting}[language=Scala]
Dijkstra(G,w,s)
  Init(G,w)
  Q <- V[G] // coda di priorita ordinata su d
  while Q non e vuota
    u <- extract_min(Q) // nodo con d[u] minimo
    per ogni v in Adj[u]
      relax(u,v,w)
\end{lstlisting}  
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={-latex, circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (S) {S};
  \node[state] (C) [below right=of S] {C};
  \node[state] (A) [above right =of S] {A};
  \node[state] (B) [right=of A] {B};
  \node[state] (D) [right=of C] {D};
 
  \draw[->] (S) -- node[above]{$3$} (A) node[above, yshift=12pt, blue] {$\infty$};
  \draw[->] (S) -- node[below]{$5$} (C) node[below, yshift=-12pt, blue] {$\infty$}; 
  \path[->] (C) edge [bend left =25] node[right =0.1 cm] {$1$} (A);
  \path[->] (A) edge [bend left =25] node[left =0.1 cm] {$1$} (C);
  \draw[->] (C) -- node[below]{$6$} (D) node[above, yshift=-22pt, blue] {$\infty$};
  \draw[->] (C) -- node[above]{$4$} (B) node[above, yshift=12pt, blue] {$\infty$};
  \draw[->] (A) -- node[above]{$6$} (B);
  \path[->] (D) edge [bend left =25] node[right =0.1 cm] {$17$} (B);
  \path[->] (B) edge [bend left =25] node[left =0.1 cm] {$2$} (D);
\end{tikzpicture} 
\end{figure}
\noindent
Iniziamo a rilassare gli archi partendo da $S$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={-latex, circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (S) {S};
  \node[state] (C) [below right=of S] {C};
  \node[state] (A) [above right =of S] {A};
  \node[state] (B) [right=of A] {B};
  \node[state] (D) [right=of C] {D};
 
  \draw[->] (S) -- node[above]{$3$} (A) node[above, yshift=12pt, blue] {$3$};
  \draw[->] (S) -- node[below]{$5$} (C) node[below, yshift=-12pt, blue] {$5$}; 
  \path[->] (C) edge [bend left =25] node[right =0.1 cm] {$1$} (A);
  \path[->] (A) edge [bend left =25] node[left =0.1 cm] {$1$} (C);
  \draw[->] (C) -- node[below]{$6$} (D) node[above, yshift=-22pt, blue] {$\infty$};
  \draw[->] (C) -- node[above]{$4$} (B) node[above, yshift=12pt, blue] {$\infty$};
  \draw[->] (A) -- node[above]{$6$} (B);
  \path[->] (D) edge [bend left =25] node[right =0.1 cm] {$17$} (B);
  \path[->] (B) edge [bend left =25] node[left =0.1 cm] {$2$} (D);

  \draw[red, ->, dashed] (A) -- (S);
  \draw[red, ->, dashed] (C) -- (S);
\end{tikzpicture}
\end{figure}
\noindent
Ogni volta che rilassiamo un arco con Dijkstra, lo stiamo rilassando da un nodo che ha già un cammino minimo.
Procediamo rilassando gli archi di $A$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={-latex, circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (S) {S};
  \node[state] (C) [below right=of S] {C};
  \node[state] (A) [above right =of S] {A};
  \node[state] (B) [right=of A] {B};
  \node[state] (D) [right=of C] {D};
 
  \draw[->] (S) -- node[above]{$3$} (A) node[above, yshift=12pt, blue] {$3$};
  \draw[->] (S) -- node[below]{$5$} (C) node[below, yshift=-12pt, blue] {$4$}; 
  \path[->] (C) edge [bend left =25] node[right =0.1 cm] {$1$} (A);
  \path[->] (A) edge [bend left =25] node[left =0.1 cm] {$1$} (C);
  \draw[->] (C) -- node[below]{$6$} (D) node[above, yshift=-22pt, blue] {$\infty$};
  \draw[->] (C) -- node[above]{$4$} (B) node[above, yshift=12pt, blue] {$9$};
  \draw[->] (A) -- node[above]{$6$} (B);
  \path[->] (D) edge [bend left =25] node[right =0.1 cm] {$17$} (B);
  \path[->] (B) edge [bend left =25] node[left =0.1 cm] {$2$} (D);

  \draw[red, ->, thick] (A) -- (S);
  \draw[red, ->, thick] (C) -- (A);
  \draw[red, ->] (B) -- (A);
\end{tikzpicture}
\end{figure}
\noindent
Ora rilassiamo gli archi di $C$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={-latex, circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (S) {S};
  \node[state] (C) [below right=of S] {C};
  \node[state] (A) [above right =of S] {A};
  \node[state] (B) [right=of A] {B};
  \node[state] (D) [right=of C] {D};
 
  \draw[->] (S) -- node[above]{$3$} (A) node[above, yshift=12pt, blue] {$3$};
  \draw[->] (S) -- node[below]{$5$} (C) node[below, yshift=-12pt, blue] {$4$}; 
  \path[->] (C) edge [bend left =25] node[right =0.1 cm] {$1$} (A);
  \path[->] (A) edge [bend left =25] node[left =0.1 cm] {$1$} (C);
  \draw[->] (C) -- node[below]{$6$} (D) node[above, yshift=-24pt, blue] {$10$};
  \draw[->] (C) -- node[above]{$4$} (B) node[above, yshift=12pt, blue] {$8$};
  \draw[->] (A) -- node[above]{$6$} (B);
  \path[->] (D) edge [bend left =25] node[right =0.1 cm] {$17$} (B);
  \path[->] (B) edge [bend left =25] node[left =0.1 cm] {$2$} (D);

  \draw[red, ->, thick] (A) -- (S);
  \draw[red, ->, thick] (C) -- (A);
  \draw[red, ->, thick] (B) -- (C);
\end{tikzpicture}
\end{figure}
\noindent
Ora rilassiamo gli archi di $B$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[auto, node distance =2 cm and 3cm ,on grid ,
  semithick ,
  state/.style ={-latex, circle ,top color =white , bottom color = processblue!20 ,
  draw,processblue , text=blue , minimum width =1 cm}]
  \node[state] (S) {S};
  \node[state] (C) [below right=of S] {C};
  \node[state] (A) [above right =of S] {A};
  \node[state] (B) [right=of A] {B};
  \node[state] (D) [right=of C] {D};
 
  \draw[->] (S) -- node[above]{$3$} (A) node[above, yshift=12pt, blue] {$3$};
  \draw[->] (S) -- node[below]{$5$} (C) node[below, yshift=-12pt, blue] {$4$}; 
  \path[->] (C) edge [bend left =25] node[right =0.1 cm] {$1$} (A);
  \path[->] (A) edge [bend left =25] node[left =0.1 cm] {$1$} (C);
  \draw[->] (C) -- node[below]{$6$} (D) node[above, yshift=-24pt, blue] {$10$};
  \draw[->] (C) -- node[above]{$4$} (B) node[above, yshift=12pt, blue] {$8$};
  \draw[->] (A) -- node[above]{$6$} (B);
  \path[->] (D) edge [bend left =25] node[right =0.1 cm] {$17$} (B);
  \path[->] (B) edge [bend left =25] node[left =0.1 cm] {$2$} (D);

  \draw[red, ->, thick] (A) -- (S);
  \draw[red, ->, thick] (C) -- (A);
  \draw[red, ->, thick] (B) -- (C);
  \draw[red, ->, thick] (D) -- (C);
  \end{tikzpicture}
\end{figure}
\noindent
Rilassando gli archi di $D$ non otteniamo nessun miglioramento, perché il cammino minimo da $S$ a $D$ è 10 e non 8.
Fatto ciò discutiamo la complessità dell'algoritmo di Dijkstra.
La Init richiede $O(V)$, l'extract min richiede $O(V)$ e rilassare un arco richiede una riduzione di chiave che richiede $O(E)$.
Di base ho \[V + E + VC(ExtMin) + EC(RedKey)\]
quindi dipende dalla struttura dati che utilizziamo per la coda di priorità e come implementata la riduzione di chiave.

\begin{itemize}
  \item Lista ordinata: $O(V^2 + E + EV) = O(V + E + EV)$
  \item Lista non ordinata: $O(V + E + V^2 + E) = O(V^2 + E) = O(V^2)$, mi costa molto mantenerla ordinata quindi è più conveniente non tenerla ordinata.
  \item Heap: $O(V + E + V \log V + E \log V) = O((V+E)\log V)$, questo perché l'extract min richiede $O(\log V)$ e la riduzione di chiave richiede $O(\log V)$.
  Tuttavia, se il grafo è sparso conviene ma se fosse completo allora conviene utilizzare la lista non ordinata.
\end{itemize}
\begin{lstlisting}[language=Scala]
BellMan-Ford(G,w,s)
  Init(G,s)
    for i <- 1 to V[G] - 1 do
      for (u,v) in E[G] do
        relax(u,v,w)
\end{lstlisting}
BellMan-Ford semplicemente rilassa tutti gli archi $V-1$ volte, perché sappiamo che un cammino minimo non può avere più di $V-1$ archi.
\begin{lstlisting}[language=Scala]
per ogni (u,v) in E[G]
  if d[v] > d[u] + w(u,v) then
    return false // questo accade se esiste un ciclo negativo
return true // albero dei cammini minimi
\end{lstlisting}
\begin{lemma}
  Dopo $i$ iterazione tutti i cammini minimi tra quelli che contengono al più $i$ archi sono stati trovati.
\end{lemma}
\noindent
\begin{proof}
  Base: $i = 0$, $S$ è l'unico nodo con un cammino minimo, quindi è vero.
  Induzione: supponiamo sia vero per $i$, dimostriamo $i+1$. 
  Consideriamo un cammino minimo $P$ da $S$ a $v$ con al più $i+1$ archi, quindi ha un predecessore $u$ che ha al più $i$ archi.
  Per ipotesi induttiva, dopo $i$ iterazioni, il cammino minimo da $S$ a $u$ è stato trovato e quindi rilassando l'arco $(u,v)$ otteniamo il cammino minimo da $S$ a $v$.
\end{proof}
La complessità di BellMan-Ford è $O(VE)$, perché rilassiamo tutti gli archi $V-1$ volte e quindi abbiamo $O(VE)$.
Supponiamo che esista un ciclo negativo ma che BF non se ne accorga.
Sia $\{v_0, v_1, \dots, v_n, v_0\}$ un ciclo negativo. Supponiamo BF non restituisca $false$.
Andiamo a controllare ogni singolo arco di questo ciclo.
\[
\begin{aligned}
  d[v_1] &\le d[v_0] + w(v_0, v_1)\\
  d[v_2] &\le d[v_1] + w(v_1, v_2)\\
  &\vdots\\
  d[v_n] &\le d[v_{n-1}] + w(v_{n-1}, v_n)\\
  d[v_0] &\le d[v_{n}] + w(v_{n}, v_0)\\
  ----&----------\\
  \sum_{i=0}^n d[v_i] &\le \sum_{i=0}^{n} d[v_i] + \sum_{i=0}^{n} w(v_i, v_{i+1})
\end{aligned}
\]
Tuttavia 
\[  \cancel{\sum_{i=0}^n d[v_i]} \le \cancel{\sum_{i=0}^{n} d[v_i]} + \underbrace{\sum_{i=0}^{n} w(v_i, v_{i+1})}_{\text{Costo del ciclo}} \]
Quindi rimane
\[0 \le \sum_{i=0}^{n} w(v_i, v_{i+1})\]
Ma questo è falso, perché abbiamo presupposto che il ciclo fosse negativo. Quindi 
l'algoritmo di BellMan-Ford non può sbagliare se il grafo ha cicli negativi.
Proviamo a vedere se esistono algoritmi per grafi aciclici.
\begin{lstlisting}[language=Scala]
DAG_SP(G,w,s)
  Init(G,s)
  Calcola ordinamento topologico di G
  per ogni u in V[G] in ordine topologico
    for ogni v in Adj[u]
      relax(u,v,w)
\end{lstlisting}
\noindent
Grazie all'ordinamento topologico possiamo direttamente sapere se il grafo è aciclico o meno, perché altrimenti non riuscirebbe a trovarne
un ordinamento topologico. L'ordinamento topologico ha come complessità $O(V+E)$, la relax la facciamo una volta per ogni arco e quindi la complessità è $O(V+E)$.
E se noi volessimo calcolare il cammino di costo \textbf{massimo}? In questo caso ci basterebbe invertire i segni dei pesi e applicare l'algoritmo di Dijkstra e adattare
l'algoritmo e ricondurmi al problema del cammino minimo. 
Se qualcuno volesse rendere massimo il prodotto? Posso utilizzare i logaritmi, perché il logaritmo di un prodotto è la somma dei logaritmi e di conseguenza 
ricondurmi nuovamente al problema del cammino minimo.

\subsection{Cammini minimi con sorgente multipla}

Dobbiamo risolvere ora il problema di cammini minimi non con una singola sorgente ma per ogni coppia di nodi (da un nodo $A$ ad un nodo $B$).
Si potrebbe risolvere facilmente applicando un qualsiasi algoritmo per cammini minimi per ogni nodo sorgente. La complessità di questo approccio sarebbe applicare Bellman-Ford ($O(VE)$) 
per ogni nodo e quindi la complessità finale sarebbe $O(V^2E)$.
Si può ottenere un algoritmo più efficiente, cercando di rappresentare il nostro problema come una matrice, dove $D_{ij}$ è il costo del camminimo minimo da $i$ a $j$ e $\pi_{ij}$ è il predecessore del nodo $j$ nel cammino da $i$ a $j$.
\begin{lstlisting}[language=Scala]
print_path(pi, i, j)
  if pi[i][j] == nil then
    print(i, j)
  else
    print_path(pi, i, pi[i][j])
    print(pi[i][j], j)
\end{lstlisting}
\noindent
Vediamo di visualizzare il problema con un esempio. Facciamo una init per ogni riga.
\[
D^0 = \begin{pmatrix}
  0 & \infty & \dots & \dots & \infty\\
  \infty & 0 & \infty & \dots & \vdots\\
  \vdots & \infty & 0 & \infty & \vdots\\
  \vdots & \infty & \infty & 0 & \vdots\\
  \infty & \infty & \infty & \infty & 0
\end{pmatrix} \; \; \; \; \; \; D^0 \leftarrow Init(G,w)
\]
\begin{lstlisting}[language=Scala]
extend_sp(D, W)
  n <- rows(D)
  sia D' una matrice n x n
  for i <- 1 to n 
    for j <- 1 to n
      D'[i][j] <- infinito 
      for k <- 1 to n
        D'[i][j] <- min(D[i][j], D[i][k] + W[k][j]) // il rilassamento
        // non e nient altro che un minimo
  return D'

all_pairs_sp(G,w) 
  D^0 <- Init(G,w)
  for l <- 1 to |V[G]| - 1 do
    D^l <- extend_sp(D^{l-1}, W)
  D^|V(G)| <- extend_sp(D^{|V(G)|-1}, W)
  if D^|V(G)|[i][i] != D^V(G)-1 then
    return false // esiste un ciclo negativo
  else
    return D^|V(G)|
\end{lstlisting}
\noindent
Come possiamo vedere la complessità è di fare un extend-sp per ogni nodo e l'extend shortest path ha come complessità $O(V^3)$
che devo fare $V$ volte e quindi la complessità finale è $O(V^4)$.
Siamo abituati a lavorare con somma e prodotti, ora stiamo lavorando con min e somma. Quindi quello che stiamo facendo è un \textbf{algebra di min-somma}.
Quello che stiamo facendo in realtà non è nient'altro che moltiplicazioni di matrici ma con un'algebra diversa.
\[D^0 \cdot W^{V-1}\]
Possiamo notare come la matrice $D^0$ non è nient'altro che la matrice identità ma con l'algebra di min-somma.
\[D_0 = \begin{pmatrix}
  0 & \infty & \infty\\
  \infty & 0 & \infty\\
  \infty & \infty & 0
\end{pmatrix} \rightarrow \begin{pmatrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 1
\end{pmatrix} = I_3\]
Questo perché  1 è l'elemento neutro della somma e 0 è l'elemento neutro del min.
Mentre per la matrice $D_0$ l'elemento neutro della somma è $\infty$ e l'elemento neutro della somma è 0.
Stiamo anche calcolando una potenza di $W$ e per ora abbiamo visto come il prodotto di matrici ha come complessità $O(V^4)$.
Si può fare meglio di così?
\begin{lstlisting}[language=Scala]
Floyd-Warshal(W)
  D^k[i][j] costo minimo da i a j che usa solo nodi di {1 ... k} come nodi intermedi
  n <- rows(W)
  D^0 <- W // le distanze iniziali coincidono con i pesi degli archi
  for k <- 1 to N
    for i <- 1 to n
      for j <- 1 to n
        D^k[i][j] <- min(D^{k-1}[i][j], D^{k-1}[i][k] + D^{k-1}[k][j])
\end{lstlisting}
Questo è il modo in cui rappresentiamo il rilassamento:
\[        D^k[i][j] \leftarrow min(D^{k-1}[i][j], D^{k-1}[i][k] + D^{k-1}[k][j])
\]
Tra $i$ e $j$ uso:
\begin{itemize}
  \item O lo stesso per corso percorso precedente (senza passaer per $k$)
  \item Oppure un nuovo percorso che passa per $k$ se è migliore 
\end{itemize}
\begin{figure}[H]
  \centering


  \begin{tikzpicture}[>=Stealth, node distance=2.8cm, on grid, auto]

    % Nodes
    \node[circle, draw, thick, minimum size=1.2cm] (s) {$i$};
    \node[circle, draw, thick, minimum size=1.2cm, right=of s] (u) {$k$};
    \node[circle, draw, thick, minimum size=1.2cm, right=of u] (v) {$j$};
  
    % Arrows
    \draw[->, thick] (s) -- node[midway, above] {$D^{k-1}[i][k]$} (u);
    \draw[->, thick] (u) -- node[midway, above] {$D^{k-1}[k][j]$} (v);
    \draw[->, thick, bend left=40] (s) to node[midway, above] {$D^{k-1}[i][j]$} (v);
  \end{tikzpicture} 
\end{figure}
\noindent
Questo algoritmo ha come complessità $O(V^3)$, perché abbiamo tre cicli annidati che iterano su $V$.
Supponiamo ora che tutti pesi non negativi, possiamo usare l'algoritmo di Dijkstra per ogni nodo sorgente $(V+E)\log{V}$.
La complessità finale è $V(V+E)\log{V} = V^2\log{V} + VE\log{V}$. 
Se il grafo è sparso ($E = \Theta(V)$) allora la complessità è $O(V^2\log{V})$ che è meglio di Floyd-Warshall.
Quindi un algoritmo migliore di Floyd-Warshall esiste solo se il grafo è sparso e non ci sono pesi negativi.
E se ci fossero pesari negativi invece? Johnson, un famoso ricercatore disse: "e se trasformasimo i pesi del grafo in modo da non avere pesi negativi?".
Una soluzione ovvia potrebbe essere di sommare una costante che potrebbe essere opposta al valore più baso nei pesi del grafo.
Tuttavia se applicassimo questo metodo vorrebbe dire che se esistesse un ciclo negativo (che di base non ha soluzione) allora 
sarebbe trasformato in un ciclo positivo e questo porterebbe ad una soluzione, quindi per contraddizione questo metodo non è applicabile.
L'idea che ha avuto Johnson è quello di prendere un \textbf{super nodo} $s$ e collegarlo a tutti i nodi del grafo con archi di peso $0$.
Usare ora $BF$ per calcolare i cammini minimi da $s$ a tutti gli altri nodi.
Sia $d$ la funzione distinza minima. Sia $h \stackrel{\Delta}{=} d$. 
La complessità: $VE + VE\log{V} = VE\log{V}$ per grafi sparsi, diventa $V^2\log{V}$. 
\begin{proof}
  Sia $h : V \rightarrow \mathbb{R}$ dove $\hat{w}(u,v) = w(u,v) + h[u] - h[v]$.
  L'algoritmo di Johnson per funzionare deve trovare una funzione $h$ buona che soddisfi l'equazione prima citata. Utilizzeremo la funzione di distanza come funziona $h$.
  Ricordiamo che BF si accorge se ci sono cicli negativi, quindi se ne trova uno si ferma subito e afferma che non esiste un albero dei cammini minimi.
  Se invece non trova cicli negativi, allora trova sicuramente una soluzione. Ci basta fare l'algoritmo di BF solo una volta e poi andare di Dijkstra sapendo
  che non ci sono cicli negativi.
  \[
  \begin{aligned}
    \forall (u,v) \in E[G] \; \; d[v] &\le d[u] + w(u,v)\\
    0 &\le d[u] + w(u,v) - d[v]\\
    &= w(u,v) + d[u]- d[v] =  \hat{w}(u,v)
  \end{aligned}
  \]
\end{proof}
\noindent
Se io invece volessi in qualche modo evitare i cicli negativi e comunque trovare una soluzione da un nodo $A$ e un nodo $B$ nello stesso grafo?
Un modo potrebbe essere quello di considerare solo i nodi che sono raggiungibili da $A$ e che possono raggiungere $B$ solo questi nodi possono essere considerati per il calcolo del cammino minimo.
Per capire quali nodi sono raggiungibili da $A$ e possono raggiungere $B$ possiamo utilizzare un DFS o BFS da entrambi i nodi.
Prima faccio una visita dal nodo $A$ e li marco, subito dopo faccio la stessa cosa da $B$ ma questa volta in modo inverso, tramite il grafico trasposto.
E quindi rimuovo tutti i nodi che non sono stati marcati in entrambi i casi e applico il cammino minimo su questo grafo ridotto.

\subsection{Flusso massimo}


\end{document}
